diff --git a/.meson-subproject-wrap-hash.txt b/.meson-subproject-wrap-hash.txt
new file mode 100644
index 000000000..e69de29bb
diff --git a/CMakeLists.txt b/CMakeLists.txt
new file mode 100644
index 000000000..1f62789ea
--- /dev/null
+++ b/CMakeLists.txt
@@ -0,0 +1,6 @@
+cmake_minimum_required(VERSION 3.20.0)
+project(LLVM)
+unset(LLVM_VERSION_MAJOR)
+unset(LLVM_VERSION_MINOR)
+unset(LLVM_VERSION_PATCH)
+add_subdirectory(llvm)
\ No newline at end of file
diff --git a/clang/cmake/modules/AddClang.cmake b/clang/cmake/modules/AddClang.cmake
index cdc8bd5cd..14af7860e 100644
--- a/clang/cmake/modules/AddClang.cmake
+++ b/clang/cmake/modules/AddClang.cmake
@@ -96,14 +96,6 @@ macro(add_clang_library name)
     else()
       set(LIBTYPE STATIC)
     endif()
-    if(NOT XCODE AND NOT MSVC_IDE)
-      # The Xcode generator doesn't handle object libraries correctly.
-      # The Visual Studio CMake generator does handle object libraries
-      # correctly, but it is preferable to list the libraries with their
-      # source files (instead of the object files and the source files in
-      # a separate target in the "Object Libraries" folder)
-      list(APPEND LIBTYPE OBJECT)
-    endif()
     set_property(GLOBAL APPEND PROPERTY CLANG_STATIC_LIBS ${name})
   endif()
   llvm_add_library(${name} ${LIBTYPE} ${ARG_UNPARSED_ARGUMENTS} ${srcs})
diff --git a/clang/include/clang/AST/ASTFwd.h b/clang/include/clang/AST/ASTFwd.h
index 882366338..6dc1be56f 100644
--- a/clang/include/clang/AST/ASTFwd.h
+++ b/clang/include/clang/AST/ASTFwd.h
@@ -18,13 +18,13 @@ namespace clang {
 
 class Decl;
 #define DECL(DERIVED, BASE) class DERIVED##Decl;
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
 class Stmt;
 #define STMT(DERIVED, BASE) class DERIVED;
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
 class Type;
 #define TYPE(DERIVED, BASE) class DERIVED##Type;
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 class CXXCtorInitializer;
 class OMPClause;
 #define GEN_CLANG_CLAUSE_CLASS
@@ -32,7 +32,7 @@ class OMPClause;
 #include "llvm/Frontend/OpenMP/OMP.inc"
 class Attr;
 #define ATTR(A) class A##Attr;
-#include "clang/Basic/AttrList.inc"
+#include "AttrList.inc"
 class ObjCProtocolLoc;
 class ConceptReference;
 
diff --git a/clang/include/clang/AST/ASTNodeTraverser.h b/clang/include/clang/AST/ASTNodeTraverser.h
index 3bc0bdff2..24e361c2e 100644
--- a/clang/include/clang/AST/ASTNodeTraverser.h
+++ b/clang/include/clang/AST/ASTNodeTraverser.h
@@ -988,7 +988,7 @@ public:
   }
 
   // Implements Visit methods for Attrs.
-#include "clang/AST/AttrNodeTraverse.inc"
+#include "AttrNodeTraverse.inc"
 };
 
 } // namespace clang
diff --git a/clang/include/clang/AST/ASTTypeTraits.h b/clang/include/clang/AST/ASTTypeTraits.h
index 3988a1597..ed4441908 100644
--- a/clang/include/clang/AST/ASTTypeTraits.h
+++ b/clang/include/clang/AST/ASTTypeTraits.h
@@ -148,20 +148,20 @@ private:
     NKI_NestedNameSpecifier,
     NKI_Decl,
 #define DECL(DERIVED, BASE) NKI_##DERIVED##Decl,
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
     NKI_Stmt,
 #define STMT(DERIVED, BASE) NKI_##DERIVED,
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
     NKI_Type,
 #define TYPE(DERIVED, BASE) NKI_##DERIVED##Type,
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
     NKI_OMPClause,
 #define GEN_CLANG_CLAUSE_CLASS
 #define CLAUSE_CLASS(Enum, Str, Class) NKI_##Class,
 #include "llvm/Frontend/OpenMP/OMP.inc"
     NKI_Attr,
 #define ATTR(A) NKI_##A##Attr,
-#include "clang/Basic/AttrList.inc"
+#include "AttrList.inc"
     NKI_ObjCProtocolLoc,
     NKI_ConceptReference,
     NKI_NumberOfKinds
@@ -225,16 +225,16 @@ KIND_TO_KIND_ID(ObjCProtocolLoc)
 KIND_TO_KIND_ID(CXXBaseSpecifier)
 KIND_TO_KIND_ID(ConceptReference)
 #define DECL(DERIVED, BASE) KIND_TO_KIND_ID(DERIVED##Decl)
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
 #define STMT(DERIVED, BASE) KIND_TO_KIND_ID(DERIVED)
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
 #define TYPE(DERIVED, BASE) KIND_TO_KIND_ID(DERIVED##Type)
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 #define GEN_CLANG_CLAUSE_CLASS
 #define CLAUSE_CLASS(Enum, Str, Class) KIND_TO_KIND_ID(Class)
 #include "llvm/Frontend/OpenMP/OMP.inc"
 #define ATTR(A) KIND_TO_KIND_ID(A##Attr)
-#include "clang/Basic/AttrList.inc"
+#include "AttrList.inc"
 #undef KIND_TO_KIND_ID
 
 inline raw_ostream &operator<<(raw_ostream &OS, ASTNodeKind K) {
diff --git a/clang/include/clang/AST/AbstractBasicReader.h b/clang/include/clang/AST/AbstractBasicReader.h
index 4b627c65e..c56c32c21 100644
--- a/clang/include/clang/AST/AbstractBasicReader.h
+++ b/clang/include/clang/AST/AbstractBasicReader.h
@@ -98,7 +98,7 @@ template <class T> inline T *makePointerFromOptional(std::optional<T *> value) {
 // };
 
 // The actual classes are auto-generated; see ClangASTPropertiesEmitter.cpp.
-#include "clang/AST/AbstractBasicReader.inc"
+#include "AbstractBasicReader.inc"
 
 /// DataStreamBasicReader provides convenience implementations for many
 /// BasicReader methods based on the assumption that the
diff --git a/clang/include/clang/AST/AbstractBasicWriter.h b/clang/include/clang/AST/AbstractBasicWriter.h
index b941add8b..da7114c11 100644
--- a/clang/include/clang/AST/AbstractBasicWriter.h
+++ b/clang/include/clang/AST/AbstractBasicWriter.h
@@ -95,7 +95,7 @@ template <class T> inline std::optional<T *> makeOptionalFromPointer(T *value) {
 // };
 
 // The actual classes are auto-generated; see ClangASTPropertiesEmitter.cpp.
-#include "clang/AST/AbstractBasicWriter.inc"
+#include "AbstractBasicWriter.inc"
 
 /// DataStreamBasicWriter provides convenience implementations for many
 /// BasicWriter methods based on the assumption that the
diff --git a/clang/include/clang/AST/AbstractTypeReader.h b/clang/include/clang/AST/AbstractTypeReader.h
index e44bbf61c..d61eef738 100644
--- a/clang/include/clang/AST/AbstractTypeReader.h
+++ b/clang/include/clang/AST/AbstractTypeReader.h
@@ -24,7 +24,7 @@ namespace serialization {
 // };
 //
 // The actual class is auto-generated; see ClangASTPropertiesEmitter.cpp.
-#include "clang/AST/AbstractTypeReader.inc"
+#include "AbstractTypeReader.inc"
 
 } // end namespace serialization
 } // end namespace clang
diff --git a/clang/include/clang/AST/AbstractTypeWriter.h b/clang/include/clang/AST/AbstractTypeWriter.h
index 62006ef0f..1aa9eb8f6 100644
--- a/clang/include/clang/AST/AbstractTypeWriter.h
+++ b/clang/include/clang/AST/AbstractTypeWriter.h
@@ -24,7 +24,7 @@ namespace serialization {
 // };
 //
 // The actual class is auto-generated; see ClangASTPropertiesEmitter.cpp.
-#include "clang/AST/AbstractTypeWriter.inc"
+#include "AbstractTypeWriter.inc"
 
 } // end namespace serialization
 } // end namespace clang
diff --git a/clang/include/clang/AST/Attr.h b/clang/include/clang/AST/Attr.h
index 3365ebe4d..1fcea1813 100644
--- a/clang/include/clang/AST/Attr.h
+++ b/clang/include/clang/AST/Attr.h
@@ -247,7 +247,7 @@ public:
   static bool classof(const Attr *A) {
     return A->getKind() >= attr::FirstParameterABIAttr &&
            A->getKind() <= attr::LastParameterABIAttr;
-   }
+  }
 };
 
 /// A single parameter index whose accessors require each use to make explicit
@@ -261,8 +261,7 @@ class ParamIdx {
   unsigned IsValid : 1;
 
   void assertComparable(const ParamIdx &I) const {
-    assert(isValid() && I.isValid() &&
-           "ParamIdx must be valid to be compared");
+    assert(isValid() && I.isValid() && "ParamIdx must be valid to be compared");
     // It's possible to compare indices from separate functions, but so far
     // it's not proven useful.  Moreover, it might be confusing because a
     // comparison on the results of getASTIndex might be inconsistent with a
@@ -377,7 +376,7 @@ public:
 static_assert(sizeof(ParamIdx) == sizeof(ParamIdx::SerialType),
               "ParamIdx does not fit its serialization type");
 
-#include "clang/AST/Attrs.inc" // IWYU pragma: export
+#include "Attrs.inc"
 
 inline const StreamingDiagnostic &operator<<(const StreamingDiagnostic &DB,
                                              const Attr *At) {
@@ -407,6 +406,6 @@ inline ParameterABI ParameterABIAttr::getABI() const {
     llvm_unreachable("bad parameter ABI attribute kind");
   }
 }
-}  // end namespace clang
+} // end namespace clang
 
 #endif
diff --git a/clang/include/clang/AST/AttrVisitor.h b/clang/include/clang/AST/AttrVisitor.h
index d271db010..89ae72fc0 100644
--- a/clang/include/clang/AST/AttrVisitor.h
+++ b/clang/include/clang/AST/AttrVisitor.h
@@ -34,7 +34,7 @@ public:
 #define ATTR(NAME)                                                             \
   case attr::NAME:                                                             \
     DISPATCH(NAME##Attr);
-#include "clang/Basic/AttrList.inc"
+#include "AttrList.inc"
     }
     llvm_unreachable("Attr that isn't part of AttrList.inc!");
   }
@@ -43,7 +43,7 @@ public:
   // method, fall back to the parent.
 #define ATTR(NAME)                                                             \
   RetTy Visit##NAME##Attr(PTR(NAME##Attr) A) { DISPATCH(Attr); }
-#include "clang/Basic/AttrList.inc"
+#include "AttrList.inc"
 
   RetTy VisitAttr(PTR(Attr)) { return RetTy(); }
 
diff --git a/clang/include/clang/AST/Comment.h b/clang/include/clang/AST/Comment.h
index dd9906727..0a60bbc76 100644
--- a/clang/include/clang/AST/Comment.h
+++ b/clang/include/clang/AST/Comment.h
@@ -57,7 +57,7 @@ enum class CommentKind {
 #define LAST_COMMENT_RANGE(BASE, FIRST, LAST)                                  \
   First##BASE##Constant = FIRST, Last##BASE##Constant = LAST
 #define ABSTRACT_COMMENT(COMMENT)
-#include "clang/AST/CommentNodes.inc"
+#include "CommentNodes.inc"
 };
 
 /// Any part of the comment.
diff --git a/clang/include/clang/AST/CommentCommandTraits.h b/clang/include/clang/AST/CommentCommandTraits.h
index 78c484fff..fbf6c7b5b 100644
--- a/clang/include/clang/AST/CommentCommandTraits.h
+++ b/clang/include/clang/AST/CommentCommandTraits.h
@@ -150,7 +150,7 @@ class CommandTraits {
 public:
   enum KnownCommandIDs {
 #define COMMENT_COMMAND(NAME) KCI_##NAME,
-#include "clang/AST/CommentCommandList.inc"
+#include "CommentCommandList.inc"
 #undef COMMENT_COMMAND
     KCI_Last
   };
diff --git a/clang/include/clang/AST/CommentVisitor.h b/clang/include/clang/AST/CommentVisitor.h
index bbb624a23..c09a569a0 100644
--- a/clang/include/clang/AST/CommentVisitor.h
+++ b/clang/include/clang/AST/CommentVisitor.h
@@ -34,7 +34,7 @@ public:
 #define COMMENT(CLASS, PARENT)                                                 \
   case CommentKind::CLASS:                                                     \
     DISPATCH(CLASS, CLASS);
-#include "clang/AST/CommentNodes.inc"
+#include "CommentNodes.inc"
 #undef ABSTRACT_COMMENT
 #undef COMMENT
     }
@@ -45,7 +45,7 @@ public:
 #define ABSTRACT_COMMENT(COMMENT) COMMENT
 #define COMMENT(CLASS, PARENT)                                                 \
   RetTy visit##CLASS(PTR(CLASS) C, ParamTys... P) { DISPATCH(PARENT, PARENT); }
-#include "clang/AST/CommentNodes.inc"
+#include "CommentNodes.inc"
 #undef ABSTRACT_COMMENT
 #undef COMMENT
 
diff --git a/clang/include/clang/AST/DeclBase.h b/clang/include/clang/AST/DeclBase.h
index 648dae283..088e8b20c 100644
--- a/clang/include/clang/AST/DeclBase.h
+++ b/clang/include/clang/AST/DeclBase.h
@@ -93,7 +93,7 @@ public:
         first##BASE = START, last##BASE = END,
 #define LAST_DECL_RANGE(BASE, START, END) \
         first##BASE = START, last##BASE = END
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
   };
 
   /// A placeholder type used to construct an empty shell of a
diff --git a/clang/include/clang/AST/DeclVisitor.h b/clang/include/clang/AST/DeclVisitor.h
index 8690cdda4..80b293b01 100644
--- a/clang/include/clang/AST/DeclVisitor.h
+++ b/clang/include/clang/AST/DeclVisitor.h
@@ -39,7 +39,7 @@ public:
 #define DECL(DERIVED, BASE) \
       case Decl::DERIVED: DISPATCH(DERIVED##Decl, DERIVED##Decl);
 #define ABSTRACT_DECL(DECL)
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
     }
     llvm_unreachable("Decl that isn't part of DeclNodes.inc!");
   }
@@ -48,7 +48,7 @@ public:
   // method, fall back to the parent.
 #define DECL(DERIVED, BASE) \
   RetTy Visit##DERIVED##Decl(PTR(DERIVED##Decl) D) { DISPATCH(BASE, BASE); }
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
 
   RetTy VisitDecl(PTR(Decl) D) { return RetTy(); }
 
diff --git a/clang/include/clang/AST/DynamicRecursiveASTVisitor.h b/clang/include/clang/AST/DynamicRecursiveASTVisitor.h
index 4382d2099..3676d5907 100644
--- a/clang/include/clang/AST/DynamicRecursiveASTVisitor.h
+++ b/clang/include/clang/AST/DynamicRecursiveASTVisitor.h
@@ -233,32 +233,32 @@ public:
   // Decls.
 #define ABSTRACT_DECL(DECL)
 #define DECL(CLASS, BASE) virtual bool Traverse##CLASS##Decl(CLASS##Decl *D);
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
 
 #define DECL(CLASS, BASE)                                                      \
   bool WalkUpFrom##CLASS##Decl(CLASS##Decl *D);                                \
   virtual bool Visit##CLASS##Decl(CLASS##Decl *D) { return true; }
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
 
   // Stmts.
 #define ABSTRACT_STMT(STMT)
 #define STMT(CLASS, PARENT) virtual bool Traverse##CLASS(CLASS *S);
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
 
 #define STMT(CLASS, PARENT)                                                    \
   bool WalkUpFrom##CLASS(CLASS *S);                                            \
   virtual bool Visit##CLASS(CLASS *S) { return true; }
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
 
   // Types.
 #define ABSTRACT_TYPE(CLASS, BASE)
 #define TYPE(CLASS, BASE) virtual bool Traverse##CLASS##Type(CLASS##Type *T);
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 
 #define TYPE(CLASS, BASE)                                                      \
   bool WalkUpFrom##CLASS##Type(CLASS##Type *T);                                \
   virtual bool Visit##CLASS##Type(CLASS##Type *T) { return true; }
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 
   // TypeLocs.
 #define ABSTRACT_TYPELOC(CLASS, BASE)
diff --git a/clang/include/clang/AST/Expr.h b/clang/include/clang/AST/Expr.h
index 7be402264..698b52c20 100644
--- a/clang/include/clang/AST/Expr.h
+++ b/clang/include/clang/AST/Expr.h
@@ -6680,7 +6680,7 @@ public:
   enum AtomicOp {
 #define BUILTIN(ID, TYPE, ATTRS)
 #define ATOMIC_BUILTIN(ID, TYPE, ATTRS) AO ## ID,
-#include "clang/Basic/Builtins.inc"
+#include "Builtins.inc"
     // Avoid trailing comma
     BI_First = 0
   };
@@ -6746,7 +6746,7 @@ public:
 #define ATOMIC_BUILTIN(ID, TYPE, ATTRS)                                        \
   case AO##ID:                                                                 \
     return #ID;
-#include "clang/Basic/Builtins.inc"
+#include "Builtins.inc"
     }
     llvm_unreachable("not an atomic operator?");
   }
diff --git a/clang/include/clang/AST/RecursiveASTVisitor.h b/clang/include/clang/AST/RecursiveASTVisitor.h
index c4a1d03f1..573650413 100644
--- a/clang/include/clang/AST/RecursiveASTVisitor.h
+++ b/clang/include/clang/AST/RecursiveASTVisitor.h
@@ -329,7 +329,7 @@ public:
 
 // Declare Traverse* and empty Visit* for all Attr classes.
 #define ATTR_VISITOR_DECLS_ONLY
-#include "clang/AST/AttrVisitor.inc"
+#include "AttrVisitor.inc"
 #undef ATTR_VISITOR_DECLS_ONLY
 
 // ---- Methods on Stmts ----
@@ -368,7 +368,7 @@ public:
 #define ABSTRACT_STMT(STMT)
 #define STMT(CLASS, PARENT) \
   bool Traverse##CLASS(CLASS *S, DataRecursionQueue *Queue = nullptr);
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
   // The above header #undefs ABSTRACT_STMT and STMT upon exit.
 
   // Define WalkUpFrom*() and empty Visit*() for all Stmt classes.
@@ -381,7 +381,7 @@ public:
     return true;                                                               \
   }                                                                            \
   bool Visit##CLASS(CLASS *S) { return true; }
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
 
 // ---- Methods on Types ----
 // FIXME: revamp to take TypeLoc's rather than Types.
@@ -389,7 +389,7 @@ public:
 // Declare Traverse*() for all concrete Type classes.
 #define ABSTRACT_TYPE(CLASS, BASE)
 #define TYPE(CLASS, BASE) bool Traverse##CLASS##Type(CLASS##Type *T);
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
   // The above header #undefs ABSTRACT_TYPE and TYPE upon exit.
 
   // Define WalkUpFrom*() and empty Visit*() for all Type classes.
@@ -402,7 +402,7 @@ public:
     return true;                                                               \
   }                                                                            \
   bool Visit##CLASS##Type(CLASS##Type *T) { return true; }
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 
 // ---- Methods on TypeLocs ----
 // FIXME: this currently just calls the matching Type methods
@@ -436,14 +436,14 @@ public:
     return true;                                                               \
   }                                                                            \
   bool Visit##CLASS##TypeLoc(CLASS##TypeLoc TL) { return true; }
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 
 // ---- Methods on Decls ----
 
 // Declare Traverse*() for all concrete Decl classes.
 #define ABSTRACT_DECL(DECL)
 #define DECL(CLASS, BASE) bool Traverse##CLASS##Decl(CLASS##Decl *D);
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
   // The above header #undefs ABSTRACT_DECL and DECL upon exit.
 
   // Define WalkUpFrom*() and empty Visit*() for all Decl classes.
@@ -456,7 +456,7 @@ public:
     return true;                                                               \
   }                                                                            \
   bool Visit##CLASS##Decl(CLASS##Decl *D) { return true; }
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
 
   bool canIgnoreChildDeclWhileTraversingDeclContext(const Decl *Child);
 
@@ -563,7 +563,7 @@ bool RecursiveASTVisitor<Derived>::dataTraverseNode(Stmt *S,
 #define STMT(CLASS, PARENT)                                                    \
   case Stmt::CLASS##Class:                                                     \
     return TRAVERSE_STMT_BASE(CLASS, CLASS, S, Queue);
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
   }
 
   return true;
@@ -645,7 +645,7 @@ bool RecursiveASTVisitor<Derived>::PostVisitStmt(Stmt *S) {
         TRY_TO(WalkUpFrom##CLASS(Sem));                                        \
     }                                                                          \
     break;
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
   }
 
   return true;
@@ -707,7 +707,7 @@ bool RecursiveASTVisitor<Derived>::TraverseType(QualType T) {
   case Type::CLASS:                                                            \
     return getDerived().Traverse##CLASS##Type(                                 \
         static_cast<CLASS##Type *>(const_cast<Type *>(T.getTypePtr())));
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
   }
 
   return true;
@@ -731,7 +731,7 @@ bool RecursiveASTVisitor<Derived>::TraverseTypeLoc(TypeLoc TL) {
 
 // Define the Traverse*Attr(Attr* A) methods
 #define VISITORCLASS RecursiveASTVisitor
-#include "clang/AST/AttrVisitor.inc"
+#include "AttrVisitor.inc"
 #undef VISITORCLASS
 
 template <typename Derived>
@@ -771,7 +771,7 @@ bool RecursiveASTVisitor<Derived>::TraverseDecl(Decl *D) {
     if (!getDerived().Traverse##CLASS##Decl(static_cast<CLASS##Decl *>(D)))    \
       return false;                                                            \
     break;
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
   }
   return true;
 }
diff --git a/clang/include/clang/AST/Stmt.h b/clang/include/clang/AST/Stmt.h
index 405c6166a..f9fa7205d 100644
--- a/clang/include/clang/AST/Stmt.h
+++ b/clang/include/clang/AST/Stmt.h
@@ -91,7 +91,7 @@ public:
 #define LAST_STMT_RANGE(BASE, FIRST, LAST) \
         first##BASE##Constant=FIRST##Class, last##BASE##Constant=LAST##Class
 #define ABSTRACT_STMT(STMT)
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
   };
 
   // Make vanilla 'new' and 'delete' illegal for Stmts.
diff --git a/clang/include/clang/AST/StmtVisitor.h b/clang/include/clang/AST/StmtVisitor.h
index 8b7b728de..ea2a3fe33 100644
--- a/clang/include/clang/AST/StmtVisitor.h
+++ b/clang/include/clang/AST/StmtVisitor.h
@@ -109,7 +109,7 @@ public:
 #define ABSTRACT_STMT(STMT)
 #define STMT(CLASS, PARENT)                              \
     case Stmt::CLASS ## Class: DISPATCH(CLASS, CLASS);
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
     }
   }
 
@@ -117,7 +117,7 @@ public:
   // back on VisitExpr or whatever else is the superclass.
 #define STMT(CLASS, PARENT)                                   \
   RetTy Visit ## CLASS(PTR(CLASS) S, ParamTys... P) { DISPATCH(PARENT, PARENT); }
-#include "clang/AST/StmtNodes.inc"
+#include "StmtNodes.inc"
 
   // If the implementation doesn't implement binary operator methods, fall back
   // on VisitBinaryOperator.
diff --git a/clang/include/clang/AST/TextNodeDumper.h b/clang/include/clang/AST/TextNodeDumper.h
index 4aaae48ba..3a9137711 100644
--- a/clang/include/clang/AST/TextNodeDumper.h
+++ b/clang/include/clang/AST/TextNodeDumper.h
@@ -242,7 +242,7 @@ public:
                                 const comments::FullComment *);
 
 // Implements Visit methods for Attrs.
-#include "clang/AST/AttrTextNodeDump.inc"
+#include "AttrTextNodeDump.inc"
 
   void VisitNullTemplateArgument(const TemplateArgument &TA);
   void VisitTypeTemplateArgument(const TemplateArgument &TA);
diff --git a/clang/include/clang/AST/Type.h b/clang/include/clang/AST/Type.h
index 1d9743520..2c4b1c71b 100644
--- a/clang/include/clang/AST/Type.h
+++ b/clang/include/clang/AST/Type.h
@@ -71,43 +71,37 @@ class TemplateParameterList;
 class Type;
 class Attr;
 
-enum {
-  TypeAlignmentInBits = 4,
-  TypeAlignment = 1 << TypeAlignmentInBits
-};
+enum { TypeAlignmentInBits = 4, TypeAlignment = 1 << TypeAlignmentInBits };
 
 namespace serialization {
-  template <class T> class AbstractTypeReader;
-  template <class T> class AbstractTypeWriter;
-}
+template <class T> class AbstractTypeReader;
+template <class T> class AbstractTypeWriter;
+} // namespace serialization
 
 } // namespace clang
 
 namespace llvm {
 
-  template <typename T>
-  struct PointerLikeTypeTraits;
-  template<>
-  struct PointerLikeTypeTraits< ::clang::Type*> {
-    static inline void *getAsVoidPointer(::clang::Type *P) { return P; }
+template <typename T> struct PointerLikeTypeTraits;
+template <> struct PointerLikeTypeTraits<::clang::Type *> {
+  static inline void *getAsVoidPointer(::clang::Type *P) { return P; }
 
-    static inline ::clang::Type *getFromVoidPointer(void *P) {
-      return static_cast< ::clang::Type*>(P);
-    }
+  static inline ::clang::Type *getFromVoidPointer(void *P) {
+    return static_cast<::clang::Type *>(P);
+  }
 
-    static constexpr int NumLowBitsAvailable = clang::TypeAlignmentInBits;
-  };
+  static constexpr int NumLowBitsAvailable = clang::TypeAlignmentInBits;
+};
 
-  template<>
-  struct PointerLikeTypeTraits< ::clang::ExtQuals*> {
-    static inline void *getAsVoidPointer(::clang::ExtQuals *P) { return P; }
+template <> struct PointerLikeTypeTraits<::clang::ExtQuals *> {
+  static inline void *getAsVoidPointer(::clang::ExtQuals *P) { return P; }
 
-    static inline ::clang::ExtQuals *getFromVoidPointer(void *P) {
-      return static_cast< ::clang::ExtQuals*>(P);
-    }
+  static inline ::clang::ExtQuals *getFromVoidPointer(void *P) {
+    return static_cast<::clang::ExtQuals *>(P);
+  }
 
-    static constexpr int NumLowBitsAvailable = clang::TypeAlignmentInBits;
-  };
+  static constexpr int NumLowBitsAvailable = clang::TypeAlignmentInBits;
+};
 
 } // namespace llvm
 
@@ -145,7 +139,7 @@ using CanQualType = CanQual<Type>;
 
 // Provide forward declarations for all of the *Type classes.
 #define TYPE(Class, Base) class Class##Type;
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 
 /// Pointer-authentication qualifiers.
 class PointerAuthQualifier {
@@ -332,11 +326,7 @@ public:
     CVRMask = Const | Volatile | Restrict
   };
 
-  enum GC {
-    GCNone = 0,
-    Weak,
-    Strong
-  };
+  enum GC { GCNone = 0, Weak, Strong };
 
   enum ObjCLifetime {
     /// There is no lifetime qualification on this type.
@@ -489,9 +479,7 @@ public:
     assert(!(mask & ~CVRMask) && "bitmask contains non-CVR bits");
     Mask &= ~static_cast<uint64_t>(mask);
   }
-  void removeCVRQualifiers() {
-    removeCVRQualifiers(CVRMask);
-  }
+  void removeCVRQualifiers() { removeCVRQualifiers(CVRMask); }
   void addCVRQualifiers(unsigned mask) {
     assert(!(mask & ~CVRMask) && "bitmask contains non-CVR bits");
     Mask |= mask;
@@ -502,9 +490,7 @@ public:
   }
 
   bool hasUnaligned() const { return Mask & UMask; }
-  void setUnaligned(bool flag) {
-    Mask = (Mask & ~UMask) | (flag ? UMask : 0);
-  }
+  void setUnaligned(bool flag) { Mask = (Mask & ~UMask) | (flag ? UMask : 0); }
   void removeUnaligned() { Mask &= ~UMask; }
   void addUnaligned() { Mask |= UMask; }
 
@@ -583,8 +569,8 @@ public:
   }
   void setAddressSpace(LangAS space) {
     assert((unsigned)space <= MaxAddressSpace);
-    Mask = (Mask & ~AddressSpaceMask)
-         | (((uint32_t) space) << AddressSpaceShift);
+    Mask =
+        (Mask & ~AddressSpaceMask) | (((uint32_t)space) << AddressSpaceShift);
   }
   void removeAddressSpace() { setAddressSpace(LangAS::Default); }
   void addAddressSpace(LangAS space) {
@@ -618,9 +604,7 @@ public:
     assert(!(mask & ~FastMask) && "bitmask contains non-fast qualifier bits");
     Mask &= ~static_cast<uint64_t>(mask);
   }
-  void removeFastQualifiers() {
-    removeFastQualifiers(FastMask);
-  }
+  void removeFastQualifiers() { removeFastQualifiers(FastMask); }
   void addFastQualifiers(unsigned mask) {
     assert(!(mask & ~FastMask) && "bitmask contains non-fast qualifier bits");
     Mask |= mask;
@@ -680,12 +664,12 @@ public:
   /// Add the qualifiers from the given set to this set, given that
   /// they don't conflict.
   void addConsistentQualifiers(Qualifiers qs) {
-    assert(getAddressSpace() == qs.getAddressSpace() ||
-           !hasAddressSpace() || !qs.hasAddressSpace());
-    assert(getObjCGCAttr() == qs.getObjCGCAttr() ||
-           !hasObjCGCAttr() || !qs.hasObjCGCAttr());
-    assert(getObjCLifetime() == qs.getObjCLifetime() ||
-           !hasObjCLifetime() || !qs.hasObjCLifetime());
+    assert(getAddressSpace() == qs.getAddressSpace() || !hasAddressSpace() ||
+           !qs.hasAddressSpace());
+    assert(getObjCGCAttr() == qs.getObjCGCAttr() || !hasObjCGCAttr() ||
+           !qs.hasObjCGCAttr());
+    assert(getObjCLifetime() == qs.getObjCLifetime() || !hasObjCLifetime() ||
+           !qs.hasObjCLifetime());
     assert(!hasPointerAuth() || !qs.hasPointerAuth() ||
            getPointerAuth() == qs.getPointerAuth());
     Mask |= qs.Mask;
@@ -872,7 +856,7 @@ struct SplitQualType {
   SplitQualType getSingleStepDesugaredType() const; // end of this file
 
   // Make std::tie work.
-  std::pair<const Type *,Qualifiers> asPair() const {
+  std::pair<const Type *, Qualifiers> asPair() const {
     return std::pair<const Type *, Qualifiers>(Ty, Quals);
   }
 
@@ -931,7 +915,8 @@ class QualType {
 
   // Thankfully, these are efficiently composable.
   llvm::PointerIntPair<llvm::PointerUnion<const Type *, const ExtQuals *>,
-                       Qualifiers::FastWidth> Value;
+                       Qualifiers::FastWidth>
+      Value;
 
   const ExtQuals *getExtQualsUnsafe() const {
     return cast<const ExtQuals *>(Value.getPointer());
@@ -945,7 +930,7 @@ class QualType {
     assert(!isNull() && "Cannot retrieve a NULL type pointer");
     auto CommonPtrVal = reinterpret_cast<uintptr_t>(Value.getOpaqueValue());
     CommonPtrVal &= ~(uintptr_t)((1 << TypeAlignmentInBits) - 1);
-    return reinterpret_cast<ExtQualsTypeCommonBase*>(CommonPtrVal);
+    return reinterpret_cast<ExtQualsTypeCommonBase *>(CommonPtrVal);
   }
 
 public:
@@ -977,25 +962,19 @@ public:
 
   static QualType getFromOpaquePtr(const void *Ptr) {
     QualType T;
-    T.Value.setFromOpaqueValue(const_cast<void*>(Ptr));
+    T.Value.setFromOpaqueValue(const_cast<void *>(Ptr));
     return T;
   }
 
-  const Type &operator*() const {
-    return *getTypePtr();
-  }
+  const Type &operator*() const { return *getTypePtr(); }
 
-  const Type *operator->() const {
-    return getTypePtr();
-  }
+  const Type *operator->() const { return getTypePtr(); }
 
   bool isCanonical() const;
   bool isCanonicalAsParam() const;
 
   /// Return true if this QualType doesn't point to a type yet.
-  bool isNull() const {
-    return Value.getPointer().isNull();
-  }
+  bool isNull() const { return Value.getPointer().isNull(); }
 
   // Determines if a type can form `T&`.
   bool isReferenceable() const;
@@ -1078,15 +1057,13 @@ public:
   /// Retrieve the set of CVR (const-volatile-restrict) qualifiers
   /// local to this particular QualType instance, not including any qualifiers
   /// acquired through typedefs or other sugar.
-  unsigned getLocalCVRQualifiers() const {
-    return getLocalFastQualifiers();
-  }
+  unsigned getLocalCVRQualifiers() const { return getLocalFastQualifiers(); }
 
   /// Retrieve the set of CVR (const-volatile-restrict) qualifiers
   /// applied to this type.
   unsigned getCVRQualifiers() const;
 
-  bool isConstant(const ASTContext& Ctx) const {
+  bool isConstant(const ASTContext &Ctx) const {
     return QualType::isConstant(*this, Ctx);
   }
 
@@ -1148,25 +1125,17 @@ public:
   // easily added.
 
   /// Add the `const` type qualifier to this QualType.
-  void addConst() {
-    addFastQualifiers(Qualifiers::Const);
-  }
-  QualType withConst() const {
-    return withFastQualifiers(Qualifiers::Const);
-  }
+  void addConst() { addFastQualifiers(Qualifiers::Const); }
+  QualType withConst() const { return withFastQualifiers(Qualifiers::Const); }
 
   /// Add the `volatile` type qualifier to this QualType.
-  void addVolatile() {
-    addFastQualifiers(Qualifiers::Volatile);
-  }
+  void addVolatile() { addFastQualifiers(Qualifiers::Volatile); }
   QualType withVolatile() const {
     return withFastQualifiers(Qualifiers::Volatile);
   }
 
   /// Add the `restrict` qualifier to this QualType.
-  void addRestrict() {
-    addFastQualifiers(Qualifiers::Restrict);
-  }
+  void addRestrict() { addFastQualifiers(Qualifiers::Restrict); }
   QualType withRestrict() const {
     return withFastQualifiers(Qualifiers::Restrict);
   }
@@ -1176,8 +1145,8 @@ public:
   }
 
   void addFastQualifiers(unsigned TQs) {
-    assert(!(TQs & ~Qualifiers::FastMask)
-           && "non-fast qualifier bits set in mask!");
+    assert(!(TQs & ~Qualifiers::FastMask) &&
+           "non-fast qualifier bits set in mask!");
     Value.setInt(Value.getInt() | TQs);
   }
 
@@ -1344,9 +1313,8 @@ public:
     return print(split.Ty, split.Quals, OS, policy, PlaceHolder, Indentation);
   }
 
-  static void print(const Type *ty, Qualifiers qs,
-                    raw_ostream &OS, const PrintingPolicy &policy,
-                    const Twine &PlaceHolder,
+  static void print(const Type *ty, Qualifiers qs, raw_ostream &OS,
+                    const PrintingPolicy &policy, const Twine &PlaceHolder,
                     unsigned Indentation = 0);
 
   void getAsStringInternal(std::string &Str,
@@ -1420,14 +1388,10 @@ public:
   inline Qualifiers::GC getObjCGCAttr() const;
 
   /// true when Type is objc's weak.
-  bool isObjCGCWeak() const {
-    return getObjCGCAttr() == Qualifiers::Weak;
-  }
+  bool isObjCGCWeak() const { return getObjCGCAttr() == Qualifiers::Weak; }
 
   /// true when Type is objc's strong.
-  bool isObjCGCStrong() const {
-    return getObjCGCAttr() == Qualifiers::Strong;
-  }
+  bool isObjCGCStrong() const { return getObjCGCAttr() == Qualifiers::Strong; }
 
   /// Returns lifetime attribute of this type.
   Qualifiers::ObjCLifetime getObjCLifetime() const {
@@ -1574,8 +1538,7 @@ public:
   /// \param context The context in which the subject type was written.
   ///
   /// \returns the resulting type.
-  QualType substObjCTypeArgs(ASTContext &ctx,
-                             ArrayRef<QualType> typeArgs,
+  QualType substObjCTypeArgs(ASTContext &ctx, ArrayRef<QualType> typeArgs,
                              ObjCSubstitutionContext context) const;
 
   /// Substitute type arguments from an object type for the Objective-C type
@@ -1598,8 +1561,7 @@ public:
   ///
   /// \returns the subject type after replacing all of the Objective-C type
   /// parameters with their corresponding arguments.
-  QualType substObjCMemberType(QualType objectType,
-                               const DeclContext *dc,
+  QualType substObjCMemberType(QualType objectType, const DeclContext *dc,
                                ObjCSubstitutionContext context) const;
 
   /// Strip Objective-C "__kindof" types from the given type.
@@ -1616,7 +1578,7 @@ private:
   // These methods are implemented in a separate translation unit;
   // "static"-ize them to avoid creating temporary QualTypes in the
   // caller.
-  static bool isConstant(QualType T, const ASTContext& Ctx);
+  static bool isConstant(QualType T, const ASTContext &Ctx);
   static QualType getDesugaredType(QualType T, const ASTContext &Context);
   static SplitQualType getSplitDesugaredType(QualType T);
   static SplitQualType getSplitUnqualifiedTypeImpl(QualType type);
@@ -1626,7 +1588,8 @@ private:
   static DestructionKind isDestructedTypeImpl(QualType type);
 
   /// Check if \param RD is or contains a non-trivial C union.
-  static bool hasNonTrivialToPrimitiveDefaultInitializeCUnion(const RecordDecl *RD);
+  static bool
+  hasNonTrivialToPrimitiveDefaultInitializeCUnion(const RecordDecl *RD);
   static bool hasNonTrivialToPrimitiveDestructCUnion(const RecordDecl *RD);
   static bool hasNonTrivialToPrimitiveCopyCUnion(const RecordDecl *RD);
 };
@@ -1639,7 +1602,7 @@ namespace llvm {
 
 /// Implement simplify_type for QualType, so that we can dyn_cast from QualType
 /// to a specific Type class.
-template<> struct simplify_type< ::clang::QualType> {
+template <> struct simplify_type<::clang::QualType> {
   using SimpleType = const ::clang::Type *;
 
   static SimpleType getSimplifiedValue(::clang::QualType Val) {
@@ -1648,8 +1611,7 @@ template<> struct simplify_type< ::clang::QualType> {
 };
 
 // Teach SmallPtrSet that QualType is "basically a pointer".
-template<>
-struct PointerLikeTypeTraits<clang::QualType> {
+template <> struct PointerLikeTypeTraits<clang::QualType> {
   static inline void *getAsVoidPointer(clang::QualType P) {
     return P.getAsOpaquePtr();
   }
@@ -1725,10 +1687,10 @@ public:
       : ExtQualsTypeCommonBase(baseType,
                                canon.isNull() ? QualType(this_(), 0) : canon),
         Quals(quals) {
-    assert(Quals.hasNonFastQualifiers()
-           && "ExtQuals created with no fast qualifiers");
-    assert(!Quals.hasFastQualifiers()
-           && "ExtQuals created with fast qualifiers");
+    assert(Quals.hasNonFastQualifiers() &&
+           "ExtQuals created with no fast qualifiers");
+    assert(!Quals.hasFastQualifiers() &&
+           "ExtQuals created with fast qualifiers");
   }
 
   Qualifiers getQualifiers() const { return Quals; }
@@ -1751,8 +1713,7 @@ public:
     Profile(ID, getBaseType(), Quals);
   }
 
-  static void Profile(llvm::FoldingSetNodeID &ID,
-                      const Type *BaseType,
+  static void Profile(llvm::FoldingSetNodeID &ID, const Type *BaseType,
                       Qualifiers Quals) {
     assert(!Quals.hasFastQualifiers() && "fast qualifiers in ExtQuals hash!");
     ID.AddPointer(BaseType);
@@ -1831,7 +1792,7 @@ public:
 #define TYPE(Class, Base) Class,
 #define LAST_TYPE(Class) TypeLast = Class
 #define ABSTRACT_TYPE(Class, Base)
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
   };
 
 private:
@@ -1865,9 +1826,7 @@ private:
     LLVM_PREFERRED_TYPE(bool)
     mutable unsigned FromAST : 1;
 
-    bool isCacheValid() const {
-      return CacheValid;
-    }
+    bool isCacheValid() const { return CacheValid; }
 
     Linkage getLinkage() const {
       assert(isCacheValid() && "getting linkage from invalid cache");
@@ -2286,7 +2245,7 @@ protected:
     SubstTemplateTypeParmPackTypeBitfields SubstTemplateTypeParmPackTypeBits;
     TemplateSpecializationTypeBitfields TemplateSpecializationTypeBits;
     DependentTemplateSpecializationTypeBitfields
-      DependentTemplateSpecializationTypeBits;
+        DependentTemplateSpecializationTypeBits;
     PackExpansionTypeBitfields PackExpansionTypeBits;
     CountAttributedTypeBitfields CountAttributedTypeBits;
   };
@@ -2295,9 +2254,7 @@ private:
   template <class T> friend class TypePropertyCache;
 
   /// Set whether this type comes from an AST file.
-  void setFromAST(bool V = true) const {
-    TypeBits.FromAST = V;
-  }
+  void setFromAST(bool V = true) const { TypeBits.FromAST = V; }
 
 protected:
   friend class ASTContext;
@@ -2438,9 +2395,7 @@ public:
 
   /// Return true if this is an incomplete or object
   /// type, in other words, not a function type.
-  bool isIncompleteOrObjectType() const {
-    return !isFunctionType();
-  }
+  bool isIncompleteOrObjectType() const { return !isFunctionType(); }
 
   /// Determine whether this type is an object type.
   bool isObjectType() const {
@@ -2485,7 +2440,7 @@ public:
 
   /// isIntegerType() does *not* include complex integers (a GCC extension).
   /// isComplexIntegerType() can be used to test for complex integers.
-  bool isIntegerType() const;     // C99 6.2.5p17 (int, char, bool, enum)
+  bool isIntegerType() const; // C99 6.2.5p17 (int, char, bool, enum)
   bool isEnumeralType() const;
 
   /// Determine whether this type is a scoped enumeration type.
@@ -2510,21 +2465,21 @@ public:
   bool isRealFloatingType() const; // C99 6.2.5p10 (float, double, long double)
   /// isComplexType() does *not* include complex integers (a GCC extension).
   /// isComplexIntegerType() can be used to test for complex integers.
-  bool isComplexType() const;      // C99 6.2.5p11 (complex)
-  bool isAnyComplexType() const;   // C99 6.2.5p11 (complex) + Complex Int.
-  bool isFloatingType() const;     // C99 6.2.5p11 (real floating + complex)
-  bool isHalfType() const;         // OpenCL 6.1.1.1, NEON (IEEE 754-2008 half)
-  bool isFloat16Type() const;      // C11 extension ISO/IEC TS 18661
+  bool isComplexType() const;    // C99 6.2.5p11 (complex)
+  bool isAnyComplexType() const; // C99 6.2.5p11 (complex) + Complex Int.
+  bool isFloatingType() const;   // C99 6.2.5p11 (real floating + complex)
+  bool isHalfType() const;       // OpenCL 6.1.1.1, NEON (IEEE 754-2008 half)
+  bool isFloat16Type() const;    // C11 extension ISO/IEC TS 18661
   bool isFloat32Type() const;
   bool isDoubleType() const;
   bool isBFloat16Type() const;
   bool isMFloat8Type() const;
   bool isFloat128Type() const;
   bool isIbm128Type() const;
-  bool isRealType() const;         // C99 6.2.5p17 (real floating + integer)
-  bool isArithmeticType() const;   // C99 6.2.5p18 (integer + floating)
-  bool isVoidType() const;         // C99 6.2.5p19
-  bool isScalarType() const;       // C99 6.2.5p21 (arithmetic + pointers)
+  bool isRealType() const;       // C99 6.2.5p17 (real floating + integer)
+  bool isArithmeticType() const; // C99 6.2.5p18 (integer + floating)
+  bool isVoidType() const;       // C99 6.2.5p19
+  bool isScalarType() const;     // C99 6.2.5p21 (arithmetic + pointers)
   bool isAggregateType() const;
   bool isFundamentalType() const;
   bool isCompoundType() const;
@@ -2537,7 +2492,7 @@ public:
   bool isPointerType() const;
   bool isPointerOrReferenceType() const;
   bool isSignableType() const;
-  bool isAnyPointerType() const;   // Any C pointer or ObjC object pointer
+  bool isAnyPointerType() const; // Any C pointer or ObjC object pointer
   bool isCountAttributedType() const;
   bool isBlockPointerType() const;
   bool isVoidPointerType() const;
@@ -2564,28 +2519,30 @@ public:
   bool isInterfaceType() const;
   bool isStructureOrClassType() const;
   bool isUnionType() const;
-  bool isComplexIntegerType() const;            // GCC _Complex integer type.
-  bool isVectorType() const;                    // GCC vector type.
-  bool isExtVectorType() const;                 // Extended vector type.
-  bool isExtVectorBoolType() const;             // Extended vector type with bool element.
+  bool isComplexIntegerType() const; // GCC _Complex integer type.
+  bool isVectorType() const;         // GCC vector type.
+  bool isExtVectorType() const;      // Extended vector type.
+  bool isExtVectorBoolType() const;  // Extended vector type with bool element.
   bool isSubscriptableVectorType() const;
-  bool isMatrixType() const;                    // Matrix type.
-  bool isConstantMatrixType() const;            // Constant matrix type.
-  bool isDependentAddressSpaceType() const;     // value-dependent address space qualifier
-  bool isObjCObjectPointerType() const;         // pointer to ObjC object
-  bool isObjCRetainableType() const;            // ObjC object or block pointer
-  bool isObjCLifetimeType() const;              // (array of)* retainable type
-  bool isObjCIndirectLifetimeType() const;      // (pointer to)* lifetime type
-  bool isObjCNSObjectType() const;              // __attribute__((NSObject))
-  bool isObjCIndependentClassType() const;      // __attribute__((objc_independent_class))
+  bool isMatrixType() const;         // Matrix type.
+  bool isConstantMatrixType() const; // Constant matrix type.
+  bool isDependentAddressSpaceType()
+      const; // value-dependent address space qualifier
+  bool isObjCObjectPointerType() const;    // pointer to ObjC object
+  bool isObjCRetainableType() const;       // ObjC object or block pointer
+  bool isObjCLifetimeType() const;         // (array of)* retainable type
+  bool isObjCIndirectLifetimeType() const; // (pointer to)* lifetime type
+  bool isObjCNSObjectType() const;         // __attribute__((NSObject))
+  bool
+  isObjCIndependentClassType() const; // __attribute__((objc_independent_class))
   // FIXME: change this to 'raw' interface type, so we can used 'interface' type
   // for the common case.
-  bool isObjCObjectType() const;                // NSString or typeof(*(id)0)
-  bool isObjCQualifiedInterfaceType() const;    // NSString<foo>
-  bool isObjCQualifiedIdType() const;           // id<foo>
-  bool isObjCQualifiedClassType() const;        // Class<foo>
+  bool isObjCObjectType() const;             // NSString or typeof(*(id)0)
+  bool isObjCQualifiedInterfaceType() const; // NSString<foo>
+  bool isObjCQualifiedIdType() const;        // id<foo>
+  bool isObjCQualifiedClassType() const;     // Class<foo>
   bool isObjCObjectOrInterfaceType() const;
-  bool isObjCIdType() const;                    // id
+  bool isObjCIdType() const; // id
   bool isDecltypeType() const;
   /// Was this type written with the special inert-in-ARC __unsafe_unretained
   /// qualifier?
@@ -2607,7 +2564,7 @@ public:
   bool isObjCIdOrObjectKindOfType(const ASTContext &ctx,
                                   const ObjCObjectType *&bound) const;
 
-  bool isObjCClassType() const;                 // Class
+  bool isObjCClassType() const; // Class
 
   /// Whether the type is Objective-C 'Class' or a __kindof type of an
   /// Class type, e.g., __kindof Class <NSCopying>.
@@ -2618,47 +2575,46 @@ public:
   bool isObjCClassOrClassKindOfType() const;
 
   bool isBlockCompatibleObjCPointerType(ASTContext &ctx) const;
-  bool isObjCSelType() const;                 // Class
-  bool isObjCBuiltinType() const;               // 'id' or 'Class'
+  bool isObjCSelType() const;     // Class
+  bool isObjCBuiltinType() const; // 'id' or 'Class'
   bool isObjCARCBridgableType() const;
   bool isCARCBridgableType() const;
-  bool isTemplateTypeParmType() const;          // C++ template type parameter
-  bool isNullPtrType() const;                   // C++11 std::nullptr_t or
-                                                // C23   nullptr_t
-  bool isNothrowT() const;                      // C++   std::nothrow_t
-  bool isAlignValT() const;                     // C++17 std::align_val_t
-  bool isStdByteType() const;                   // C++17 std::byte
-  bool isAtomicType() const;                    // C11 _Atomic()
-  bool isUndeducedAutoType() const;             // C++11 auto or
-                                                // C++14 decltype(auto)
-  bool isTypedefNameType() const;               // typedef or alias template
-
-#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
+  bool isTemplateTypeParmType() const; // C++ template type parameter
+  bool isNullPtrType() const;          // C++11 std::nullptr_t or
+                                       // C23   nullptr_t
+  bool isNothrowT() const;             // C++   std::nothrow_t
+  bool isAlignValT() const;            // C++17 std::align_val_t
+  bool isStdByteType() const;          // C++17 std::byte
+  bool isAtomicType() const;           // C11 _Atomic()
+  bool isUndeducedAutoType() const;    // C++11 auto or
+                                       // C++14 decltype(auto)
+  bool isTypedefNameType() const;      // typedef or alias template
+
+#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix)                   \
   bool is##Id##Type() const;
 #include "clang/Basic/OpenCLImageTypes.def"
 
-  bool isImageType() const;                     // Any OpenCL image type
+  bool isImageType() const; // Any OpenCL image type
 
-  bool isSamplerT() const;                      // OpenCL sampler_t
-  bool isEventT() const;                        // OpenCL event_t
-  bool isClkEventT() const;                     // OpenCL clk_event_t
-  bool isQueueT() const;                        // OpenCL queue_t
-  bool isReserveIDT() const;                    // OpenCL reserve_id_t
+  bool isSamplerT() const;   // OpenCL sampler_t
+  bool isEventT() const;     // OpenCL event_t
+  bool isClkEventT() const;  // OpenCL clk_event_t
+  bool isQueueT() const;     // OpenCL queue_t
+  bool isReserveIDT() const; // OpenCL reserve_id_t
 
-#define EXT_OPAQUE_TYPE(ExtType, Id, Ext) \
-  bool is##Id##Type() const;
+#define EXT_OPAQUE_TYPE(ExtType, Id, Ext) bool is##Id##Type() const;
 #include "clang/Basic/OpenCLExtensionTypes.def"
   // Type defined in cl_intel_device_side_avc_motion_estimation OpenCL extension
   bool isOCLIntelSubgroupAVCType() const;
-  bool isOCLExtOpaqueType() const;              // Any OpenCL extension type
+  bool isOCLExtOpaqueType() const; // Any OpenCL extension type
 
-  bool isPipeType() const;                      // OpenCL pipe type
-  bool isBitIntType() const;                    // Bit-precise integer type
-  bool isOpenCLSpecificType() const;            // Any OpenCL specific type
+  bool isPipeType() const;           // OpenCL pipe type
+  bool isBitIntType() const;         // Bit-precise integer type
+  bool isOpenCLSpecificType() const; // Any OpenCL specific type
 
 #define HLSL_INTANGIBLE_TYPE(Name, Id, SingletonId) bool is##Id##Type() const;
 #include "clang/Basic/HLSLIntangibleTypes.def"
-  bool isHLSLSpecificType() const; // Any HLSL specific type
+  bool isHLSLSpecificType() const;          // Any HLSL specific type
   bool isHLSLBuiltinIntangibleType() const; // Any HLSL builtin intangible type
   bool isHLSLAttributedResourceType() const;
   bool isHLSLIntangibleType()
@@ -2987,9 +2943,7 @@ public:
 
   const char *getTypeClassName() const;
 
-  QualType getCanonicalTypeInternal() const {
-    return CanonicalType;
-  }
+  QualType getCanonicalTypeInternal() const { return CanonicalType; }
 
   CanQualType getCanonicalTypeUnqualified() const; // in CanonicalType.h
   void dump() const;
@@ -3021,14 +2975,14 @@ template <> const CountAttributedType *Type::getAs() const;
 // We can do canonical leaf types faster, because we don't have to
 // worry about preserving child type decoration.
 #define TYPE(Class, Base)
-#define LEAF_TYPE(Class) \
-template <> inline const Class##Type *Type::getAs() const { \
-  return dyn_cast<Class##Type>(CanonicalType); \
-} \
-template <> inline const Class##Type *Type::castAs() const { \
-  return cast<Class##Type>(CanonicalType); \
-}
-#include "clang/AST/TypeNodes.inc"
+#define LEAF_TYPE(Class)                                                       \
+  template <> inline const Class##Type *Type::getAs() const {                  \
+    return dyn_cast<Class##Type>(CanonicalType);                               \
+  }                                                                            \
+  template <> inline const Class##Type *Type::castAs() const {                 \
+    return cast<Class##Type>(CanonicalType);                                   \
+  }
+#include "TypeNodes.inc"
 
 /// This class is used for builtin types like 'int'.  Builtin
 /// types are always canonical and have a literal name field.
@@ -3093,9 +3047,7 @@ public:
   bool isSugared() const { return false; }
   QualType desugar() const { return QualType(this, 0); }
 
-  bool isInteger() const {
-    return getKind() >= Bool && getKind() <= Int128;
-  }
+  bool isInteger() const { return getKind() >= Bool && getKind() <= Int128; }
 
   bool isSignedInteger() const {
     return getKind() >= Char_S && getKind() <= Int128;
@@ -3114,16 +3066,12 @@ public:
   bool isSVECount() const { return getKind() == Kind::SveCount; }
 
   /// Determines whether the given kind corresponds to a placeholder type.
-  static bool isPlaceholderTypeKind(Kind K) {
-    return K >= Overload;
-  }
+  static bool isPlaceholderTypeKind(Kind K) { return K >= Overload; }
 
   /// Determines whether this type is a placeholder type, i.e. a type
   /// which cannot appear in arbitrary positions in a fully-formed
   /// expression.
-  bool isPlaceholderType() const {
-    return isPlaceholderTypeKind(getKind());
-  }
+  bool isPlaceholderType() const { return isPlaceholderTypeKind(getKind()); }
 
   /// Determines whether this type is a placeholder type other than
   /// Overload.  Most placeholder types require only syntactic
@@ -3134,9 +3082,7 @@ public:
   /// from their context, like whether the context expects a
   /// specific function-pointer type, and so frequently need
   /// special treatment.
-  bool isNonOverloadPlaceholderType() const {
-    return getKind() > Overload;
-  }
+  bool isNonOverloadPlaceholderType() const { return getKind() > Overload; }
 
   static bool classof(const Type *T) { return T->getTypeClass() == Builtin; }
 };
@@ -3158,9 +3104,7 @@ public:
   bool isSugared() const { return false; }
   QualType desugar() const { return QualType(this, 0); }
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getElementType());
-  }
+  void Profile(llvm::FoldingSetNodeID &ID) { Profile(ID, getElementType()); }
 
   static void Profile(llvm::FoldingSetNodeID &ID, QualType Element) {
     ID.AddPointer(Element.getAsOpaquePtr());
@@ -3184,9 +3128,7 @@ public:
   bool isSugared() const { return true; }
   QualType desugar() const { return getInnerType(); }
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getInnerType());
-  }
+  void Profile(llvm::FoldingSetNodeID &ID) { Profile(ID, getInnerType()); }
 
   static void Profile(llvm::FoldingSetNodeID &ID, QualType Inner) {
     Inner.Profile(ID);
@@ -3211,9 +3153,7 @@ public:
   bool isSugared() const { return false; }
   QualType desugar() const { return QualType(this, 0); }
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getPointeeType());
-  }
+  void Profile(llvm::FoldingSetNodeID &ID) { Profile(ID, getPointeeType()); }
 
   static void Profile(llvm::FoldingSetNodeID &ID, QualType Pointee) {
     ID.AddPointer(Pointee.getAsOpaquePtr());
@@ -3392,8 +3332,8 @@ public:
 class DecayedType : public AdjustedType {
   friend class ASTContext; // ASTContext creates these.
 
-  inline
-  DecayedType(QualType OriginalType, QualType Decayed, QualType Canonical);
+  inline DecayedType(QualType OriginalType, QualType Decayed,
+                     QualType Canonical);
 
 public:
   QualType getDecayedType() const { return getAdjustedType(); }
@@ -3423,12 +3363,10 @@ public:
   bool isSugared() const { return false; }
   QualType desugar() const { return QualType(this, 0); }
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-      Profile(ID, getPointeeType());
-  }
+  void Profile(llvm::FoldingSetNodeID &ID) { Profile(ID, getPointeeType()); }
 
   static void Profile(llvm::FoldingSetNodeID &ID, QualType Pointee) {
-      ID.AddPointer(Pointee.getAsOpaquePtr());
+    ID.AddPointer(Pointee.getAsOpaquePtr());
   }
 
   static bool classof(const Type *T) {
@@ -3467,8 +3405,7 @@ public:
     Profile(ID, PointeeType, isSpelledAsLValue());
   }
 
-  static void Profile(llvm::FoldingSetNodeID &ID,
-                      QualType Referencee,
+  static void Profile(llvm::FoldingSetNodeID &ID, QualType Referencee,
                       bool SpelledAsLValue) {
     ID.AddPointer(Referencee.getAsOpaquePtr());
     ID.AddBoolean(SpelledAsLValue);
@@ -3503,7 +3440,7 @@ class RValueReferenceType : public ReferenceType {
   friend class ASTContext; // ASTContext creates these
 
   RValueReferenceType(QualType Referencee, QualType CanonicalRef)
-       : ReferenceType(RValueReference, Referencee, CanonicalRef, false) {}
+      : ReferenceType(RValueReference, Referencee, CanonicalRef, false) {}
 
 public:
   bool isSugared() const { return false; }
@@ -3765,8 +3702,8 @@ public:
 class IncompleteArrayType : public ArrayType {
   friend class ASTContext; // ASTContext creates these.
 
-  IncompleteArrayType(QualType et, QualType can,
-                      ArraySizeModifier sm, unsigned tq)
+  IncompleteArrayType(QualType et, QualType can, ArraySizeModifier sm,
+                      unsigned tq)
       : ArrayType(IncompleteArray, et, can, sm, tq) {}
 
 public:
@@ -3816,11 +3753,10 @@ class VariableArrayType : public ArrayType {
   /// The range spanned by the left and right array brackets.
   SourceRange Brackets;
 
-  VariableArrayType(QualType et, QualType can, Expr *e,
-                    ArraySizeModifier sm, unsigned tq,
-                    SourceRange brackets)
-      : ArrayType(VariableArray, et, can, sm, tq, e),
-        SizeExpr((Stmt*) e), Brackets(brackets) {}
+  VariableArrayType(QualType et, QualType can, Expr *e, ArraySizeModifier sm,
+                    unsigned tq, SourceRange brackets)
+      : ArrayType(VariableArray, et, can, sm, tq, e), SizeExpr((Stmt *)e),
+        Brackets(brackets) {}
 
 public:
   friend class StmtIteratorBase;
@@ -3828,7 +3764,7 @@ public:
   Expr *getSizeExpr() const {
     // We use C-style casts instead of cast<> here because we do not wish
     // to have a dependency of Type.h on Stmt.h/Expr.h.
-    return (Expr*) SizeExpr;
+    return (Expr *)SizeExpr;
   }
 
   SourceRange getBracketsRange() const { return Brackets; }
@@ -3883,7 +3819,7 @@ public:
   Expr *getSizeExpr() const {
     // We use C-style casts instead of cast<> here because we do not wish
     // to have a dependency of Type.h on Stmt.h/Expr.h.
-    return (Expr*) SizeExpr;
+    return (Expr *)SizeExpr;
   }
 
   SourceRange getBracketsRange() const { return Brackets; }
@@ -3898,8 +3834,8 @@ public:
   }
 
   void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context) {
-    Profile(ID, Context, getElementType(),
-            getSizeModifier(), getIndexTypeCVRQualifiers(), getSizeExpr());
+    Profile(ID, Context, getElementType(), getSizeModifier(),
+            getIndexTypeCVRQualifiers(), getSizeExpr());
   }
 
   static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,
@@ -3909,7 +3845,8 @@ public:
 
 /// Represents an extended address space qualifier where the input address space
 /// value is dependent. Non-dependent address spaces are not represented with a
-/// special Type subclass; they are stored on an ExtQuals node as part of a QualType.
+/// special Type subclass; they are stored on an ExtQuals node as part of a
+/// QualType.
 ///
 /// For example:
 /// \code
@@ -4057,8 +3994,8 @@ public:
   }
 
   void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getElementType(), getNumElements(),
-            getTypeClass(), getVectorKind());
+    Profile(ID, getElementType(), getNumElements(), getTypeClass(),
+            getVectorKind());
   }
 
   static void Profile(llvm::FoldingSetNodeID &ID, QualType ElementType,
@@ -4134,39 +4071,65 @@ class ExtVectorType : public VectorType {
 public:
   static int getPointAccessorIdx(char c) {
     switch (c) {
-    default: return -1;
-    case 'x': case 'r': return 0;
-    case 'y': case 'g': return 1;
-    case 'z': case 'b': return 2;
-    case 'w': case 'a': return 3;
+    default:
+      return -1;
+    case 'x':
+    case 'r':
+      return 0;
+    case 'y':
+    case 'g':
+      return 1;
+    case 'z':
+    case 'b':
+      return 2;
+    case 'w':
+    case 'a':
+      return 3;
     }
   }
 
   static int getNumericAccessorIdx(char c) {
     switch (c) {
-      default: return -1;
-      case '0': return 0;
-      case '1': return 1;
-      case '2': return 2;
-      case '3': return 3;
-      case '4': return 4;
-      case '5': return 5;
-      case '6': return 6;
-      case '7': return 7;
-      case '8': return 8;
-      case '9': return 9;
-      case 'A':
-      case 'a': return 10;
-      case 'B':
-      case 'b': return 11;
-      case 'C':
-      case 'c': return 12;
-      case 'D':
-      case 'd': return 13;
-      case 'E':
-      case 'e': return 14;
-      case 'F':
-      case 'f': return 15;
+    default:
+      return -1;
+    case '0':
+      return 0;
+    case '1':
+      return 1;
+    case '2':
+      return 2;
+    case '3':
+      return 3;
+    case '4':
+      return 4;
+    case '5':
+      return 5;
+    case '6':
+      return 6;
+    case '7':
+      return 7;
+    case '8':
+      return 8;
+    case '9':
+      return 9;
+    case 'A':
+    case 'a':
+      return 10;
+    case 'B':
+    case 'b':
+      return 11;
+    case 'C':
+    case 'c':
+      return 12;
+    case 'D':
+    case 'd':
+      return 13;
+    case 'E':
+    case 'e':
+      return 14;
+    case 'F':
+    case 'f':
+      return 15;
     }
   }
 
@@ -4178,17 +4141,15 @@ public:
   }
 
   bool isAccessorWithinNumElements(char c, bool isNumericAccessor) const {
-    if (int idx = getAccessorIdx(c, isNumericAccessor)+1)
-      return unsigned(idx-1) < getNumElements();
+    if (int idx = getAccessorIdx(c, isNumericAccessor) + 1)
+      return unsigned(idx - 1) < getNumElements();
     return false;
   }
 
   bool isSugared() const { return false; }
   QualType desugar() const { return QualType(this, 0); }
 
-  static bool classof(const Type *T) {
-    return T->getTypeClass() == ExtVector;
-  }
+  static bool classof(const Type *T) { return T->getTypeClass() == ExtVector; }
 };
 
 /// Represents a matrix type, as defined in the Matrix Types clang extensions.
@@ -4445,10 +4406,7 @@ public:
     enum { NoReturnMask = 0x20 };
     enum { ProducesResultMask = 0x40 };
     enum { NoCallerSavedRegsMask = 0x80 };
-    enum {
-      RegParmMask =  0x700,
-      RegParmOffset = 8
-    };
+    enum { RegParmMask = 0x700, RegParmOffset = 8 };
     enum { NoCfCheckMask = 0x800 };
     enum { CmseNSCallMask = 0x1000 };
     uint16_t Bits = CC_C;
@@ -4483,7 +4441,9 @@ public:
     bool getCmseNSCall() const { return Bits & CmseNSCallMask; }
     bool getNoCallerSavedRegs() const { return Bits & NoCallerSavedRegsMask; }
     bool getNoCfCheck() const { return Bits & NoCfCheckMask; }
-    bool getHasRegParm() const { return ((Bits & RegParmMask) >> RegParmOffset) != 0; }
+    bool getHasRegParm() const {
+      return ((Bits & RegParmMask) >> RegParmOffset) != 0;
+    }
 
     unsigned getRegParm() const {
       unsigned RegParm = (Bits & RegParmMask) >> RegParmOffset;
@@ -4494,12 +4454,8 @@ public:
 
     CallingConv getCC() const { return CallingConv(Bits & CallConvMask); }
 
-    bool operator==(ExtInfo Other) const {
-      return Bits == Other.Bits;
-    }
-    bool operator!=(ExtInfo Other) const {
-      return Bits != Other.Bits;
-    }
+    bool operator==(ExtInfo Other) const { return Bits == Other.Bits; }
+    bool operator!=(ExtInfo Other) const { return Bits != Other.Bits; }
 
     // Note that we don't have setters. That is by design, use
     // the following with methods instead of mutating these objects.
@@ -4541,23 +4497,22 @@ public:
 
     ExtInfo withRegParm(unsigned RegParm) const {
       assert(RegParm < 7 && "Invalid regparm value");
-      return ExtInfo((Bits & ~RegParmMask) |
-                     ((RegParm + 1) << RegParmOffset));
+      return ExtInfo((Bits & ~RegParmMask) | ((RegParm + 1) << RegParmOffset));
     }
 
     ExtInfo withCallingConv(CallingConv cc) const {
-      return ExtInfo((Bits & ~CallConvMask) | (unsigned) cc);
+      return ExtInfo((Bits & ~CallConvMask) | (unsigned)cc);
     }
 
-    void Profile(llvm::FoldingSetNodeID &ID) const {
-      ID.AddInteger(Bits);
-    }
+    void Profile(llvm::FoldingSetNodeID &ID) const { ID.AddInteger(Bits); }
   };
 
   /// A simple holder for a QualType representing a type in an
   /// exception specification. Unfortunately needed by FunctionProtoType
   /// because TrailingObjects cannot handle repeated types.
-  struct ExceptionType { QualType Type; };
+  struct ExceptionType {
+    QualType Type;
+  };
 
   /// A simple holder for various uncommon bits which do not fit in
   /// FunctionTypeBitfields. Aligned to alignof(void *) to maintain the
@@ -5344,7 +5299,6 @@ private:
             FunctionTypeBits.HasExtraBitfields) &&
            "ExtraBitfields are required for given ExceptionSpecType");
     return FunctionTypeBits.HasExtraBitfields;
-
   }
 
   bool hasArmTypeAttributes() const {
@@ -5353,9 +5307,7 @@ private:
                ->HasArmTypeAttributes;
   }
 
-  bool hasExtQualifiers() const {
-    return FunctionTypeBits.HasExtQuals;
-  }
+  bool hasExtQualifiers() const { return FunctionTypeBits.HasExtQuals; }
 
 public:
   unsigned getNumParams() const { return FunctionTypeBits.NumParams; }
@@ -5690,9 +5642,7 @@ public:
     return T->getTypeClass() == UnresolvedUsing;
   }
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    return Profile(ID, Decl);
-  }
+  void Profile(llvm::FoldingSetNodeID &ID) { return Profile(ID, Decl); }
 
   static void Profile(llvm::FoldingSetNodeID &ID,
                       UnresolvedUsingTypenameDecl *D) {
@@ -6080,13 +6030,13 @@ protected:
   friend class ASTContext; // ASTContext creates these.
 
   explicit RecordType(const RecordDecl *D)
-      : TagType(Record, reinterpret_cast<const TagDecl*>(D), QualType()) {}
+      : TagType(Record, reinterpret_cast<const TagDecl *>(D), QualType()) {}
   explicit RecordType(TypeClass TC, RecordDecl *D)
-      : TagType(TC, reinterpret_cast<const TagDecl*>(D), QualType()) {}
+      : TagType(TC, reinterpret_cast<const TagDecl *>(D), QualType()) {}
 
 public:
   RecordDecl *getDecl() const {
-    return reinterpret_cast<RecordDecl*>(TagType::getDecl());
+    return reinterpret_cast<RecordDecl *>(TagType::getDecl());
   }
 
   /// Recursively check all fields in the record for const-ness. If any field
@@ -6105,11 +6055,11 @@ class EnumType : public TagType {
   friend class ASTContext; // ASTContext creates these.
 
   explicit EnumType(const EnumDecl *D)
-      : TagType(Enum, reinterpret_cast<const TagDecl*>(D), QualType()) {}
+      : TagType(Enum, reinterpret_cast<const TagDecl *>(D), QualType()) {}
 
 public:
   EnumDecl *getDecl() const {
-    return reinterpret_cast<EnumDecl*>(TagType::getDecl());
+    return reinterpret_cast<EnumDecl *>(TagType::getDecl());
   }
 
   bool isSugared() const { return false; }
@@ -6215,9 +6165,7 @@ public:
     ID.AddPointer(attr);
   }
 
-  static bool classof(const Type *T) {
-    return T->getTypeClass() == Attributed;
-  }
+  static bool classof(const Type *T) { return T->getTypeClass() == Attributed; }
 };
 
 class BTFTagAttributedType : public Type, public llvm::FoldingSetNode {
@@ -6578,9 +6526,7 @@ public:
     return TypeConstraintConcept;
   }
 
-  bool isConstrained() const {
-    return TypeConstraintConcept != nullptr;
-  }
+  bool isConstrained() const { return TypeConstraintConcept != nullptr; }
 
   bool isDecltypeAuto() const {
     return getKeyword() == AutoTypeKeyword::DecltypeAuto;
@@ -6600,9 +6546,7 @@ public:
                       bool IsDependent, ConceptDecl *CD,
                       ArrayRef<TemplateArgument> Arguments);
 
-  static bool classof(const Type *T) {
-    return T->getTypeClass() == Auto;
-  }
+  static bool classof(const Type *T) { return T->getTypeClass() == Auto; }
 };
 
 /// Represents a C++17 deduced template specialization type.
@@ -6626,7 +6570,7 @@ class DeducedTemplateSpecializationType : public DeducedType,
 
 public:
   /// Retrieve the name of the template that we are deducing.
-  TemplateName getTemplateName() const { return Template;}
+  TemplateName getTemplateName() const { return Template; }
 
   void Profile(llvm::FoldingSetNodeID &ID) const {
     Profile(ID, getTemplateName(), getDeducedType(), isDependentType());
@@ -6676,10 +6620,8 @@ class TemplateSpecializationType : public Type, public llvm::FoldingSetNode {
   /// replacement must, recursively, be one of these).
   TemplateName Template;
 
-  TemplateSpecializationType(TemplateName T,
-                             ArrayRef<TemplateArgument> Args,
-                             QualType Canon,
-                             QualType Aliased);
+  TemplateSpecializationType(TemplateName T, ArrayRef<TemplateArgument> Args,
+                             QualType Canon, QualType Aliased);
 
 public:
   /// Determine whether any of the given template arguments are dependent.
@@ -6757,8 +6699,7 @@ public:
 
 /// Print a template argument list, including the '<' and '>'
 /// enclosing the template arguments.
-void printTemplateArgumentList(raw_ostream &OS,
-                               ArrayRef<TemplateArgument> Args,
+void printTemplateArgumentList(raw_ostream &OS, ArrayRef<TemplateArgument> Args,
                                const PrintingPolicy &Policy,
                                const TemplateParameterList *TPL = nullptr);
 
@@ -7052,9 +6993,7 @@ public:
 
   /// Retrieve the identifier that terminates this type name.
   /// For example, "type" in "typename T::type".
-  const IdentifierInfo *getIdentifier() const {
-    return Name;
-  }
+  const IdentifierInfo *getIdentifier() const { return Name; }
 
   bool isSugared() const { return false; }
   QualType desugar() const { return QualType(this, 0); }
@@ -7110,8 +7049,7 @@ public:
     Profile(ID, Context, getKeyword(), NNS, Name, template_arguments());
   }
 
-  static void Profile(llvm::FoldingSetNodeID &ID,
-                      const ASTContext &Context,
+  static void Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,
                       ElaboratedTypeKeyword Keyword,
                       NestedNameSpecifier *Qualifier,
                       const IdentifierInfo *Name,
@@ -7197,21 +7135,20 @@ public:
 
 /// This class wraps the list of protocol qualifiers. For types that can
 /// take ObjC protocol qualifers, they can subclass this class.
-template <class T>
-class ObjCProtocolQualifiers {
+template <class T> class ObjCProtocolQualifiers {
 protected:
   ObjCProtocolQualifiers() = default;
 
-  ObjCProtocolDecl * const *getProtocolStorage() const {
-    return const_cast<ObjCProtocolQualifiers*>(this)->getProtocolStorage();
+  ObjCProtocolDecl *const *getProtocolStorage() const {
+    return const_cast<ObjCProtocolQualifiers *>(this)->getProtocolStorage();
   }
 
   ObjCProtocolDecl **getProtocolStorage() {
-    return static_cast<T*>(this)->getProtocolStorageImpl();
+    return static_cast<T *>(this)->getProtocolStorageImpl();
   }
 
   void setNumProtocols(unsigned N) {
-    static_cast<T*>(this)->setNumProtocolsImpl(N);
+    static_cast<T *>(this)->setNumProtocolsImpl(N);
   }
 
   void initialize(ArrayRef<ObjCProtocolDecl *> protocols) {
@@ -7220,11 +7157,11 @@ protected:
            "bitfield overflow in protocol count");
     if (!protocols.empty())
       memcpy(getProtocolStorage(), protocols.data(),
-             protocols.size() * sizeof(ObjCProtocolDecl*));
+             protocols.size() * sizeof(ObjCProtocolDecl *));
   }
 
 public:
-  using qual_iterator = ObjCProtocolDecl * const *;
+  using qual_iterator = ObjCProtocolDecl *const *;
   using qual_range = llvm::iterator_range<qual_iterator>;
 
   qual_range quals() const { return qual_range(qual_begin(), qual_end()); }
@@ -7236,7 +7173,7 @@ public:
   /// Return the number of qualifying protocols in this type, or 0 if
   /// there are none.
   unsigned getNumProtocols() const {
-    return static_cast<const T*>(this)->getNumProtocolsImpl();
+    return static_cast<const T *>(this)->getNumProtocolsImpl();
   }
 
   /// Fetch a protocol by index.
@@ -7271,16 +7208,11 @@ class ObjCTypeParamType : public Type,
 
   /// Return the number of qualifying protocols in this interface type,
   /// or 0 if there are none.
-  unsigned getNumProtocolsImpl() const {
-    return NumProtocols;
-  }
+  unsigned getNumProtocolsImpl() const { return NumProtocols; }
 
-  void setNumProtocolsImpl(unsigned N) {
-    NumProtocols = N;
-  }
+  void setNumProtocolsImpl(unsigned N) { NumProtocols = N; }
 
-  ObjCTypeParamType(const ObjCTypeParamDecl *D,
-                    QualType can,
+  ObjCTypeParamType(const ObjCTypeParamDecl *D, QualType can,
                     ArrayRef<ObjCProtocolDecl *> protocols);
 
 public:
@@ -7293,8 +7225,7 @@ public:
 
   void Profile(llvm::FoldingSetNodeID &ID);
   static void Profile(llvm::FoldingSetNodeID &ID,
-                      const ObjCTypeParamDecl *OTPDecl,
-                      QualType CanonicalType,
+                      const ObjCTypeParamDecl *OTPDecl, QualType CanonicalType,
                       ArrayRef<ObjCProtocolDecl *> protocols);
 
   ObjCTypeParamDecl *getDecl() const { return OTPDecl; }
@@ -7349,7 +7280,7 @@ class ObjCObjectType : public Type,
 
   /// Cached superclass type.
   mutable llvm::PointerIntPair<const ObjCObjectType *, 1, bool>
-    CachedSuperClassType;
+      CachedSuperClassType;
 
   QualType *getTypeArgStorage();
   const QualType *getTypeArgStorage() const {
@@ -7362,17 +7293,13 @@ class ObjCObjectType : public Type,
   unsigned getNumProtocolsImpl() const {
     return ObjCObjectTypeBits.NumProtocols;
   }
-  void setNumProtocolsImpl(unsigned N) {
-    ObjCObjectTypeBits.NumProtocols = N;
-  }
+  void setNumProtocolsImpl(unsigned N) { ObjCObjectTypeBits.NumProtocols = N; }
 
 protected:
   enum Nonce_ObjCInterface { Nonce_ObjCInterface };
 
-  ObjCObjectType(QualType Canonical, QualType Base,
-                 ArrayRef<QualType> typeArgs,
-                 ArrayRef<ObjCProtocolDecl *> protocols,
-                 bool isKindOf);
+  ObjCObjectType(QualType Canonical, QualType Base, ArrayRef<QualType> typeArgs,
+                 ArrayRef<ObjCProtocolDecl *> protocols, bool isKindOf);
 
   ObjCObjectType(enum Nonce_ObjCInterface)
       : Type(ObjCInterface, QualType(), TypeDependence::None),
@@ -7404,7 +7331,8 @@ public:
   bool isObjCUnqualifiedId() const { return qual_empty() && isObjCId(); }
   bool isObjCUnqualifiedClass() const { return qual_empty() && isObjCClass(); }
   bool isObjCUnqualifiedIdOrClass() const {
-    if (!qual_empty()) return false;
+    if (!qual_empty())
+      return false;
     if (const BuiltinType *T = getBaseType()->getAs<BuiltinType>())
       return T->getKind() == BuiltinType::ObjCId ||
              T->getKind() == BuiltinType::ObjCClass;
@@ -7488,31 +7416,29 @@ class ObjCObjectTypeImpl : public ObjCObjectType, public llvm::FoldingSetNode {
 
   ObjCObjectTypeImpl(QualType Canonical, QualType Base,
                      ArrayRef<QualType> typeArgs,
-                     ArrayRef<ObjCProtocolDecl *> protocols,
-                     bool isKindOf)
+                     ArrayRef<ObjCProtocolDecl *> protocols, bool isKindOf)
       : ObjCObjectType(Canonical, Base, typeArgs, protocols, isKindOf) {}
 
 public:
   void Profile(llvm::FoldingSetNodeID &ID);
-  static void Profile(llvm::FoldingSetNodeID &ID,
-                      QualType Base,
+  static void Profile(llvm::FoldingSetNodeID &ID, QualType Base,
                       ArrayRef<QualType> typeArgs,
-                      ArrayRef<ObjCProtocolDecl *> protocols,
-                      bool isKindOf);
+                      ArrayRef<ObjCProtocolDecl *> protocols, bool isKindOf);
 };
 
 inline QualType *ObjCObjectType::getTypeArgStorage() {
-  return reinterpret_cast<QualType *>(static_cast<ObjCObjectTypeImpl*>(this)+1);
+  return reinterpret_cast<QualType *>(static_cast<ObjCObjectTypeImpl *>(this) +
+                                      1);
 }
 
 inline ObjCProtocolDecl **ObjCObjectType::getProtocolStorageImpl() {
-    return reinterpret_cast<ObjCProtocolDecl**>(
-             getTypeArgStorage() + ObjCObjectTypeBits.NumTypeArgs);
+  return reinterpret_cast<ObjCProtocolDecl **>(getTypeArgStorage() +
+                                               ObjCObjectTypeBits.NumTypeArgs);
 }
 
 inline ObjCProtocolDecl **ObjCTypeParamType::getProtocolStorageImpl() {
-    return reinterpret_cast<ObjCProtocolDecl**>(
-             static_cast<ObjCTypeParamType*>(this)+1);
+  return reinterpret_cast<ObjCProtocolDecl **>(
+      static_cast<ObjCTypeParamType *>(this) + 1);
 }
 
 /// Interfaces are the core concept in Objective-C for object oriented design.
@@ -7536,7 +7462,7 @@ class ObjCInterfaceType : public ObjCObjectType {
 
   ObjCInterfaceType(const ObjCInterfaceDecl *D)
       : ObjCObjectType(Nonce_ObjCInterface),
-        Decl(const_cast<ObjCInterfaceDecl*>(D)) {}
+        Decl(const_cast<ObjCInterfaceDecl *>(D)) {}
 
 public:
   /// Get the declaration of this interface.
@@ -7553,13 +7479,7 @@ public:
   // class.  People asking for protocols on an ObjCInterfaceType are
   // not going to get what they want: ObjCInterfaceTypes are
   // guaranteed to have no protocols.
-  enum {
-    qual_iterator,
-    qual_begin,
-    qual_end,
-    getNumProtocols,
-    getProtocol
-  };
+  enum { qual_iterator, qual_begin, qual_end, getNumProtocols, getProtocol };
 };
 
 inline ObjCInterfaceDecl *ObjCObjectType::getInterface() const {
@@ -7641,9 +7561,7 @@ public:
 
   /// True if this is equivalent to the 'id' type, i.e. if
   /// its object type is the primitive 'id' type with no protocols.
-  bool isObjCIdType() const {
-    return getObjectType()->isObjCUnqualifiedId();
-  }
+  bool isObjCIdType() const { return getObjectType()->isObjCUnqualifiedId(); }
 
   /// True if this is equivalent to the 'Class' type,
   /// i.e. if its object tive is the primitive 'Class' type with no protocols.
@@ -7704,13 +7622,9 @@ public:
 
   qual_range quals() const { return qual_range(qual_begin(), qual_end()); }
 
-  qual_iterator qual_begin() const {
-    return getObjectType()->qual_begin();
-  }
+  qual_iterator qual_begin() const { return getObjectType()->qual_begin(); }
 
-  qual_iterator qual_end() const {
-    return getObjectType()->qual_end();
-  }
+  qual_iterator qual_end() const { return getObjectType()->qual_end(); }
 
   bool qual_empty() const { return getObjectType()->qual_empty(); }
 
@@ -7737,12 +7651,10 @@ public:
 
   /// Strip off the Objective-C "kindof" type and (with it) any
   /// protocol qualifiers.
-  const ObjCObjectPointerType *stripObjCKindOfTypeAndQuals(
-                                 const ASTContext &ctx) const;
+  const ObjCObjectPointerType *
+  stripObjCKindOfTypeAndQuals(const ASTContext &ctx) const;
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getPointeeType());
-  }
+  void Profile(llvm::FoldingSetNodeID &ID) { Profile(ID, getPointeeType()); }
 
   static void Profile(llvm::FoldingSetNodeID &ID, QualType T) {
     ID.AddPointer(T.getAsOpaquePtr());
@@ -7769,17 +7681,13 @@ public:
   bool isSugared() const { return false; }
   QualType desugar() const { return QualType(this, 0); }
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getValueType());
-  }
+  void Profile(llvm::FoldingSetNodeID &ID) { Profile(ID, getValueType()); }
 
   static void Profile(llvm::FoldingSetNodeID &ID, QualType T) {
     ID.AddPointer(T.getAsOpaquePtr());
   }
 
-  static bool classof(const Type *T) {
-    return T->getTypeClass() == Atomic;
-  }
+  static bool classof(const Type *T) { return T->getTypeClass() == Atomic; }
 };
 
 /// PipeType - OpenCL20.
@@ -7809,9 +7717,7 @@ public:
     ID.AddBoolean(isRead);
   }
 
-  static bool classof(const Type *T) {
-    return T->getTypeClass() == Pipe;
-  }
+  static bool classof(const Type *T) { return T->getTypeClass() == Pipe; }
 
   bool isReadOnly() const { return isRead; }
 };
@@ -7849,7 +7755,7 @@ public:
 
 class DependentBitIntType final : public Type, public llvm::FoldingSetNode {
   friend class ASTContext;
-  llvm::PointerIntPair<Expr*, 1, bool> ExprAndUnsigned;
+  llvm::PointerIntPair<Expr *, 1, bool> ExprAndUnsigned;
 
 protected:
   DependentBitIntType(bool IsUnsigned, Expr *NumBits);
@@ -7895,7 +7801,7 @@ public:
   QualType apply(const ASTContext &Context, QualType QT) const;
 
   /// Apply the collected qualifiers to the given type.
-  QualType apply(const ASTContext &Context, const Type* T) const;
+  QualType apply(const ASTContext &Context, const Type *T) const;
 };
 
 /// A container of type source information.
@@ -7929,7 +7835,7 @@ public:
 
 inline SplitQualType SplitQualType::getSingleStepDesugaredType() const {
   SplitQualType desugar =
-    Ty->getLocallyUnqualifiedSingleStepDesugaredType().split();
+      Ty->getLocallyUnqualifiedSingleStepDesugaredType().split();
   desugar.Quals.addConsistentQualifiers(Quals);
   return desugar;
 }
@@ -7996,8 +7902,10 @@ inline bool QualType::isCanonical() const {
 }
 
 inline bool QualType::isCanonicalAsParam() const {
-  if (!isCanonical()) return false;
-  if (hasLocalQualifiers()) return false;
+  if (!isCanonical())
+    return false;
+  if (hasLocalQualifiers())
+    return false;
 
   const Type *T = getTypePtr();
   if (T->isVariablyModifiedType() && T->hasSizedVLAType())
@@ -8017,7 +7925,6 @@ inline bool QualType::isRestrictQualified() const {
          getCommonPtr()->CanonicalType.isLocalRestrictQualified();
 }
 
-
 inline bool QualType::isVolatileQualified() const {
   return isLocalVolatileQualified() ||
          getCommonPtr()->CanonicalType.isLocalVolatileQualified();
@@ -8153,8 +8060,7 @@ inline bool QualType::isCForbiddenLValueType() const {
 ///
 /// \returns True for types specified in C++0x [basic.fundamental].
 inline bool Type::isFundamentalType() const {
-  return isVoidType() ||
-         isNullPtrType() ||
+  return isVoidType() || isNullPtrType() ||
          // FIXME: It's really annoying that we don't have an
          // 'isArithmeticType()' which agrees with the standard definition.
          (isArithmeticType() && !isEnumeralType());
@@ -8168,20 +8074,23 @@ inline bool Type::isCompoundType() const {
   //   Compound types can be constructed in the following ways:
   //    -- arrays of objects of a given type [...];
   return isArrayType() ||
-  //    -- functions, which have parameters of given types [...];
+         //    -- functions, which have parameters of given types [...];
          isFunctionType() ||
-  //    -- pointers to void or objects or functions [...];
+         //    -- pointers to void or objects or functions [...];
          isPointerType() ||
-  //    -- references to objects or functions of a given type. [...]
+         //    -- references to objects or functions of a given type. [...]
          isReferenceType() ||
-  //    -- classes containing a sequence of objects of various types, [...];
+         //    -- classes containing a sequence of objects of various types,
+         //    [...];
          isRecordType() ||
-  //    -- unions, which are classes capable of containing objects of different
-  //               types at different times;
+         //    -- unions, which are classes capable of containing objects of
+         //    different
+         //               types at different times;
          isUnionType() ||
-  //    -- enumerations, which comprise a set of named constant values. [...];
+         //    -- enumerations, which comprise a set of named constant values.
+         //    [...];
          isEnumeralType() ||
-  //    -- pointers to non-static class members, [...].
+         //    -- pointers to non-static class members, [...].
          isMemberPointerType();
 }
 
@@ -8261,9 +8170,7 @@ inline bool Type::isMemberDataPointerType() const {
     return false;
 }
 
-inline bool Type::isArrayType() const {
-  return isa<ArrayType>(CanonicalType);
-}
+inline bool Type::isArrayType() const { return isa<ArrayType>(CanonicalType); }
 
 inline bool Type::isConstantArrayType() const {
   return isa<ConstantArrayType>(CanonicalType);
@@ -8341,7 +8248,7 @@ inline bool Type::isObjCObjectType() const {
 
 inline bool Type::isObjCObjectOrInterfaceType() const {
   return isa<ObjCInterfaceType>(CanonicalType) ||
-    isa<ObjCObjectType>(CanonicalType);
+         isa<ObjCObjectType>(CanonicalType);
 }
 
 inline bool Type::isAtomicType() const {
@@ -8386,13 +8293,11 @@ inline bool Type::isObjCBuiltinType() const {
   return isObjCIdType() || isObjCClassType() || isObjCSelType();
 }
 
-inline bool Type::isDecltypeType() const {
-  return isa<DecltypeType>(this);
-}
+inline bool Type::isDecltypeType() const { return isa<DecltypeType>(this); }
 
-#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
-  inline bool Type::is##Id##Type() const { \
-    return isSpecificBuiltinType(BuiltinType::Id); \
+#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix)                   \
+  inline bool Type::is##Id##Type() const {                                     \
+    return isSpecificBuiltinType(BuiltinType::Id);                             \
   }
 #include "clang/Basic/OpenCLImageTypes.def"
 
@@ -8423,33 +8328,31 @@ inline bool Type::isImageType() const {
       false; // end boolean or operation
 }
 
-inline bool Type::isPipeType() const {
-  return isa<PipeType>(CanonicalType);
-}
+inline bool Type::isPipeType() const { return isa<PipeType>(CanonicalType); }
 
 inline bool Type::isBitIntType() const {
   return isa<BitIntType>(CanonicalType);
 }
 
-#define EXT_OPAQUE_TYPE(ExtType, Id, Ext) \
-  inline bool Type::is##Id##Type() const { \
-    return isSpecificBuiltinType(BuiltinType::Id); \
+#define EXT_OPAQUE_TYPE(ExtType, Id, Ext)                                      \
+  inline bool Type::is##Id##Type() const {                                     \
+    return isSpecificBuiltinType(BuiltinType::Id);                             \
   }
 #include "clang/Basic/OpenCLExtensionTypes.def"
 
 inline bool Type::isOCLIntelSubgroupAVCType() const {
-#define INTEL_SUBGROUP_AVC_TYPE(ExtType, Id) \
+#define INTEL_SUBGROUP_AVC_TYPE(ExtType, Id)                                   \
   isOCLIntelSubgroupAVC##Id##Type() ||
   return
 #include "clang/Basic/OpenCLExtensionTypes.def"
-    false; // end of boolean or operation
+      false; // end of boolean or operation
 }
 
 inline bool Type::isOCLExtOpaqueType() const {
 #define EXT_OPAQUE_TYPE(ExtType, Id, Ext) is##Id##Type() ||
   return
 #include "clang/Basic/OpenCLExtensionTypes.def"
-    false; // end of boolean or operation
+      false; // end of boolean or operation
 }
 
 inline bool Type::isOpenCLSpecificType() const {
@@ -8503,7 +8406,7 @@ inline const BuiltinType *Type::getAsPlaceholderType() const {
 }
 
 inline bool Type::isSpecificPlaceholderType(unsigned K) const {
-  assert(BuiltinType::isPlaceholderTypeKind((BuiltinType::Kind) K));
+  assert(BuiltinType::isPlaceholderTypeKind((BuiltinType::Kind)K));
   return isSpecificBuiltinType(K);
 }
 
@@ -8565,7 +8468,7 @@ inline bool Type::isIntegerType() const {
     // Incomplete enum types are not treated as integer types.
     // FIXME: In C++, enum types are never integer types.
     return IsEnumDeclComplete(ET->getDecl()) &&
-      !IsEnumDeclScoped(ET->getDecl());
+           !IsEnumDeclScoped(ET->getDecl());
   }
   return isBitIntType();
 }
@@ -8628,8 +8531,7 @@ inline bool Type::isScalarType() const {
          isa<BlockPointerType>(CanonicalType) ||
          isa<MemberPointerType>(CanonicalType) ||
          isa<ComplexType>(CanonicalType) ||
-         isa<ObjCObjectPointerType>(CanonicalType) ||
-         isBitIntType();
+         isa<ObjCObjectPointerType>(CanonicalType) || isBitIntType();
 }
 
 inline bool Type::isIntegralOrEnumerationType() const {
@@ -8756,7 +8658,8 @@ template <typename T> const T *Type::getAs() const {
 }
 
 template <typename T> const T *Type::getAsAdjusted() const {
-  static_assert(!TypeIsArrayType<T>::value, "ArrayType cannot be used with getAsAdjusted!");
+  static_assert(!TypeIsArrayType<T>::value,
+                "ArrayType cannot be used with getAsAdjusted!");
 
   // If this is directly a T type, return it.
   if (const auto *Ty = dyn_cast<T>(this))
@@ -8811,14 +8714,16 @@ template <typename T> const T *Type::castAs() const {
   static_assert(!TypeIsArrayType<T>::value,
                 "ArrayType cannot be used with castAs!");
 
-  if (const auto *ty = dyn_cast<T>(this)) return ty;
+  if (const auto *ty = dyn_cast<T>(this))
+    return ty;
   assert(isa<T>(CanonicalType));
   return cast<T>(getUnqualifiedDesugaredType());
 }
 
 inline const ArrayType *Type::castAsArrayTypeUnsafe() const {
   assert(isa<ArrayType>(CanonicalType));
-  if (const auto *arr = dyn_cast<ArrayType>(this)) return arr;
+  if (const auto *arr = dyn_cast<ArrayType>(this))
+    return arr;
   return cast<ArrayType>(getUnqualifiedDesugaredType());
 }
 
diff --git a/clang/include/clang/AST/TypeLoc.h b/clang/include/clang/AST/TypeLoc.h
index a55a38335..04a2cb140 100644
--- a/clang/include/clang/AST/TypeLoc.h
+++ b/clang/include/clang/AST/TypeLoc.h
@@ -109,7 +109,7 @@ public:
 #define ABSTRACT_TYPE(Class, Base)
 #define TYPE(Class, Base) \
     Class = Type::Class,
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
     Qualified
   };
 
diff --git a/clang/include/clang/AST/TypeLocNodes.def b/clang/include/clang/AST/TypeLocNodes.def
index 81448c7e7..69a48df0b 100644
--- a/clang/include/clang/AST/TypeLocNodes.def
+++ b/clang/include/clang/AST/TypeLocNodes.def
@@ -31,7 +31,7 @@
 TYPELOC(Qualified, TypeLoc)
 #define TYPE(Class, Base) UNQUAL_TYPELOC(Class, Base##Loc)
 #define ABSTRACT_TYPE(Class, Base) ABSTRACT_TYPELOC(Class, Base##Loc)
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 
 #undef DECLARATOR_TYPELOC
 #undef TYPESPEC_TYPELOC
diff --git a/clang/include/clang/AST/TypeVisitor.h b/clang/include/clang/AST/TypeVisitor.h
index 17301835f..e0aea24b7 100644
--- a/clang/include/clang/AST/TypeVisitor.h
+++ b/clang/include/clang/AST/TypeVisitor.h
@@ -70,7 +70,7 @@ public:
     switch (T->getTypeClass()) {
 #define ABSTRACT_TYPE(CLASS, PARENT)
 #define TYPE(CLASS, PARENT) case Type::CLASS: DISPATCH(CLASS##Type);
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
     }
     llvm_unreachable("Unknown type class!");
   }
@@ -80,7 +80,7 @@ public:
 #define TYPE(CLASS, PARENT) RetTy Visit##CLASS##Type(const CLASS##Type *T) { \
   DISPATCH(PARENT);                                                          \
 }
-#include "clang/AST/TypeNodes.inc"
+#include "TypeNodes.inc"
 
   /// Method called if \c ImpClass doesn't provide specific handler
   /// for some type class.
diff --git a/clang/include/clang/Basic/Attr.td b/clang/include/clang/Basic/Attr.td
index 2a3a29bd2..2fbdd8d83 100644
--- a/clang/include/clang/Basic/Attr.td
+++ b/clang/include/clang/Basic/Attr.td
@@ -4972,3 +4972,25 @@ def NoTrivialAutoVarInit: InheritableAttr {
   let Documentation = [NoTrivialAutoVarInitDocs];
   let SimpleHandler = 1;
 }
+
+def SilicaReflect : InheritableAttr {
+  let Spellings = [CXX11<"silica", "reflect", 1>];
+  let Subjects = SubjectList<[CXXRecord, Enum]>;
+  let Args = [VariadicEnumArgument<"Options", "Option", 1,
+              ["all", "base", "nonPublic", "data", "func"],
+              ["All", "Base", "NonPublic", "Data", "Func"]>];
+  let Documentation = [SilicaReflectDocs];
+}
+
+def SilicaAlias : InheritableAttr {
+  let Spellings = [CXX11<"silica", "alias", 1>];
+  let Subjects = SubjectList<[Field, Function, CXXRecord]>;
+  let Args = [StringArgument<"Name">];
+  let Documentation = [SilicaAliasDocs];
+}
+
+def SilicaIgnore : InheritableAttr {
+  let Spellings = [CXX11<"silica", "ignore", 1>];
+  let Subjects = SubjectList<[Field, Function, CXXRecord]>;
+  let Documentation = [SilicaIgnoreDocs];
+}
diff --git a/clang/include/clang/Basic/AttrDocs.td b/clang/include/clang/Basic/AttrDocs.td
index a8b588169..a547ec8a5 100644
--- a/clang/include/clang/Basic/AttrDocs.td
+++ b/clang/include/clang/Basic/AttrDocs.td
@@ -9026,3 +9026,31 @@ Declares that a function potentially allocates heap memory, and prevents any pot
 of ``nonallocating`` by the compiler.
   }];
 }
+
+def SilicaReflectDocs : Documentation {
+  let Category = DocCatFunction;
+  let Content = [{
+The attribute marks an object for later analysis and code generation. It accepts
+optional arguments with the following settings:
+
+* ``Base``      - includes entities from the base class
+* ``NonPublic`` - includes protected and private members
+* ``Data``      - includes fields and static member variables
+* ``Func``      - includes methods
+* ``All``       - a combination of all the options above, the same as an empty argument
+  }];
+}
+
+def SilicaAliasDocs : Documentation {
+  let Category = DocCatFunction;
+  let Content = [{
+Set alias for an entity as ``[[silica::alias("new_name")]]``
+  }];
+}
+
+def SilicaIgnoreDocs : Documentation {
+  let Category = DocCatFunction;
+  let Content = [{
+The attribute prevents the generation of reflection code for the marked entity.
+  }];
+}
diff --git a/clang/include/clang/Basic/AttrKinds.h b/clang/include/clang/Basic/AttrKinds.h
index ec0052dfe..023f4ded5 100644
--- a/clang/include/clang/Basic/AttrKinds.h
+++ b/clang/include/clang/Basic/AttrKinds.h
@@ -24,7 +24,7 @@ enum Kind {
 #define ATTR_RANGE(CLASS, FIRST_NAME, LAST_NAME) \
   First##CLASS = FIRST_NAME,                    \
   Last##CLASS = LAST_NAME,
-#include "clang/Basic/AttrList.inc"
+#include "AttrList.inc"
 };
 
 } // end namespace attr
diff --git a/clang/include/clang/Basic/AttrSubjectMatchRules.h b/clang/include/clang/Basic/AttrSubjectMatchRules.h
index bec8122ea..3cd778346 100644
--- a/clang/include/clang/Basic/AttrSubjectMatchRules.h
+++ b/clang/include/clang/Basic/AttrSubjectMatchRules.h
@@ -20,10 +20,10 @@ namespace attr {
 /// A list of all the recognized kinds of attributes.
 enum SubjectMatchRule {
 #define ATTR_MATCH_RULE(X, Spelling, IsAbstract) X,
-#include "clang/Basic/AttrSubMatchRulesList.inc"
+#include "AttrSubMatchRulesList.inc"
   SubjectMatchRule_Last = -1
 #define ATTR_MATCH_RULE(X, Spelling, IsAbstract) +1
-#include "clang/Basic/AttrSubMatchRulesList.inc"
+#include "AttrSubMatchRulesList.inc"
 };
 
 const char *getSubjectMatchRuleSpelling(SubjectMatchRule Rule);
diff --git a/clang/include/clang/Basic/AttributeCommonInfo.h b/clang/include/clang/Basic/AttributeCommonInfo.h
index 4af5a8fd1..cd9f55d0f 100644
--- a/clang/include/clang/Basic/AttributeCommonInfo.h
+++ b/clang/include/clang/Basic/AttributeCommonInfo.h
@@ -61,13 +61,13 @@ public:
   };
   enum Kind {
 #define PARSED_ATTR(NAME) AT_##NAME,
-#include "clang/Basic/AttrParsedAttrList.inc"
+#include "AttrParsedAttrList.inc"
 #undef PARSED_ATTR
     NoSemaHandlerAttribute,
     IgnoredAttribute,
     UnknownAttribute,
   };
-  enum class Scope { NONE, CLANG, GNU, MSVC, OMP, HLSL, GSL, RISCV };
+  enum class Scope { NONE, CLANG, GNU, MSVC, OMP, HLSL, GSL, RISCV, SILICA };
   enum class AttrArgsInfo {
     None,
     Optional,
@@ -271,7 +271,7 @@ inline bool doesKeywordAttributeTakeArgs(tok::TokenKind Kind) {
 #define KEYWORD_ATTRIBUTE(NAME, HASARG, ...)                                   \
   case tok::kw_##NAME:                                                         \
     return HASARG;
-#include "clang/Basic/RegularKeywordAttrInfo.inc"
+#include "RegularKeywordAttrInfo.inc"
 #undef KEYWORD_ATTRIBUTE
   }
 }
diff --git a/clang/include/clang/Basic/Builtins.h b/clang/include/clang/Basic/Builtins.h
index 63559d977..aac490f03 100644
--- a/clang/include/clang/Basic/Builtins.h
+++ b/clang/include/clang/Basic/Builtins.h
@@ -64,7 +64,7 @@ namespace Builtin {
 enum ID {
   NotBuiltin  = 0,      // This is not a builtin function.
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/Builtins.inc"
+#include "Builtins.inc"
   FirstTSBuiltin
 };
 
diff --git a/clang/include/clang/Basic/DiagnosticAST.h b/clang/include/clang/Basic/DiagnosticAST.h
index 4f82114b7..2b198cf1e 100644
--- a/clang/include/clang/Basic/DiagnosticAST.h
+++ b/clang/include/clang/Basic/DiagnosticAST.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define ASTSTART
-#include "clang/Basic/DiagnosticASTKinds.inc"
+#include "DiagnosticASTKinds.inc"
 #undef DIAG
   NUM_BUILTIN_AST_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticASTEnums.inc"
+#include "DiagnosticASTEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticAnalysis.h b/clang/include/clang/Basic/DiagnosticAnalysis.h
index 1a49461bc..1e2c7c02c 100644
--- a/clang/include/clang/Basic/DiagnosticAnalysis.h
+++ b/clang/include/clang/Basic/DiagnosticAnalysis.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define ANALYSISSTART
-#include "clang/Basic/DiagnosticAnalysisKinds.inc"
+#include "DiagnosticAnalysisKinds.inc"
 #undef DIAG
   NUM_BUILTIN_ANALYSIS_DIAGNOSTICS
 };
@@ -30,7 +30,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticAnalysisEnums.inc"
+#include "DiagnosticAnalysisEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticCategories.h b/clang/include/clang/Basic/DiagnosticCategories.h
index 839f8dee3..2e5925f36 100644
--- a/clang/include/clang/Basic/DiagnosticCategories.h
+++ b/clang/include/clang/Basic/DiagnosticCategories.h
@@ -14,7 +14,7 @@ namespace clang {
     enum {
 #define GET_CATEGORY_TABLE
 #define CATEGORY(X, ENUM) ENUM,
-#include "clang/Basic/DiagnosticGroups.inc"
+#include "DiagnosticGroups.inc"
 #undef CATEGORY
 #undef GET_CATEGORY_TABLE
       DiagCat_NUM_CATEGORIES
@@ -23,7 +23,7 @@ namespace clang {
     enum class Group {
 #define DIAG_ENTRY(GroupName, FlagNameOffset, Members, SubGroups, Docs)    \
       GroupName,
-#include "clang/Basic/DiagnosticGroups.inc"
+#include "DiagnosticGroups.inc"
 #undef CATEGORY
 #undef DIAG_ENTRY
       NUM_GROUPS
diff --git a/clang/include/clang/Basic/DiagnosticComment.h b/clang/include/clang/Basic/DiagnosticComment.h
index 53143ef13..73dba2a39 100644
--- a/clang/include/clang/Basic/DiagnosticComment.h
+++ b/clang/include/clang/Basic/DiagnosticComment.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define COMMENTSTART
-#include "clang/Basic/DiagnosticCommentKinds.inc"
+#include "DiagnosticCommentKinds.inc"
 #undef DIAG
   NUM_BUILTIN_COMMENT_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticCommentEnums.inc"
+#include "DiagnosticCommentEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticCrossTU.h b/clang/include/clang/Basic/DiagnosticCrossTU.h
index 428da9501..549fe98ce 100644
--- a/clang/include/clang/Basic/DiagnosticCrossTU.h
+++ b/clang/include/clang/Basic/DiagnosticCrossTU.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define CROSSTUSTART
-#include "clang/Basic/DiagnosticCrossTUKinds.inc"
+#include "DiagnosticCrossTUKinds.inc"
 #undef DIAG
   NUM_BUILTIN_CROSSTU_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticCrossTUEnums.inc"
+#include "DiagnosticCrossTUEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticDriver.h b/clang/include/clang/Basic/DiagnosticDriver.h
index c472afa3f..e3d2108d0 100644
--- a/clang/include/clang/Basic/DiagnosticDriver.h
+++ b/clang/include/clang/Basic/DiagnosticDriver.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define DRIVERSTART
-#include "clang/Basic/DiagnosticDriverKinds.inc"
+#include "DiagnosticDriverKinds.inc"
 #undef DIAG
   NUM_BUILTIN_DRIVER_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticDriverEnums.inc"
+#include "DiagnosticDriverEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticFrontend.h b/clang/include/clang/Basic/DiagnosticFrontend.h
index 766cac3d6..4697ad046 100644
--- a/clang/include/clang/Basic/DiagnosticFrontend.h
+++ b/clang/include/clang/Basic/DiagnosticFrontend.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define FRONTENDSTART
-#include "clang/Basic/DiagnosticFrontendKinds.inc"
+#include "DiagnosticFrontendKinds.inc"
 #undef DIAG
   NUM_BUILTIN_FRONTEND_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticFrontendEnums.inc"
+#include "DiagnosticFrontendEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticIDs.h b/clang/include/clang/Basic/DiagnosticIDs.h
index b49185c33..8eed800c7 100644
--- a/clang/include/clang/Basic/DiagnosticIDs.h
+++ b/clang/include/clang/Basic/DiagnosticIDs.h
@@ -76,7 +76,7 @@ namespace clang {
              NOWERROR, SHOWINSYSHEADER, SHOWINSYSMACRO, DEFFERABLE)            \
   ENUM,
 #define COMMONSTART
-#include "clang/Basic/DiagnosticCommonKinds.inc"
+#include "DiagnosticCommonKinds.inc"
       NUM_BUILTIN_COMMON_DIAGNOSTICS
 #undef DIAG
     };
diff --git a/clang/include/clang/Basic/DiagnosticInstallAPI.h b/clang/include/clang/Basic/DiagnosticInstallAPI.h
index cbdb00362..9b6c449bd 100644
--- a/clang/include/clang/Basic/DiagnosticInstallAPI.h
+++ b/clang/include/clang/Basic/DiagnosticInstallAPI.h
@@ -17,7 +17,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define INSTALLAPISTART
-#include "clang/Basic/DiagnosticInstallAPIKinds.inc"
+#include "DiagnosticInstallAPIKinds.inc"
 #undef DIAG
   NUM_BUILTIN_INSTALLAPI_DIAGNOSTICS
 };
@@ -30,7 +30,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticInstallAPIEnums.inc"
+#include "DiagnosticInstallAPIEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticLex.h b/clang/include/clang/Basic/DiagnosticLex.h
index d14bf97e8..77b6a1902 100644
--- a/clang/include/clang/Basic/DiagnosticLex.h
+++ b/clang/include/clang/Basic/DiagnosticLex.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define LEXSTART
-#include "clang/Basic/DiagnosticLexKinds.inc"
+#include "DiagnosticLexKinds.inc"
 #undef DIAG
   NUM_BUILTIN_LEX_DIAGNOSTICS
 };
@@ -30,7 +30,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticLexEnums.inc"
+#include "DiagnosticLexEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticParse.h b/clang/include/clang/Basic/DiagnosticParse.h
index 275e1a4c3..a79ab2a0c 100644
--- a/clang/include/clang/Basic/DiagnosticParse.h
+++ b/clang/include/clang/Basic/DiagnosticParse.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define PARSESTART
-#include "clang/Basic/DiagnosticParseKinds.inc"
+#include "DiagnosticParseKinds.inc"
 #undef DIAG
   NUM_BUILTIN_PARSE_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticParseEnums.inc"
+#include "DiagnosticParseEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticRefactoring.h b/clang/include/clang/Basic/DiagnosticRefactoring.h
index 59d4bc912..56b9cb0fd 100644
--- a/clang/include/clang/Basic/DiagnosticRefactoring.h
+++ b/clang/include/clang/Basic/DiagnosticRefactoring.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define REFACTORINGSTART
-#include "clang/Basic/DiagnosticRefactoringKinds.inc"
+#include "DiagnosticRefactoringKinds.inc"
 #undef DIAG
   NUM_BUILTIN_REFACTORING_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticRefactoringEnums.inc"
+#include "DiagnosticRefactoringEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticSema.h b/clang/include/clang/Basic/DiagnosticSema.h
index 84986c7bc..518f9cc3f 100644
--- a/clang/include/clang/Basic/DiagnosticSema.h
+++ b/clang/include/clang/Basic/DiagnosticSema.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define SEMASTART
-#include "clang/Basic/DiagnosticSemaKinds.inc"
+#include "DiagnosticSemaKinds.inc"
 #undef DIAG
   NUM_BUILTIN_SEMA_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticSemaEnums.inc"
+#include "DiagnosticSemaEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/DiagnosticSerialization.h b/clang/include/clang/Basic/DiagnosticSerialization.h
index 6fb836dca..2307ac630 100644
--- a/clang/include/clang/Basic/DiagnosticSerialization.h
+++ b/clang/include/clang/Basic/DiagnosticSerialization.h
@@ -18,7 +18,7 @@ enum {
              SHOWINSYSHEADER, SHOWINSYSMACRO, DEFERRABLE, CATEGORY)            \
   ENUM,
 #define SERIALIZATIONSTART
-#include "clang/Basic/DiagnosticSerializationKinds.inc"
+#include "DiagnosticSerializationKinds.inc"
 #undef DIAG
   NUM_BUILTIN_SERIALIZATION_DIAGNOSTICS
 };
@@ -31,7 +31,7 @@ enum {
   }                                                                            \
   ;                                                                            \
   }
-#include "clang/Basic/DiagnosticSerializationEnums.inc"
+#include "DiagnosticSerializationEnums.inc"
 #undef DIAG_ENUM_END
 #undef DIAG_ENUM_ITEM
 #undef DIAG_ENUM
diff --git a/clang/include/clang/Basic/IdentifierTable.h b/clang/include/clang/Basic/IdentifierTable.h
index e5e6be3c9..504e38222 100644
--- a/clang/include/clang/Basic/IdentifierTable.h
+++ b/clang/include/clang/Basic/IdentifierTable.h
@@ -102,7 +102,7 @@ enum class InterestingIdentifier {
 
   NotBuiltin,
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/Builtins.inc"
+#include "Builtins.inc"
   FirstTSBuiltin,
 
   NotInterestingIdentifier = 65534
diff --git a/clang/include/clang/Basic/TargetBuiltins.h b/clang/include/clang/Basic/TargetBuiltins.h
index 95eb110bb..c595082ae 100644
--- a/clang/include/clang/Basic/TargetBuiltins.h
+++ b/clang/include/clang/Basic/TargetBuiltins.h
@@ -84,7 +84,7 @@ namespace clang {
   enum {
     LastTIBuiltin = clang::Builtin::FirstTSBuiltin - 1,
   #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-  #include "clang/Basic/BuiltinsBPF.inc"
+  #include "BuiltinsBPF.inc"
     LastTSBuiltin
   };
   }
@@ -104,7 +104,7 @@ namespace clang {
   enum {
     LastTIBuiltin = clang::Builtin::FirstTSBuiltin - 1,
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/BuiltinsNVPTX.inc"
+#include "BuiltinsNVPTX.inc"
     LastTSBuiltin
   };
   }
@@ -124,7 +124,7 @@ namespace clang {
   enum {
     LastTIBuiltin = clang::Builtin::FirstTSBuiltin - 1,
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/BuiltinsSPIRV.inc"
+#include "BuiltinsSPIRV.inc"
     LastTSBuiltin
   };
   } // namespace SPIRV
@@ -134,11 +134,11 @@ namespace clang {
   enum {
     LastTIBuiltin = clang::Builtin::FirstTSBuiltin - 1,
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/BuiltinsX86.inc"
+#include "BuiltinsX86.inc"
     FirstX86_64Builtin,
     LastX86CommonBuiltin = FirstX86_64Builtin - 1,
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/BuiltinsX86_64.inc"
+#include "BuiltinsX86_64.inc"
     LastTSBuiltin
   };
   }
@@ -169,7 +169,7 @@ namespace clang {
     FirstRVVBuiltin = clang::Builtin::FirstTSBuiltin,
     LastRVVBuiltin = RISCVVector::FirstTSBuiltin - 1,
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/BuiltinsRISCV.inc"
+#include "BuiltinsRISCV.inc"
     LastTSBuiltin
   };
   } // namespace RISCV
@@ -255,7 +255,7 @@ namespace clang {
   // Shared between SVE/SME and NEON
   enum ImmCheckType {
 #define LLVM_GET_ARM_INTRIN_IMMCHECKTYPES
-#include "clang/Basic/arm_immcheck_types.inc"
+#include "arm_immcheck_types.inc"
 #undef LLVM_GET_ARM_INTRIN_IMMCHECKTYPES
   };
 
@@ -269,24 +269,24 @@ namespace clang {
 
   public:
 #define LLVM_GET_SVE_TYPEFLAGS
-#include "clang/Basic/arm_sve_typeflags.inc"
+#include "arm_sve_typeflags.inc"
 #undef LLVM_GET_SVE_TYPEFLAGS
 
     enum EltType {
 #define LLVM_GET_SVE_ELTTYPES
-#include "clang/Basic/arm_sve_typeflags.inc"
+#include "arm_sve_typeflags.inc"
 #undef LLVM_GET_SVE_ELTTYPES
     };
 
     enum MemEltType {
 #define LLVM_GET_SVE_MEMELTTYPES
-#include "clang/Basic/arm_sve_typeflags.inc"
+#include "arm_sve_typeflags.inc"
 #undef LLVM_GET_SVE_MEMELTTYPES
     };
 
     enum MergeType {
 #define LLVM_GET_SVE_MERGETYPES
-#include "clang/Basic/arm_sve_typeflags.inc"
+#include "arm_sve_typeflags.inc"
 #undef LLVM_GET_SVE_MERGETYPES
     };
 
@@ -357,7 +357,7 @@ namespace clang {
   enum {
     LastTIBuiltin = clang::Builtin::FirstTSBuiltin - 1,
 #define BUILTIN(ID, TYPE, ATTRS) BI##ID,
-#include "clang/Basic/BuiltinsHexagon.inc"
+#include "BuiltinsHexagon.inc"
     LastTSBuiltin
   };
   }
diff --git a/clang/include/clang/Basic/TokenKinds.def b/clang/include/clang/Basic/TokenKinds.def
index 8902a20b0..8f61a5e42 100644
--- a/clang/include/clang/Basic/TokenKinds.def
+++ b/clang/include/clang/Basic/TokenKinds.def
@@ -798,7 +798,7 @@ KEYWORD(__builtin_sycl_unique_stable_name, KEYSYCL)
 #ifndef KEYWORD_ATTRIBUTE
 #define KEYWORD_ATTRIBUTE(X, HASARG, EMPTY) KEYWORD(EMPTY ## X, KEYALL)
 #endif
-#include "clang/Basic/RegularKeywordAttrInfo.inc"
+#include "RegularKeywordAttrInfo.inc"
 
 // Clang-specific keywords enabled only in testing.
 TESTING_KEYWORD(__unknown_anytype , KEYALL)
diff --git a/clang/include/clang/Basic/TokenKinds.h b/clang/include/clang/Basic/TokenKinds.h
index 1b133dde8..c8e00e303 100644
--- a/clang/include/clang/Basic/TokenKinds.h
+++ b/clang/include/clang/Basic/TokenKinds.h
@@ -110,7 +110,7 @@ bool isPragmaAnnotation(TokenKind K);
 inline constexpr bool isRegularKeywordAttribute(TokenKind K) {
   return (false
 #define KEYWORD_ATTRIBUTE(X, ...) || (K == tok::kw_##X)
-#include "clang/Basic/RegularKeywordAttrInfo.inc"
+#include "RegularKeywordAttrInfo.inc"
   );
 }
 
diff --git a/clang/include/clang/CIR/Dialect/IR/CIRAttrs.h b/clang/include/clang/CIR/Dialect/IR/CIRAttrs.h
index 438fb7d09..fdde67ee7 100644
--- a/clang/include/clang/CIR/Dialect/IR/CIRAttrs.h
+++ b/clang/include/clang/CIR/Dialect/IR/CIRAttrs.h
@@ -31,6 +31,6 @@ class RecordDecl;
 } // namespace clang
 
 #define GET_ATTRDEF_CLASSES
-#include "clang/CIR/Dialect/IR/CIROpsAttributes.h.inc"
+#include "CIROpsAttributes.h.inc"
 
 #endif // LLVM_CLANG_CIR_DIALECT_IR_CIRATTRS_H
diff --git a/clang/include/clang/CIR/Dialect/IR/CIRDialect.h b/clang/include/clang/CIR/Dialect/IR/CIRDialect.h
index 683176b13..ab9afc515 100644
--- a/clang/include/clang/CIR/Dialect/IR/CIRDialect.h
+++ b/clang/include/clang/CIR/Dialect/IR/CIRDialect.h
@@ -27,12 +27,12 @@
 #include "mlir/Interfaces/SideEffectInterfaces.h"
 
 #include "clang/CIR/Dialect/IR/CIRAttrs.h"
-#include "clang/CIR/Dialect/IR/CIROpsDialect.h.inc"
+#include "CIROpsDialect.h.inc"
 
 // TableGen'erated files for MLIR dialects require that a macro be defined when
 // they are included.  GET_OP_CLASSES tells the file to define the classes for
 // the operations of that dialect.
 #define GET_OP_CLASSES
-#include "clang/CIR/Dialect/IR/CIROps.h.inc"
+#include "CIROps.h.inc"
 
 #endif // LLVM_CLANG_CIR_DIALECT_IR_CIRDIALECT_H
diff --git a/clang/include/clang/CIR/Dialect/IR/CIRTypes.h b/clang/include/clang/CIR/Dialect/IR/CIRTypes.h
index 5d1eb17e1..075948d62 100644
--- a/clang/include/clang/CIR/Dialect/IR/CIRTypes.h
+++ b/clang/include/clang/CIR/Dialect/IR/CIRTypes.h
@@ -29,6 +29,6 @@ bool isAnyFloatingPointType(mlir::Type t);
 //===----------------------------------------------------------------------===//
 
 #define GET_TYPEDEF_CLASSES
-#include "clang/CIR/Dialect/IR/CIROpsTypes.h.inc"
+#include "CIROpsTypes.h.inc"
 
 #endif // MLIR_DIALECT_CIR_IR_CIRTYPES_H_
diff --git a/clang/include/clang/CIR/Interfaces/CIRFPTypeInterface.h b/clang/include/clang/CIR/Interfaces/CIRFPTypeInterface.h
index 40b85ef6c..665db58c6 100644
--- a/clang/include/clang/CIR/Interfaces/CIRFPTypeInterface.h
+++ b/clang/include/clang/CIR/Interfaces/CIRFPTypeInterface.h
@@ -17,6 +17,6 @@
 #include "llvm/ADT/APFloat.h"
 
 /// Include the tablegen'd interface declarations.
-#include "clang/CIR/Interfaces/CIRFPTypeInterface.h.inc"
+#include "CIRFPTypeInterface.h.inc"
 
 #endif // LLVM_CLANG_INCLUDE_CLANG_CIR_INTERFACES_CIRFPTYPEINTERFACE_H
diff --git a/clang/include/clang/Driver/Options.h b/clang/include/clang/Driver/Options.h
index 0797410e9..d14417d24 100644
--- a/clang/include/clang/Driver/Options.h
+++ b/clang/include/clang/Driver/Options.h
@@ -44,7 +44,7 @@ enum ClangVisibility {
 enum ID {
     OPT_INVALID = 0, // This is not an option ID.
 #define OPTION(...) LLVM_MAKE_OPT_ID(__VA_ARGS__),
-#include "clang/Driver/Options.inc"
+#include "Options.inc"
     LastOption
 #undef OPTION
   };
diff --git a/clang/include/clang/Sema/Template.h b/clang/include/clang/Sema/Template.h
index 4206bd50b..9ec19dc04 100644
--- a/clang/include/clang/Sema/Template.h
+++ b/clang/include/clang/Sema/Template.h
@@ -635,7 +635,7 @@ enum class TemplateSubstitutionKind : char {
 #define CAPTURED(DERIVED, BASE)
 #define IMPLICITPARAM(DERIVED, BASE)
 
-#include "clang/AST/DeclNodes.inc"
+#include "DeclNodes.inc"
 
     enum class RewriteKind { None, RewriteSpaceshipAsEqualEqual };
 
diff --git a/clang/include/clang/StaticAnalyzer/Checkers/BuiltinCheckerRegistration.h b/clang/include/clang/StaticAnalyzer/Checkers/BuiltinCheckerRegistration.h
index bdfe3901c..0dbe56b91 100644
--- a/clang/include/clang/StaticAnalyzer/Checkers/BuiltinCheckerRegistration.h
+++ b/clang/include/clang/StaticAnalyzer/Checkers/BuiltinCheckerRegistration.h
@@ -25,7 +25,7 @@ class CheckerManager;
 #define CHECKER(FULLNAME, CLASS, HELPTEXT, DOC_URI, IS_HIDDEN)                 \
   void register##CLASS(CheckerManager &mgr);                                   \
   bool shouldRegister##CLASS(const CheckerManager &mgr);
-#include "clang/StaticAnalyzer/Checkers/Checkers.inc"
+#include "Checkers.inc"
 #undef CHECKER
 #undef GET_CHECKERS
 
diff --git a/clang/include/clang/StaticAnalyzer/Core/AnalyzerOptions.h b/clang/include/clang/StaticAnalyzer/Core/AnalyzerOptions.h
index 2c9703018..90d55952a 100644
--- a/clang/include/clang/StaticAnalyzer/Core/AnalyzerOptions.h
+++ b/clang/include/clang/StaticAnalyzer/Core/AnalyzerOptions.h
@@ -432,7 +432,7 @@ AnalyzerOptions::getRegisteredCheckers(bool IncludeExperimental) {
 #define GET_CHECKERS
 #define CHECKER(FULLNAME, CLASS, HELPTEXT, DOC_URI, IS_HIDDEN)                 \
   llvm::StringLiteral(FULLNAME),
-#include "clang/StaticAnalyzer/Checkers/Checkers.inc"
+#include "Checkers.inc"
 #undef CHECKER
 #undef GET_CHECKERS
   };
@@ -450,7 +450,7 @@ AnalyzerOptions::getRegisteredPackages(bool IncludeExperimental) {
   static constexpr llvm::StringLiteral StaticAnalyzerPackageNames[] = {
 #define GET_PACKAGES
 #define PACKAGE(FULLNAME) llvm::StringLiteral(FULLNAME),
-#include "clang/StaticAnalyzer/Checkers/Checkers.inc"
+#include "Checkers.inc"
 #undef PACKAGE
 #undef GET_PACKAGES
   };
diff --git a/clang/include/clang/Tooling/Syntax/Nodes.h b/clang/include/clang/Tooling/Syntax/Nodes.h
index c4f31900d..15c2b98ab 100644
--- a/clang/include/clang/Tooling/Syntax/Nodes.h
+++ b/clang/include/clang/Tooling/Syntax/Nodes.h
@@ -31,7 +31,7 @@ namespace syntax {
 /// of syntax::Node.
 enum class NodeKind : uint16_t {
 #define CONCRETE_NODE(Kind, Base) Kind,
-#include "clang/Tooling/Syntax/Nodes.inc"
+#include "Nodes.inc"
 };
 /// For debugging purposes.
 raw_ostream &operator<<(raw_ostream &OS, NodeKind K);
@@ -109,7 +109,7 @@ enum class NodeRole : uint8_t {
 /// For debugging purposes.
 raw_ostream &operator<<(raw_ostream &OS, NodeRole R);
 
-#include "clang/Tooling/Syntax/NodeClasses.inc"
+#include "NodeClasses.inc"
 
 /// Models a `nested-name-specifier`. C++ [expr.prim.id.qual]
 /// e.g. the `std::vector<int>::` in `std::vector<int>::size`.
@@ -583,7 +583,7 @@ public:
   inline bool Kind::classof(const Node *N) {                                   \
     return N->getKind() >= NodeKind::First && N->getKind() <= NodeKind::Last;  \
   }
-#include "clang/Tooling/Syntax/Nodes.inc"
+#include "Nodes.inc"
 
 } // namespace syntax
 } // namespace clang
diff --git a/clang/lib/Basic/Attributes.cpp b/clang/lib/Basic/Attributes.cpp
index 2035d4c0a..7a46d7bdd 100644
--- a/clang/lib/Basic/Attributes.cpp
+++ b/clang/lib/Basic/Attributes.cpp
@@ -187,7 +187,8 @@ getScopeFromNormalizedScopeName(StringRef ScopeName) {
       .Case("hlsl", AttributeCommonInfo::Scope::HLSL)
       .Case("msvc", AttributeCommonInfo::Scope::MSVC)
       .Case("omp", AttributeCommonInfo::Scope::OMP)
-      .Case("riscv", AttributeCommonInfo::Scope::RISCV);
+      .Case("riscv", AttributeCommonInfo::Scope::RISCV)
+      .Case("silica", AttributeCommonInfo::Scope::SILICA);
 }
 
 unsigned AttributeCommonInfo::calculateAttributeSpellingListIndex() const {
diff --git a/clang/lib/Basic/CMakeLists.txt b/clang/lib/Basic/CMakeLists.txt
index 331dfbb3f..83e85f903 100644
--- a/clang/lib/Basic/CMakeLists.txt
+++ b/clang/lib/Basic/CMakeLists.txt
@@ -47,7 +47,7 @@ add_custom_command(OUTPUT "${version_inc}"
 # Mark the generated header as being generated.
 set_source_files_properties("${version_inc}"
   PROPERTIES GENERATED TRUE
-             HEADER_FILE_ONLY TRUE)
+             HEADER_FILE_ONLY FALSE)
 
 if(CLANG_VENDOR)
   set_source_files_properties(Version.cpp
@@ -132,6 +132,7 @@ add_clang_library(clangBasic
   DEPENDS
   omp_gen
   ClangDriverOptions
+  ClangDiagnosticCommon
   # These generated headers are included transitively.
   ARMTargetParserTableGen
   AArch64TargetParserTableGen
diff --git a/clang/lib/Basic/Targets/BPF.cpp b/clang/lib/Basic/Targets/BPF.cpp
index f4684765b..0eb5825e0 100644
--- a/clang/lib/Basic/Targets/BPF.cpp
+++ b/clang/lib/Basic/Targets/BPF.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "BPF.h"
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/MacroBuilder.h"
 #include "clang/Basic/TargetBuiltins.h"
 #include "llvm/ADT/StringRef.h"
@@ -70,8 +70,8 @@ void BPFTargetInfo::getTargetDefines(const LangOptions &Opts,
   }
 }
 
-static constexpr llvm::StringLiteral ValidCPUNames[] = {"generic", "v1", "v2",
-                                                        "v3", "v4", "probe"};
+static constexpr llvm::StringLiteral ValidCPUNames[] = {
+    "generic", "v1", "v2", "v3", "v4", "probe"};
 
 bool BPFTargetInfo::isValidCPUName(StringRef Name) const {
   return llvm::is_contained(ValidCPUNames, Name);
diff --git a/clang/lib/Basic/Targets/DirectX.cpp b/clang/lib/Basic/Targets/DirectX.cpp
index 0dd27e6e9..479fd865f 100644
--- a/clang/lib/Basic/Targets/DirectX.cpp
+++ b/clang/lib/Basic/Targets/DirectX.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "DirectX.h"
-#include "Targets.h"
+#include "../Targets.h"
 
 using namespace clang;
 using namespace clang::targets;
diff --git a/clang/lib/Basic/Targets/Hexagon.cpp b/clang/lib/Basic/Targets/Hexagon.cpp
index 2e173e01e..5e9431b58 100644
--- a/clang/lib/Basic/Targets/Hexagon.cpp
+++ b/clang/lib/Basic/Targets/Hexagon.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "Hexagon.h"
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/MacroBuilder.h"
 #include "clang/Basic/TargetBuiltins.h"
 #include "llvm/ADT/StringSwitch.h"
@@ -158,36 +158,195 @@ bool HexagonTargetInfo::handleTargetFeatures(std::vector<std::string> &Features,
 
 const char *const HexagonTargetInfo::GCCRegNames[] = {
     // Scalar registers:
-    "r0", "r1", "r2", "r3", "r4", "r5", "r6", "r7", "r8", "r9", "r10", "r11",
-    "r12", "r13", "r14", "r15", "r16", "r17", "r18", "r19", "r20", "r21",
-    "r22", "r23", "r24", "r25", "r26", "r27", "r28", "r29", "r30", "r31",
-    "r1:0", "r3:2", "r5:4", "r7:6", "r9:8", "r11:10", "r13:12", "r15:14",
-    "r17:16", "r19:18", "r21:20", "r23:22", "r25:24", "r27:26", "r29:28",
+    "r0",
+    "r1",
+    "r2",
+    "r3",
+    "r4",
+    "r5",
+    "r6",
+    "r7",
+    "r8",
+    "r9",
+    "r10",
+    "r11",
+    "r12",
+    "r13",
+    "r14",
+    "r15",
+    "r16",
+    "r17",
+    "r18",
+    "r19",
+    "r20",
+    "r21",
+    "r22",
+    "r23",
+    "r24",
+    "r25",
+    "r26",
+    "r27",
+    "r28",
+    "r29",
+    "r30",
+    "r31",
+    "r1:0",
+    "r3:2",
+    "r5:4",
+    "r7:6",
+    "r9:8",
+    "r11:10",
+    "r13:12",
+    "r15:14",
+    "r17:16",
+    "r19:18",
+    "r21:20",
+    "r23:22",
+    "r25:24",
+    "r27:26",
+    "r29:28",
     "r31:30",
     // Predicate registers:
-    "p0", "p1", "p2", "p3",
+    "p0",
+    "p1",
+    "p2",
+    "p3",
     // Control registers:
-    "c0", "c1", "c2", "c3", "c4", "c5", "c6", "c7", "c8", "c9", "c10", "c11",
-    "c12", "c13", "c14", "c15", "c16", "c17", "c18", "c19", "c20", "c21",
-    "c22", "c23", "c24", "c25", "c26", "c27", "c28", "c29", "c30", "c31",
-    "c1:0", "c3:2", "c5:4", "c7:6", "c9:8", "c11:10", "c13:12", "c15:14",
-    "c17:16", "c19:18", "c21:20", "c23:22", "c25:24", "c27:26", "c29:28",
+    "c0",
+    "c1",
+    "c2",
+    "c3",
+    "c4",
+    "c5",
+    "c6",
+    "c7",
+    "c8",
+    "c9",
+    "c10",
+    "c11",
+    "c12",
+    "c13",
+    "c14",
+    "c15",
+    "c16",
+    "c17",
+    "c18",
+    "c19",
+    "c20",
+    "c21",
+    "c22",
+    "c23",
+    "c24",
+    "c25",
+    "c26",
+    "c27",
+    "c28",
+    "c29",
+    "c30",
+    "c31",
+    "c1:0",
+    "c3:2",
+    "c5:4",
+    "c7:6",
+    "c9:8",
+    "c11:10",
+    "c13:12",
+    "c15:14",
+    "c17:16",
+    "c19:18",
+    "c21:20",
+    "c23:22",
+    "c25:24",
+    "c27:26",
+    "c29:28",
     "c31:30",
     // Control register aliases:
-    "sa0", "lc0", "sa1", "lc1", "p3:0", "m0",  "m1",  "usr", "pc", "ugp",
-    "gp", "cs0", "cs1", "upcyclelo", "upcyclehi", "framelimit", "framekey",
-    "pktcountlo", "pktcounthi", "utimerlo", "utimerhi",
-    "upcycle", "pktcount", "utimer",
+    "sa0",
+    "lc0",
+    "sa1",
+    "lc1",
+    "p3:0",
+    "m0",
+    "m1",
+    "usr",
+    "pc",
+    "ugp",
+    "gp",
+    "cs0",
+    "cs1",
+    "upcyclelo",
+    "upcyclehi",
+    "framelimit",
+    "framekey",
+    "pktcountlo",
+    "pktcounthi",
+    "utimerlo",
+    "utimerhi",
+    "upcycle",
+    "pktcount",
+    "utimer",
     // HVX vector registers:
-    "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11",
-    "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21",
-    "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31",
-    "v1:0", "v3:2", "v5:4", "v7:6", "v9:8", "v11:10", "v13:12", "v15:14",
-    "v17:16", "v19:18", "v21:20", "v23:22", "v25:24", "v27:26", "v29:28",
+    "v0",
+    "v1",
+    "v2",
+    "v3",
+    "v4",
+    "v5",
+    "v6",
+    "v7",
+    "v8",
+    "v9",
+    "v10",
+    "v11",
+    "v12",
+    "v13",
+    "v14",
+    "v15",
+    "v16",
+    "v17",
+    "v18",
+    "v19",
+    "v20",
+    "v21",
+    "v22",
+    "v23",
+    "v24",
+    "v25",
+    "v26",
+    "v27",
+    "v28",
+    "v29",
+    "v30",
+    "v31",
+    "v1:0",
+    "v3:2",
+    "v5:4",
+    "v7:6",
+    "v9:8",
+    "v11:10",
+    "v13:12",
+    "v15:14",
+    "v17:16",
+    "v19:18",
+    "v21:20",
+    "v23:22",
+    "v25:24",
+    "v27:26",
+    "v29:28",
     "v31:30",
-    "v3:0", "v7:4", "v11:8", "v15:12", "v19:16", "v23:20", "v27:24", "v31:28",
+    "v3:0",
+    "v7:4",
+    "v11:8",
+    "v15:12",
+    "v19:16",
+    "v23:20",
+    "v27:24",
+    "v31:28",
     // HVX vector predicates:
-    "q0", "q1", "q2", "q3",
+    "q0",
+    "q1",
+    "q2",
+    "q3",
 };
 
 ArrayRef<const char *> HexagonTargetInfo::getGCCRegNames() const {
diff --git a/clang/lib/Basic/Targets/Mips.cpp b/clang/lib/Basic/Targets/Mips.cpp
index d56995e3c..9b6697290 100644
--- a/clang/lib/Basic/Targets/Mips.cpp
+++ b/clang/lib/Basic/Targets/Mips.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "Mips.h"
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/Diagnostic.h"
 #include "clang/Basic/MacroBuilder.h"
 #include "clang/Basic/TargetBuiltins.h"
@@ -47,7 +47,7 @@ static constexpr llvm::StringLiteral ValidCPUNames[] = {
     {"mips1"},  {"mips2"},    {"mips3"},    {"mips4"},    {"mips5"},
     {"mips32"}, {"mips32r2"}, {"mips32r3"}, {"mips32r5"}, {"mips32r6"},
     {"mips64"}, {"mips64r2"}, {"mips64r3"}, {"mips64r5"}, {"mips64r6"},
-    {"octeon"}, {"octeon+"}, {"p5600"}};
+    {"octeon"}, {"octeon+"},  {"p5600"}};
 
 bool MipsTargetInfo::isValidCPUName(StringRef Name) const {
   return llvm::is_contained(ValidCPUNames, Name);
@@ -60,12 +60,12 @@ void MipsTargetInfo::fillValidCPUList(
 
 unsigned MipsTargetInfo::getISARev() const {
   return llvm::StringSwitch<unsigned>(getCPU())
-             .Cases("mips32", "mips64", 1)
-             .Cases("mips32r2", "mips64r2", "octeon", "octeon+", 2)
-             .Cases("mips32r3", "mips64r3", 3)
-             .Cases("mips32r5", "mips64r5", 5)
-             .Cases("mips32r6", "mips64r6", 6)
-             .Default(0);
+      .Cases("mips32", "mips64", 1)
+      .Cases("mips32r2", "mips64r2", "octeon", "octeon+", 2)
+      .Cases("mips32r3", "mips64r3", 3)
+      .Cases("mips32r5", "mips64r5", 5)
+      .Cases("mips32r6", "mips64r6", 6)
+      .Default(0);
 }
 
 void MipsTargetInfo::getTargetDefines(const LangOptions &Opts,
@@ -143,7 +143,7 @@ void MipsTargetInfo::getTargetDefines(const LangOptions &Opts,
   case FP64:
     Builder.defineMacro("__mips_fpr", Twine(64));
     break;
-}
+  }
 
   if (FPMode == FP64 || IsSingleFloat)
     Builder.defineMacro("_MIPS_FPSET", Twine(32));
@@ -262,14 +262,14 @@ bool MipsTargetInfo::validateTarget(DiagnosticsEngine &Diags) const {
     return false;
   }
   // Mips revision 6 and -mfp32 are incompatible
-  if (FPMode != FP64 && FPMode != FPXX && (CPU == "mips32r6" ||
-      CPU == "mips64r6")) {
+  if (FPMode != FP64 && FPMode != FPXX &&
+      (CPU == "mips32r6" || CPU == "mips64r6")) {
     Diags.Report(diag::err_opt_not_valid_with_opt) << "-mfp32" << CPU;
     return false;
   }
   // Option -mfp64 permitted on Mips32 iff revision 2 or higher is present
-  if (FPMode == FP64 && (CPU == "mips1" || CPU == "mips2" ||
-      getISARev() < 2) && ABI == "o32") {
+  if (FPMode == FP64 && (CPU == "mips1" || CPU == "mips2" || getISARev() < 2) &&
+      ABI == "o32") {
     Diags.Report(diag::err_mips_fp64_req) << "-mfp64";
     return false;
   }
diff --git a/clang/lib/Basic/Targets/NVPTX.cpp b/clang/lib/Basic/Targets/NVPTX.cpp
index 9be12cbe7..67a6519a2 100644
--- a/clang/lib/Basic/Targets/NVPTX.cpp
+++ b/clang/lib/Basic/Targets/NVPTX.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "NVPTX.h"
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/Builtins.h"
 #include "clang/Basic/MacroBuilder.h"
 #include "clang/Basic/TargetBuiltins.h"
@@ -285,24 +285,25 @@ void NVPTXTargetInfo::getTargetDefines(const LangOptions &Opts,
         return "1000";
       case OffloadArch::SM_101:
       case OffloadArch::SM_101a:
-         return "1010";
+        return "1010";
       case OffloadArch::SM_120:
       case OffloadArch::SM_120a:
-         return "1200";
+        return "1200";
       }
       llvm_unreachable("unhandled OffloadArch");
     }();
     Builder.defineMacro("__CUDA_ARCH__", CUDAArchCode);
-    switch(GPU) {
-      case OffloadArch::SM_90a:
-      case OffloadArch::SM_100a:
-      case OffloadArch::SM_101a:
-      case OffloadArch::SM_120a:
-        Builder.defineMacro("__CUDA_ARCH_FEAT_SM" + CUDAArchCode.drop_back() + "_ALL", "1");
-        break;
-      default:
-        // Do nothing if this is not an enhanced architecture.
-        break;
+    switch (GPU) {
+    case OffloadArch::SM_90a:
+    case OffloadArch::SM_100a:
+    case OffloadArch::SM_101a:
+    case OffloadArch::SM_120a:
+      Builder.defineMacro(
+          "__CUDA_ARCH_FEAT_SM" + CUDAArchCode.drop_back() + "_ALL", "1");
+      break;
+    default:
+      // Do nothing if this is not an enhanced architecture.
+      break;
     }
   }
 }
diff --git a/clang/lib/Basic/Targets/OSTargets.h b/clang/lib/Basic/Targets/OSTargets.h
index 991efd2bd..1e0e03596 100644
--- a/clang/lib/Basic/Targets/OSTargets.h
+++ b/clang/lib/Basic/Targets/OSTargets.h
@@ -12,7 +12,7 @@
 #ifndef LLVM_CLANG_LIB_BASIC_TARGETS_OSTARGETS_H
 #define LLVM_CLANG_LIB_BASIC_TARGETS_OSTARGETS_H
 
-#include "Targets.h"
+#include "../Targets.h"
 
 namespace clang {
 namespace targets {
@@ -326,6 +326,7 @@ protected:
     if (Opts.CPlusPlus)
       Builder.defineMacro("_GNU_SOURCE");
   }
+
 public:
   using OSTargetInfo<Target>::OSTargetInfo;
 };
@@ -351,7 +352,7 @@ protected:
         Builder.defineMacro("__ANDROID_API__", "__ANDROID_MIN_SDK_VERSION__");
       }
     } else {
-        Builder.defineMacro("__gnu_linux__");
+      Builder.defineMacro("__gnu_linux__");
     }
     if (Opts.POSIXThreads)
       Builder.defineMacro("_REENTRANT");
@@ -660,8 +661,7 @@ public:
 };
 
 // AIX Target
-template <typename Target>
-class AIXTargetInfo : public OSTargetInfo<Target> {
+template <typename Target> class AIXTargetInfo : public OSTargetInfo<Target> {
 protected:
   void getOSDefines(const LangOptions &Opts, const llvm::Triple &Triple,
                     MacroBuilder &Builder) const override {
diff --git a/clang/lib/Basic/Targets/SPIR.cpp b/clang/lib/Basic/Targets/SPIR.cpp
index f242fedc1..e223b90a5 100644
--- a/clang/lib/Basic/Targets/SPIR.cpp
+++ b/clang/lib/Basic/Targets/SPIR.cpp
@@ -11,8 +11,8 @@
 //===----------------------------------------------------------------------===//
 
 #include "SPIR.h"
+#include "../Targets.h"
 #include "AMDGPU.h"
-#include "Targets.h"
 #include "clang/Basic/MacroBuilder.h"
 #include "clang/Basic/TargetBuiltins.h"
 #include "llvm/TargetParser/TargetParser.h"
diff --git a/clang/lib/Basic/Targets/SPIR.h b/clang/lib/Basic/Targets/SPIR.h
index c0849b69d..2d99b2e55 100644
--- a/clang/lib/Basic/Targets/SPIR.h
+++ b/clang/lib/Basic/Targets/SPIR.h
@@ -13,7 +13,7 @@
 #ifndef LLVM_CLANG_LIB_BASIC_TARGETS_SPIR_H
 #define LLVM_CLANG_LIB_BASIC_TARGETS_SPIR_H
 
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/TargetInfo.h"
 #include "clang/Basic/TargetOptions.h"
 #include "llvm/Support/Compiler.h"
@@ -190,9 +190,7 @@ public:
                                                             : CCCR_Warning;
   }
 
-  CallingConv getDefaultCallingConv() const override {
-    return CC_SpirFunction;
-  }
+  CallingConv getDefaultCallingConv() const override { return CC_SpirFunction; }
 
   void setAddressSpaceMap(bool DefaultIsGeneric) {
     AddrSpaceMap = DefaultIsGeneric ? &SPIRDefIsGenMap : &SPIRDefIsPrivMap;
diff --git a/clang/lib/Basic/Targets/Sparc.cpp b/clang/lib/Basic/Targets/Sparc.cpp
index d1a891092..b62e12a4a 100644
--- a/clang/lib/Basic/Targets/Sparc.cpp
+++ b/clang/lib/Basic/Targets/Sparc.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "Sparc.h"
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/MacroBuilder.h"
 #include "llvm/ADT/StringSwitch.h"
 
@@ -20,16 +20,88 @@ using namespace clang::targets;
 
 const char *const SparcTargetInfo::GCCRegNames[] = {
     // Integer registers
-    "r0",  "r1",  "r2",  "r3",  "r4",  "r5",  "r6",  "r7",  "r8",  "r9",  "r10",
-    "r11", "r12", "r13", "r14", "r15", "r16", "r17", "r18", "r19", "r20", "r21",
-    "r22", "r23", "r24", "r25", "r26", "r27", "r28", "r29", "r30", "r31",
+    "r0",
+    "r1",
+    "r2",
+    "r3",
+    "r4",
+    "r5",
+    "r6",
+    "r7",
+    "r8",
+    "r9",
+    "r10",
+    "r11",
+    "r12",
+    "r13",
+    "r14",
+    "r15",
+    "r16",
+    "r17",
+    "r18",
+    "r19",
+    "r20",
+    "r21",
+    "r22",
+    "r23",
+    "r24",
+    "r25",
+    "r26",
+    "r27",
+    "r28",
+    "r29",
+    "r30",
+    "r31",
 
     // Floating-point registers
-    "f0",  "f1",  "f2",  "f3",  "f4",  "f5",  "f6",  "f7",  "f8",  "f9",  "f10",
-    "f11", "f12", "f13", "f14", "f15", "f16", "f17", "f18", "f19", "f20", "f21",
-    "f22", "f23", "f24", "f25", "f26", "f27", "f28", "f29", "f30", "f31", "f32",
-    "f34", "f36", "f38", "f40", "f42", "f44", "f46", "f48", "f50", "f52", "f54",
-    "f56", "f58", "f60", "f62",
+    "f0",
+    "f1",
+    "f2",
+    "f3",
+    "f4",
+    "f5",
+    "f6",
+    "f7",
+    "f8",
+    "f9",
+    "f10",
+    "f11",
+    "f12",
+    "f13",
+    "f14",
+    "f15",
+    "f16",
+    "f17",
+    "f18",
+    "f19",
+    "f20",
+    "f21",
+    "f22",
+    "f23",
+    "f24",
+    "f25",
+    "f26",
+    "f27",
+    "f28",
+    "f29",
+    "f30",
+    "f31",
+    "f32",
+    "f34",
+    "f36",
+    "f38",
+    "f40",
+    "f42",
+    "f44",
+    "f46",
+    "f48",
+    "f50",
+    "f52",
+    "f54",
+    "f56",
+    "f58",
+    "f60",
+    "f62",
 };
 
 ArrayRef<const char *> SparcTargetInfo::getGCCRegNames() const {
diff --git a/clang/lib/Basic/Targets/TCE.cpp b/clang/lib/Basic/Targets/TCE.cpp
index 91194b568..a9be4d16a 100644
--- a/clang/lib/Basic/Targets/TCE.cpp
+++ b/clang/lib/Basic/Targets/TCE.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "TCE.h"
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/MacroBuilder.h"
 
 using namespace clang;
diff --git a/clang/lib/Basic/Targets/WebAssembly.cpp b/clang/lib/Basic/Targets/WebAssembly.cpp
index 7b0fd0c84..8851d75e4 100644
--- a/clang/lib/Basic/Targets/WebAssembly.cpp
+++ b/clang/lib/Basic/Targets/WebAssembly.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "WebAssembly.h"
-#include "Targets.h"
+#include "../Targets.h"
 #include "clang/Basic/Builtins.h"
 #include "clang/Basic/Diagnostic.h"
 #include "clang/Basic/TargetBuiltins.h"
diff --git a/clang/lib/CodeGen/CMakeLists.txt b/clang/lib/CodeGen/CMakeLists.txt
index 868ec847b..ea3f1ae2d 100644
--- a/clang/lib/CodeGen/CMakeLists.txt
+++ b/clang/lib/CodeGen/CMakeLists.txt
@@ -147,6 +147,7 @@ add_clang_library(clangCodeGen
   vt_gen
   intrinsics_gen
   ClangDriverOptions
+  ClangDiagnosticCommon
   # These generated headers are included transitively.
   ARMTargetParserTableGen
   AArch64TargetParserTableGen
diff --git a/clang/lib/CodeGen/Targets/AArch64.cpp b/clang/lib/CodeGen/Targets/AArch64.cpp
index 170ce1640..36856e3ba 100644
--- a/clang/lib/CodeGen/Targets/AArch64.cpp
+++ b/clang/lib/CodeGen/Targets/AArch64.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "clang/AST/Decl.h"
 #include "clang/Basic/DiagnosticFrontend.h"
 #include "llvm/TargetParser/AArch64TargetParser.h"
@@ -222,7 +222,7 @@ void WindowsAArch64TargetCodeGenInfo::setTargetAttributes(
     return;
   addStackProbeTargetAttributes(D, GV, CGM);
 }
-}
+} // namespace
 
 llvm::Type *
 AArch64ABIInfo::convertFixedToScalableVectorType(const VectorType *VT) const {
@@ -412,7 +412,7 @@ ABIArgInfo AArch64ABIInfo::classifyArgumentType(QualType Ty, bool IsVariadicFn,
   // copy constructor are always indirect.
   if (CGCXXABI::RecordArgABI RAA = getRecordArgABI(Ty, getCXXABI())) {
     return getNaturalAlignIndirect(Ty, /*ByVal=*/RAA ==
-                                     CGCXXABI::RAA_DirectInMemory);
+                                           CGCXXABI::RAA_DirectInMemory);
   }
 
   // Empty records are always ignored on Darwin, but actually passed in C++ mode
@@ -973,8 +973,8 @@ RValue AArch64ABIInfo::EmitAAPCSVAArg(Address VAListAddr, QualType Ty,
     auto BaseTyInfo = getContext().getTypeInfoInChars(QualType(Base, 0));
     llvm::Type *BaseTy = CGF.ConvertType(QualType(Base, 0));
     llvm::Type *HFATy = llvm::ArrayType::get(BaseTy, NumMembers);
-    Address Tmp = CGF.CreateTempAlloca(HFATy,
-                                       std::max(TyAlign, BaseTyInfo.Align));
+    Address Tmp =
+        CGF.CreateTempAlloca(HFATy, std::max(TyAlign, BaseTyInfo.Align));
 
     // On big-endian platforms, the value will be right-aligned in its slot.
     int Offset = 0;
@@ -985,7 +985,7 @@ RValue AArch64ABIInfo::EmitAAPCSVAArg(Address VAListAddr, QualType Ty,
     for (unsigned i = 0; i < NumMembers; ++i) {
       CharUnits BaseOffset = CharUnits::fromQuantity(16 * i + Offset);
       Address LoadAddr =
-        CGF.Builder.CreateConstInBoundsByteGEP(BaseAddr, BaseOffset);
+          CGF.Builder.CreateConstInBoundsByteGEP(BaseAddr, BaseOffset);
       LoadAddr = LoadAddr.withElementType(BaseTy);
 
       Address StoreAddr = CGF.Builder.CreateConstArrayGEP(Tmp, i);
@@ -1001,8 +1001,7 @@ RValue AArch64ABIInfo::EmitAAPCSVAArg(Address VAListAddr, QualType Ty,
     // It might be right-aligned in its slot.
     CharUnits SlotSize = BaseAddr.getAlignment();
     if (CGF.CGM.getDataLayout().isBigEndian() && !IsIndirect &&
-        (IsHFA || !isAggregateTypeForABI(Ty)) &&
-        TySize < SlotSize) {
+        (IsHFA || !isAggregateTypeForABI(Ty)) && TySize < SlotSize) {
       CharUnits Offset = SlotSize - TySize;
       BaseAddr = CGF.Builder.CreateConstInBoundsByteGEP(BaseAddr, Offset);
     }
diff --git a/clang/lib/CodeGen/Targets/AMDGPU.cpp b/clang/lib/CodeGen/Targets/AMDGPU.cpp
index 788eac5f2..faca92533 100644
--- a/clang/lib/CodeGen/Targets/AMDGPU.cpp
+++ b/clang/lib/CodeGen/Targets/AMDGPU.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "clang/Basic/TargetOptions.h"
 #include "llvm/Support/AMDGPUAddrSpace.h"
 
@@ -41,8 +41,7 @@ private:
   }
 
 public:
-  explicit AMDGPUABIInfo(CodeGen::CodeGenTypes &CGT) :
-    DefaultABIInfo(CGT) {}
+  explicit AMDGPUABIInfo(CodeGen::CodeGenTypes &CGT) : DefaultABIInfo(CGT) {}
 
   ABIArgInfo classifyReturnType(QualType RetTy) const;
   ABIArgInfo classifyKernelArgumentType(QualType Ty) const;
@@ -69,8 +68,8 @@ bool AMDGPUABIInfo::isHomogeneousAggregateBaseType(QualType Ty) const {
   return true;
 }
 
-bool AMDGPUABIInfo::isHomogeneousAggregateSmallEnough(
-  const Type *Base, uint64_t Members) const {
+bool AMDGPUABIInfo::isHomogeneousAggregateSmallEnough(const Type *Base,
+                                                      uint64_t Members) const {
   uint32_t NumRegs = (getContext().getTypeSize(Base) + 31) / 32;
 
   // Homogeneous Aggregates may occupy at most 16 registers.
@@ -311,7 +310,8 @@ public:
   unsigned getOpenCLKernelCallingConv() const override;
 
   llvm::Constant *getNullPointer(const CodeGen::CodeGenModule &CGM,
-      llvm::PointerType *T, QualType QT) const override;
+                                 llvm::PointerType *T,
+                                 QualType QT) const override;
 
   LangAS getASTAllocaAddressSpace() const override {
     return getLangASFromTargetAS(
@@ -333,7 +333,7 @@ public:
   bool shouldEmitDWARFBitFieldSeparators() const override;
   void setCUDAKernelCallingConvention(const FunctionType *&FT) const override;
 };
-}
+} // namespace
 
 static bool requiresAMDGPUProtectedVisibility(const Decl *D,
                                               llvm::GlobalValue *GV) {
@@ -419,7 +419,8 @@ void AMDGPUTargetCodeGenInfo::emitTargetGlobals(
     CodeGen::CodeGenModule &CGM) const {
   StringRef Name = "__oclc_ABI_version";
   llvm::GlobalVariable *OriginalGV = CGM.getModule().getNamedGlobal(Name);
-  if (OriginalGV && !llvm::GlobalVariable::isExternalLinkage(OriginalGV->getLinkage()))
+  if (OriginalGV &&
+      !llvm::GlobalVariable::isExternalLinkage(OriginalGV->getLinkage()))
     return;
 
   if (CGM.getTarget().getTargetOpts().CodeObjectVersion ==
@@ -478,9 +479,10 @@ unsigned AMDGPUTargetCodeGenInfo::getOpenCLKernelCallingConv() const {
 // emitting null pointers in private and local address spaces, a null
 // pointer in generic address space is emitted which is casted to a
 // pointer in local or private address space.
-llvm::Constant *AMDGPUTargetCodeGenInfo::getNullPointer(
-    const CodeGen::CodeGenModule &CGM, llvm::PointerType *PT,
-    QualType QT) const {
+llvm::Constant *
+AMDGPUTargetCodeGenInfo::getNullPointer(const CodeGen::CodeGenModule &CGM,
+                                        llvm::PointerType *PT,
+                                        QualType QT) const {
   if (CGM.getContext().getTargetNullPointerValue(QT) == 0)
     return llvm::ConstantPointerNull::get(PT);
 
@@ -515,11 +517,9 @@ AMDGPUTargetCodeGenInfo::getGlobalVarAddressSpace(CodeGenModule &CGM,
   return DefaultGlobalAS;
 }
 
-llvm::SyncScope::ID
-AMDGPUTargetCodeGenInfo::getLLVMSyncScopeID(const LangOptions &LangOpts,
-                                            SyncScope Scope,
-                                            llvm::AtomicOrdering Ordering,
-                                            llvm::LLVMContext &Ctx) const {
+llvm::SyncScope::ID AMDGPUTargetCodeGenInfo::getLLVMSyncScopeID(
+    const LangOptions &LangOpts, SyncScope Scope, llvm::AtomicOrdering Ordering,
+    llvm::LLVMContext &Ctx) const {
   std::string Name;
   switch (Scope) {
   case SyncScope::HIPSingleThread:
diff --git a/clang/lib/CodeGen/Targets/ARC.cpp b/clang/lib/CodeGen/Targets/ARC.cpp
index 1904e8fdb..59d48cc9b 100644
--- a/clang/lib/CodeGen/Targets/ARC.cpp
+++ b/clang/lib/CodeGen/Targets/ARC.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -67,10 +67,9 @@ public:
       : TargetCodeGenInfo(std::make_unique<ARCABIInfo>(CGT)) {}
 };
 
-
 ABIArgInfo ARCABIInfo::getIndirectByRef(QualType Ty, bool HasFreeRegs) const {
-  return HasFreeRegs ? getNaturalAlignIndirectInReg(Ty) :
-                       getNaturalAlignIndirect(Ty, false);
+  return HasFreeRegs ? getNaturalAlignIndirectInReg(Ty)
+                     : getNaturalAlignIndirect(Ty, false);
 }
 
 ABIArgInfo ARCABIInfo::getIndirectByValue(QualType Ty) const {
@@ -122,9 +121,9 @@ ABIArgInfo ARCABIInfo::classifyArgumentType(QualType Ty,
     SmallVector<llvm::Type *, 3> Elements(SizeInRegs, Int32);
     llvm::Type *Result = llvm::StructType::get(LLVMContext, Elements);
 
-    return FreeRegs >= SizeInRegs ?
-        ABIArgInfo::getDirectInReg(Result) :
-        ABIArgInfo::getDirect(Result, 0, nullptr, false);
+    return FreeRegs >= SizeInRegs
+               ? ABIArgInfo::getDirectInReg(Result)
+               : ABIArgInfo::getDirect(Result, 0, nullptr, false);
   }
 
   if (const auto *EIT = Ty->getAs<BitIntType>())
diff --git a/clang/lib/CodeGen/Targets/ARM.cpp b/clang/lib/CodeGen/Targets/ARM.cpp
index 77641ce10..057486c0c 100644
--- a/clang/lib/CodeGen/Targets/ARM.cpp
+++ b/clang/lib/CodeGen/Targets/ARM.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -26,7 +26,7 @@ public:
   ARMABIInfo(CodeGenTypes &CGT, ARMABIKind Kind) : ABIInfo(CGT), Kind(Kind) {
     setCCs();
     IsFloatABISoftFP = CGT.getCodeGenOpts().FloatABI == "softfp" ||
-        CGT.getCodeGenOpts().FloatABI == ""; // default
+                       CGT.getCodeGenOpts().FloatABI == ""; // default
   }
 
   bool isEABI() const {
@@ -180,12 +180,24 @@ public:
 
     const char *Kind;
     switch (Attr->getInterrupt()) {
-    case ARMInterruptAttr::Generic: Kind = ""; break;
-    case ARMInterruptAttr::IRQ:     Kind = "IRQ"; break;
-    case ARMInterruptAttr::FIQ:     Kind = "FIQ"; break;
-    case ARMInterruptAttr::SWI:     Kind = "SWI"; break;
-    case ARMInterruptAttr::ABORT:   Kind = "ABORT"; break;
-    case ARMInterruptAttr::UNDEF:   Kind = "UNDEF"; break;
+    case ARMInterruptAttr::Generic:
+      Kind = "";
+      break;
+    case ARMInterruptAttr::IRQ:
+      Kind = "IRQ";
+      break;
+    case ARMInterruptAttr::FIQ:
+      Kind = "FIQ";
+      break;
+    case ARMInterruptAttr::SWI:
+      Kind = "SWI";
+      break;
+    case ARMInterruptAttr::ABORT:
+      Kind = "ABORT";
+      break;
+    case ARMInterruptAttr::UNDEF:
+      Kind = "UNDEF";
+      break;
     }
 
     Fn->addFnAttr("interrupt", Kind);
@@ -229,7 +241,7 @@ void WindowsARMTargetCodeGenInfo::setTargetAttributes(
     return;
   addStackProbeTargetAttributes(D, GV, CGM);
 }
-}
+} // namespace
 
 void ARMABIInfo::computeInfo(CGFunctionInfo &FI) const {
   if (!::classifyReturnType(getCXXABI(), FI, *this))
@@ -240,7 +252,6 @@ void ARMABIInfo::computeInfo(CGFunctionInfo &FI) const {
     I.info = classifyArgumentType(I.type, FI.isVariadic(),
                                   FI.getCallingConvention());
 
-
   // Always honor user-specified calling convention.
   if (FI.getCallingConvention() != llvm::CallingConv::C)
     return;
@@ -290,8 +301,7 @@ void ARMABIInfo::setCCs() {
 ABIArgInfo ARMABIInfo::coerceIllegalVector(QualType Ty) const {
   uint64_t Size = getContext().getTypeSize(Ty);
   if (Size <= 32) {
-    llvm::Type *ResType =
-        llvm::Type::getInt32Ty(getVMContext());
+    llvm::Type *ResType = llvm::Type::getInt32Ty(getVMContext());
     return ABIArgInfo::getDirect(ResType);
   }
   if (Size == 64 || Size == 128) {
@@ -364,8 +374,8 @@ ABIArgInfo ARMABIInfo::classifyArgumentType(QualType Ty, bool isVariadic,
   //   64-bit containerized vectors or 128-bit containerized vectors with one
   //   to four Elements.
   // Variadic functions should always marshal to the base standard.
-  bool IsAAPCS_VFP =
-      !isVariadic && isEffectivelyAAPCS_VFP(functionCallConv, /* AAPCS16 */ false);
+  bool IsAAPCS_VFP = !isVariadic && isEffectivelyAAPCS_VFP(functionCallConv,
+                                                           /* AAPCS16 */ false);
 
   Ty = useFirstFieldIfTransparentUnion(Ty);
 
@@ -418,7 +428,7 @@ ABIArgInfo ARMABIInfo::classifyArgumentType(QualType Ty, bool isVariadic,
     if (isHomogeneousAggregate(Ty, Base, Members)) {
       assert(Base && Members <= 4 && "unexpected homogeneous aggregate");
       llvm::Type *Ty =
-        llvm::ArrayType::get(CGT.ConvertType(QualType(Base, 0)), Members);
+          llvm::ArrayType::get(CGT.ConvertType(QualType(Base, 0)), Members);
       return ABIArgInfo::getDirect(Ty, 0, nullptr, false);
     }
   }
@@ -453,7 +463,7 @@ ABIArgInfo ARMABIInfo::classifyArgumentType(QualType Ty, bool isVariadic,
   }
 
   // Otherwise, pass by coercing to a structure of the appropriate size.
-  llvm::Type* ElemTy;
+  llvm::Type *ElemTy;
   unsigned SizeRegs;
   // FIXME: Try to match the types of the arguments more accurately where
   // we can.
@@ -501,7 +511,8 @@ static bool isIntegerLikeType(QualType Ty, ASTContext &Context,
 
   // Otherwise, it must be a record type.
   const RecordType *RT = Ty->getAs<RecordType>();
-  if (!RT) return false;
+  if (!RT)
+    return false;
 
   // Ignore records with flexible arrays.
   const RecordDecl *RD = RT->getDecl();
@@ -557,8 +568,8 @@ ABIArgInfo ARMABIInfo::classifyReturnType(QualType RetTy, bool isVariadic,
                                           unsigned functionCallConv) const {
 
   // Variadic functions should always marshal to the base standard.
-  bool IsAAPCS_VFP =
-      !isVariadic && isEffectivelyAAPCS_VFP(functionCallConv, /* AAPCS16 */ true);
+  bool IsAAPCS_VFP = !isVariadic && isEffectivelyAAPCS_VFP(functionCallConv,
+                                                           /* AAPCS16 */ true);
 
   if (RetTy->isVoidType())
     return ABIArgInfo::getIgnore();
@@ -570,10 +581,9 @@ ABIArgInfo ARMABIInfo::classifyReturnType(QualType RetTy, bool isVariadic,
     // TODO: FP16/BF16 vectors should be converted to integer vectors
     // This check is similar  to isIllegalVectorType - refactor?
     if ((!getTarget().hasLegalHalfType() &&
-        (VT->getElementType()->isFloat16Type() ||
-         VT->getElementType()->isHalfType())) ||
-        (IsFloatABISoftFP &&
-         VT->getElementType()->isBFloat16Type()))
+         (VT->getElementType()->isFloat16Type() ||
+          VT->getElementType()->isHalfType())) ||
+        (IsFloatABISoftFP && VT->getElementType()->isBFloat16Type()))
       return coerceIllegalVector(RetTy);
   }
 
@@ -658,7 +668,7 @@ ABIArgInfo ARMABIInfo::classifyReturnType(QualType RetTy, bool isVariadic,
 
 /// isIllegalVector - check whether Ty is an illegal vector type.
 bool ARMABIInfo::isIllegalVectorType(QualType Ty) const {
-  if (const VectorType *VT = Ty->getAs<VectorType> ()) {
+  if (const VectorType *VT = Ty->getAs<VectorType>()) {
     // On targets that don't support half, fp16 or bfloat, they are expanded
     // into float, and we don't want the ABI to depend on whether or not they
     // are supported in hardware. Thus return false to coerce vectors of these
@@ -666,10 +676,9 @@ bool ARMABIInfo::isIllegalVectorType(QualType Ty) const {
     // We do not depend on hasLegalHalfType for bfloat as it is a
     // separate IR type.
     if ((!getTarget().hasLegalHalfType() &&
-        (VT->getElementType()->isFloat16Type() ||
-         VT->getElementType()->isHalfType())) ||
-        (IsFloatABISoftFP &&
-         VT->getElementType()->isBFloat16Type()))
+         (VT->getElementType()->isFloat16Type() ||
+          VT->getElementType()->isHalfType())) ||
+        (IsFloatABISoftFP && VT->getElementType()->isBFloat16Type()))
       return true;
     if (isAndroid()) {
       // Android shipped using Clang 3.1, which supported a slightly different
@@ -800,17 +809,17 @@ RValue ARMABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
   if (TySize > CharUnits::fromQuantity(16) && isIllegalVectorType(Ty)) {
     IsIndirect = true;
 
-  // ARMv7k passes structs bigger than 16 bytes indirectly, in space
-  // allocated by the caller.
+    // ARMv7k passes structs bigger than 16 bytes indirectly, in space
+    // allocated by the caller.
   } else if (TySize > CharUnits::fromQuantity(16) &&
              getABIKind() == ARMABIKind::AAPCS16_VFP &&
              !isHomogeneousAggregate(Ty, Base, Members)) {
     IsIndirect = true;
 
-  // Otherwise, bound the type's ABI alignment.
-  // The ABI alignment for 64-bit or 128-bit vectors is 8 for AAPCS and 4 for
-  // APCS. For AAPCS, the ABI alignment is at least 4-byte and at most 8-byte.
-  // Our callers should be prepared to handle an under-aligned address.
+    // Otherwise, bound the type's ABI alignment.
+    // The ABI alignment for 64-bit or 128-bit vectors is 8 for AAPCS and 4 for
+    // APCS. For AAPCS, the ABI alignment is at least 4-byte and at most 8-byte.
+    // Our callers should be prepared to handle an under-aligned address.
   } else if (getABIKind() == ARMABIKind::AAPCS_VFP ||
              getABIKind() == ARMABIKind::AAPCS) {
     TyAlignForABI = std::max(TyAlignForABI, CharUnits::fromQuantity(4));
diff --git a/clang/lib/CodeGen/Targets/AVR.cpp b/clang/lib/CodeGen/Targets/AVR.cpp
index 50547dd6d..80c759d51 100644
--- a/clang/lib/CodeGen/Targets/AVR.cpp
+++ b/clang/lib/CodeGen/Targets/AVR.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "clang/Basic/DiagnosticFrontend.h"
 
 using namespace clang;
@@ -135,7 +135,8 @@ public:
     if (GV->isDeclaration())
       return;
     const auto *FD = dyn_cast_or_null<FunctionDecl>(D);
-    if (!FD) return;
+    if (!FD)
+      return;
     auto *Fn = cast<llvm::Function>(GV);
 
     if (FD->getAttr<AVRInterruptAttr>())
@@ -145,7 +146,7 @@ public:
       Fn->addFnAttr("signal");
   }
 };
-}
+} // namespace
 
 std::unique_ptr<TargetCodeGenInfo>
 CodeGen::createAVRTargetCodeGenInfo(CodeGenModule &CGM, unsigned NPR,
diff --git a/clang/lib/CodeGen/Targets/BPF.cpp b/clang/lib/CodeGen/Targets/BPF.cpp
index 2849222f7..21fc99f7c 100644
--- a/clang/lib/CodeGen/Targets/BPF.cpp
+++ b/clang/lib/CodeGen/Targets/BPF.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -83,7 +83,6 @@ public:
     for (auto &I : FI.arguments())
       I.info = classifyArgumentType(I.type);
   }
-
 };
 
 class BPFTargetCodeGenInfo : public TargetCodeGenInfo {
@@ -92,7 +91,7 @@ public:
       : TargetCodeGenInfo(std::make_unique<BPFABIInfo>(CGT)) {}
 };
 
-}
+} // namespace
 
 std::unique_ptr<TargetCodeGenInfo>
 CodeGen::createBPFTargetCodeGenInfo(CodeGenModule &CGM) {
diff --git a/clang/lib/CodeGen/Targets/CSKY.cpp b/clang/lib/CodeGen/Targets/CSKY.cpp
index d8720afd1..456cb21b7 100644
--- a/clang/lib/CodeGen/Targets/CSKY.cpp
+++ b/clang/lib/CodeGen/Targets/CSKY.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
diff --git a/clang/lib/CodeGen/Targets/DirectX.cpp b/clang/lib/CodeGen/Targets/DirectX.cpp
index 7935f7ae3..42fb46cf0 100644
--- a/clang/lib/CodeGen/Targets/DirectX.cpp
+++ b/clang/lib/CodeGen/Targets/DirectX.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "llvm/IR/DerivedTypes.h"
 
 using namespace clang;
diff --git a/clang/lib/CodeGen/Targets/Hexagon.cpp b/clang/lib/CodeGen/Targets/Hexagon.cpp
index 8fd2a8149..62158280d 100644
--- a/clang/lib/CodeGen/Targets/Hexagon.cpp
+++ b/clang/lib/CodeGen/Targets/Hexagon.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -145,8 +145,8 @@ ABIArgInfo HexagonABIInfo::classifyReturnType(QualType RetTy) const {
     // HVX vectors are returned in vector registers or register pairs.
     if (T.hasFeature("hvx")) {
       assert(T.hasFeature("hvx-length64b") || T.hasFeature("hvx-length128b"));
-      uint64_t VecSize = T.hasFeature("hvx-length64b") ? 64*8 : 128*8;
-      if (Size == VecSize || Size == 2*VecSize)
+      uint64_t VecSize = T.hasFeature("hvx-length64b") ? 64 * 8 : 128 * 8;
+      if (Size == VecSize || Size == 2 * VecSize)
         return ABIArgInfo::getDirectInReg();
     }
     // Large vector types should be returned via memory.
@@ -219,10 +219,10 @@ Address HexagonABIInfo::EmitVAArgFromMemory(CodeGenFunction &CGF,
   // Round up to the minimum stack alignment for varargs which is 4 bytes.
   uint64_t Offset = llvm::alignTo(CGF.getContext().getTypeSize(Ty) / 8, 4);
 
-  __overflow_area_pointer = CGF.Builder.CreateGEP(
-      CGF.Int8Ty, __overflow_area_pointer,
-      llvm::ConstantInt::get(CGF.Int32Ty, Offset),
-      "__overflow_area_pointer.next");
+  __overflow_area_pointer =
+      CGF.Builder.CreateGEP(CGF.Int8Ty, __overflow_area_pointer,
+                            llvm::ConstantInt::get(CGF.Int32Ty, Offset),
+                            "__overflow_area_pointer.next");
   CGF.Builder.CreateStore(__overflow_area_pointer, __overflow_area_pointer_p);
 
   return AddrTyped;
@@ -377,10 +377,10 @@ Address HexagonABIInfo::EmitVAArgForHexagonLinux(CodeGenFunction &CGF,
 
   // Get the pointer for next argument in overflow area and store it
   // to overflow area pointer.
-  llvm::Value *__new_overflow_area_pointer = CGF.Builder.CreateGEP(
-      CGF.Int8Ty, __overflow_area_pointer,
-      llvm::ConstantInt::get(CGF.Int32Ty, ArgSize),
-      "__overflow_area_pointer.next");
+  llvm::Value *__new_overflow_area_pointer =
+      CGF.Builder.CreateGEP(CGF.Int8Ty, __overflow_area_pointer,
+                            llvm::ConstantInt::get(CGF.Int32Ty, ArgSize),
+                            "__overflow_area_pointer.next");
 
   CGF.Builder.CreateStore(__new_overflow_area_pointer,
                           __overflow_area_pointer_p);
diff --git a/clang/lib/CodeGen/Targets/Lanai.cpp b/clang/lib/CodeGen/Targets/Lanai.cpp
index 2578fc029..cecd48140 100644
--- a/clang/lib/CodeGen/Targets/Lanai.cpp
+++ b/clang/lib/CodeGen/Targets/Lanai.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -146,7 +146,7 @@ public:
   LanaiTargetCodeGenInfo(CodeGen::CodeGenTypes &CGT)
       : TargetCodeGenInfo(std::make_unique<LanaiABIInfo>(CGT)) {}
 };
-}
+} // namespace
 
 std::unique_ptr<TargetCodeGenInfo>
 CodeGen::createLanaiTargetCodeGenInfo(CodeGenModule &CGM) {
diff --git a/clang/lib/CodeGen/Targets/LoongArch.cpp b/clang/lib/CodeGen/Targets/LoongArch.cpp
index 6c90e48a5..f15d9bf24 100644
--- a/clang/lib/CodeGen/Targets/LoongArch.cpp
+++ b/clang/lib/CodeGen/Targets/LoongArch.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
diff --git a/clang/lib/CodeGen/Targets/M68k.cpp b/clang/lib/CodeGen/Targets/M68k.cpp
index 120022105..e2ddac979 100644
--- a/clang/lib/CodeGen/Targets/M68k.cpp
+++ b/clang/lib/CodeGen/Targets/M68k.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
diff --git a/clang/lib/CodeGen/Targets/MSP430.cpp b/clang/lib/CodeGen/Targets/MSP430.cpp
index 8ce70e211..11578a39b 100644
--- a/clang/lib/CodeGen/Targets/MSP430.cpp
+++ b/clang/lib/CodeGen/Targets/MSP430.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -68,7 +68,7 @@ public:
                            CodeGen::CodeGenModule &M) const override;
 };
 
-}
+} // namespace
 
 void MSP430TargetCodeGenInfo::setTargetAttributes(
     const Decl *D, llvm::GlobalValue *GV, CodeGen::CodeGenModule &M) const {
diff --git a/clang/lib/CodeGen/Targets/Mips.cpp b/clang/lib/CodeGen/Targets/Mips.cpp
index 771a85c84..03632e83d 100644
--- a/clang/lib/CodeGen/Targets/Mips.cpp
+++ b/clang/lib/CodeGen/Targets/Mips.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -23,13 +23,14 @@ class MipsABIInfo : public ABIInfo {
   const unsigned MinABIStackAlignInBytes, StackAlignInBytes;
   void CoerceToIntArgs(uint64_t TySize,
                        SmallVectorImpl<llvm::Type *> &ArgList) const;
-  llvm::Type* HandleAggregates(QualType Ty, uint64_t TySize) const;
-  llvm::Type* returnAggregateInRegs(QualType RetTy, uint64_t Size) const;
-  llvm::Type* getPaddingType(uint64_t Align, uint64_t Offset) const;
+  llvm::Type *HandleAggregates(QualType Ty, uint64_t TySize) const;
+  llvm::Type *returnAggregateInRegs(QualType RetTy, uint64_t Size) const;
+  llvm::Type *getPaddingType(uint64_t Align, uint64_t Offset) const;
+
 public:
-  MipsABIInfo(CodeGenTypes &CGT, bool _IsO32) :
-    ABIInfo(CGT), IsO32(_IsO32), MinABIStackAlignInBytes(IsO32 ? 4 : 8),
-    StackAlignInBytes(IsO32 ? 8 : 16) {}
+  MipsABIInfo(CodeGenTypes &CGT, bool _IsO32)
+      : ABIInfo(CGT), IsO32(_IsO32), MinABIStackAlignInBytes(IsO32 ? 4 : 8),
+        StackAlignInBytes(IsO32 ? 8 : 16) {}
 
   ABIArgInfo classifyReturnType(QualType RetTy) const;
   ABIArgInfo classifyArgumentType(QualType RetTy, uint64_t &Offset) const;
@@ -41,6 +42,7 @@ public:
 
 class MIPSTargetCodeGenInfo : public TargetCodeGenInfo {
   unsigned SizeOfUnwindException;
+
 public:
   MIPSTargetCodeGenInfo(CodeGenTypes &CGT, bool IsO32)
       : TargetCodeGenInfo(std::make_unique<MipsABIInfo>(CGT, IsO32)),
@@ -53,7 +55,8 @@ public:
   void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
                            CodeGen::CodeGenModule &CGM) const override {
     const FunctionDecl *FD = dyn_cast_or_null<FunctionDecl>(D);
-    if (!FD) return;
+    if (!FD)
+      return;
     llvm::Function *Fn = cast<llvm::Function>(GV);
 
     if (FD->hasAttr<MipsLongCallAttr>())
@@ -67,8 +70,7 @@ public:
 
     if (FD->hasAttr<Mips16Attr>()) {
       Fn->addFnAttr("mips16");
-    }
-    else if (FD->hasAttr<NoMips16Attr>()) {
+    } else if (FD->hasAttr<NoMips16Attr>()) {
       Fn->addFnAttr("nomips16");
     }
 
@@ -83,19 +85,36 @@ public:
 
     const char *Kind;
     switch (Attr->getInterrupt()) {
-    case MipsInterruptAttr::eic:     Kind = "eic"; break;
-    case MipsInterruptAttr::sw0:     Kind = "sw0"; break;
-    case MipsInterruptAttr::sw1:     Kind = "sw1"; break;
-    case MipsInterruptAttr::hw0:     Kind = "hw0"; break;
-    case MipsInterruptAttr::hw1:     Kind = "hw1"; break;
-    case MipsInterruptAttr::hw2:     Kind = "hw2"; break;
-    case MipsInterruptAttr::hw3:     Kind = "hw3"; break;
-    case MipsInterruptAttr::hw4:     Kind = "hw4"; break;
-    case MipsInterruptAttr::hw5:     Kind = "hw5"; break;
+    case MipsInterruptAttr::eic:
+      Kind = "eic";
+      break;
+    case MipsInterruptAttr::sw0:
+      Kind = "sw0";
+      break;
+    case MipsInterruptAttr::sw1:
+      Kind = "sw1";
+      break;
+    case MipsInterruptAttr::hw0:
+      Kind = "hw0";
+      break;
+    case MipsInterruptAttr::hw1:
+      Kind = "hw1";
+      break;
+    case MipsInterruptAttr::hw2:
+      Kind = "hw2";
+      break;
+    case MipsInterruptAttr::hw3:
+      Kind = "hw3";
+      break;
+    case MipsInterruptAttr::hw4:
+      Kind = "hw4";
+      break;
+    case MipsInterruptAttr::hw5:
+      Kind = "hw5";
+      break;
     }
 
     Fn->addFnAttr("interrupt", Kind);
-
   }
 
   bool initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
@@ -122,12 +141,12 @@ public:
     Opt = "/FAILIFMISMATCH:\"" + Name.str() + "=" + Value.str() + "\"";
   }
 };
-}
+} // namespace
 
 void MipsABIInfo::CoerceToIntArgs(
     uint64_t TySize, SmallVectorImpl<llvm::Type *> &ArgList) const {
   llvm::IntegerType *IntTy =
-    llvm::IntegerType::get(getVMContext(), MinABIStackAlignInBytes * 8);
+      llvm::IntegerType::get(getVMContext(), MinABIStackAlignInBytes * 8);
 
   // Add (TySize / MinABIStackAlignInBytes) args of IntTy.
   for (unsigned N = TySize / (MinABIStackAlignInBytes * 8); N; --N)
@@ -142,8 +161,8 @@ void MipsABIInfo::CoerceToIntArgs(
 
 // In N32/64, an aligned double precision floating point field is passed in
 // a register.
-llvm::Type* MipsABIInfo::HandleAggregates(QualType Ty, uint64_t TySize) const {
-  SmallVector<llvm::Type*, 8> ArgList, IntArgList;
+llvm::Type *MipsABIInfo::HandleAggregates(QualType Ty, uint64_t TySize) const {
+  SmallVector<llvm::Type *, 8> ArgList, IntArgList;
 
   if (IsO32) {
     CoerceToIntArgs(TySize, ArgList);
@@ -206,8 +225,8 @@ llvm::Type *MipsABIInfo::getPaddingType(uint64_t OrigOffset,
   return llvm::IntegerType::get(getVMContext(), (Offset - OrigOffset) * 8);
 }
 
-ABIArgInfo
-MipsABIInfo::classifyArgumentType(QualType Ty, uint64_t &Offset) const {
+ABIArgInfo MipsABIInfo::classifyArgumentType(QualType Ty,
+                                             uint64_t &Offset) const {
   Ty = useFirstFieldIfTransparentUnion(Ty);
 
   uint64_t OrigOffset = Offset;
@@ -258,10 +277,10 @@ MipsABIInfo::classifyArgumentType(QualType Ty, uint64_t &Offset) const {
       nullptr, 0, IsO32 ? nullptr : getPaddingType(OrigOffset, CurrOffset));
 }
 
-llvm::Type*
-MipsABIInfo::returnAggregateInRegs(QualType RetTy, uint64_t Size) const {
+llvm::Type *MipsABIInfo::returnAggregateInRegs(QualType RetTy,
+                                               uint64_t Size) const {
   const RecordType *RT = RetTy->getAs<RecordType>();
-  SmallVector<llvm::Type*, 8> RTList;
+  SmallVector<llvm::Type *, 8> RTList;
 
   if (RT && RT->isStructureOrClassType()) {
     const RecordDecl *RD = RT->getDecl();
@@ -345,7 +364,8 @@ ABIArgInfo MipsABIInfo::classifyReturnType(QualType RetTy) const {
     return ABIArgInfo::getExtend(RetTy);
 
   if ((RetTy->isUnsignedIntegerOrEnumerationType() ||
-      RetTy->isSignedIntegerOrEnumerationType()) && Size == 32 && !IsO32)
+       RetTy->isSignedIntegerOrEnumerationType()) &&
+      Size == 32 && !IsO32)
     return ABIArgInfo::getSignExtend(RetTy);
 
   return ABIArgInfo::getDirect();
@@ -372,8 +392,7 @@ RValue MipsABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
   unsigned SlotSizeInBits = IsO32 ? 32 : 64;
   unsigned PtrWidth = getTarget().getPointerWidth(LangAS::Default);
   bool DidPromote = false;
-  if ((Ty->isIntegerType() &&
-          getContext().getIntWidth(Ty) < SlotSizeInBits) ||
+  if ((Ty->isIntegerType() && getContext().getIntWidth(Ty) < SlotSizeInBits) ||
       (Ty->isPointerType() && PtrWidth < SlotSizeInBits)) {
     DidPromote = true;
     Ty = getContext().getIntTypeForBitwidth(SlotSizeInBits,
@@ -385,7 +404,7 @@ RValue MipsABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
   // The alignment of things in the argument area is never larger than
   // StackAlignInBytes.
   TyInfo.Align =
-    std::min(TyInfo.Align, CharUnits::fromQuantity(StackAlignInBytes));
+      std::min(TyInfo.Align, CharUnits::fromQuantity(StackAlignInBytes));
 
   // MinABIStackAlignInBytes is the size of argument slots on the stack.
   CharUnits ArgSlotSize = CharUnits::fromQuantity(MinABIStackAlignInBytes);
@@ -421,9 +440,8 @@ ABIArgInfo MipsABIInfo::extendType(QualType Ty) const {
   return ABIArgInfo::getExtend(Ty);
 }
 
-bool
-MIPSTargetCodeGenInfo::initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
-                                               llvm::Value *Address) const {
+bool MIPSTargetCodeGenInfo::initDwarfEHRegSizeTable(
+    CodeGen::CodeGenFunction &CGF, llvm::Value *Address) const {
   // This information comes from gcc's implementation, which seems to
   // as canonical as it gets.
 
diff --git a/clang/lib/CodeGen/Targets/NVPTX.cpp b/clang/lib/CodeGen/Targets/NVPTX.cpp
index b82e4ddb9..99edcc3bf 100644
--- a/clang/lib/CodeGen/Targets/NVPTX.cpp
+++ b/clang/lib/CodeGen/Targets/NVPTX.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
diff --git a/clang/lib/CodeGen/Targets/PNaCl.cpp b/clang/lib/CodeGen/Targets/PNaCl.cpp
index 9b7d757df..5fe099417 100644
--- a/clang/lib/CodeGen/Targets/PNaCl.cpp
+++ b/clang/lib/CodeGen/Targets/PNaCl.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -20,7 +20,7 @@ using namespace clang::CodeGen;
 //===----------------------------------------------------------------------===//
 
 class PNaClABIInfo : public ABIInfo {
- public:
+public:
   PNaClABIInfo(CodeGen::CodeGenTypes &CGT) : ABIInfo(CGT) {}
 
   ABIArgInfo classifyReturnType(QualType RetTy) const;
@@ -32,9 +32,9 @@ class PNaClABIInfo : public ABIInfo {
 };
 
 class PNaClTargetCodeGenInfo : public TargetCodeGenInfo {
- public:
-   PNaClTargetCodeGenInfo(CodeGen::CodeGenTypes &CGT)
-       : TargetCodeGenInfo(std::make_unique<PNaClABIInfo>(CGT)) {}
+public:
+  PNaClTargetCodeGenInfo(CodeGen::CodeGenTypes &CGT)
+      : TargetCodeGenInfo(std::make_unique<PNaClABIInfo>(CGT)) {}
 };
 
 void PNaClABIInfo::computeInfo(CGFunctionInfo &FI) const {
diff --git a/clang/lib/CodeGen/Targets/PPC.cpp b/clang/lib/CodeGen/Targets/PPC.cpp
index 989e46f4b..4ce0ad2e4 100644
--- a/clang/lib/CodeGen/Targets/PPC.cpp
+++ b/clang/lib/CodeGen/Targets/PPC.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "clang/Basic/DiagnosticFrontend.h"
 
 using namespace clang;
@@ -368,7 +368,7 @@ public:
   bool initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
                                llvm::Value *Address) const override;
 };
-}
+} // namespace
 
 CharUnits PPC32_SVR4_ABIInfo::getParamTypeAlignment(QualType Ty) const {
   // Complex types are passed just like their elements.
@@ -474,7 +474,7 @@ RValue PPC32_SVR4_ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAList,
   // "Align" the register count when TY is i64.
   if (isI64 || (isF64 && IsSoftFloatABI)) {
     NumRegs = Builder.CreateAdd(NumRegs, Builder.getInt8(1));
-    NumRegs = Builder.CreateAnd(NumRegs, Builder.getInt8((uint8_t) ~1U));
+    NumRegs = Builder.CreateAnd(NumRegs, Builder.getInt8((uint8_t)~1U));
   }
 
   llvm::Value *CC =
@@ -508,7 +508,8 @@ RValue PPC32_SVR4_ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAList,
 
     // Get the address of the saved value by scaling the number of
     // registers we've used by the number of
-    CharUnits RegSize = CharUnits::fromQuantity((isInt || IsSoftFloatABI) ? 4 : 8);
+    CharUnits RegSize =
+        CharUnits::fromQuantity((isInt || IsSoftFloatABI) ? 4 : 8);
     llvm::Value *RegOffset =
         Builder.CreateMul(NumRegs, Builder.getInt8(RegSize.getQuantity()));
     RegAddr = Address(Builder.CreateInBoundsGEP(
@@ -517,9 +518,8 @@ RValue PPC32_SVR4_ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAList,
                       RegAddr.getAlignment().alignmentOfArrayElement(RegSize));
 
     // Increase the used-register count.
-    NumRegs =
-      Builder.CreateAdd(NumRegs,
-                        Builder.getInt8((isI64 || (isF64 && IsSoftFloatABI)) ? 2 : 1));
+    NumRegs = Builder.CreateAdd(
+        NumRegs, Builder.getInt8((isI64 || (isF64 && IsSoftFloatABI)) ? 2 : 1));
     Builder.CreateStore(NumRegs, NumRegsAddr);
 
     CGF.EmitBranch(Cont);
@@ -597,9 +597,8 @@ bool PPC32TargetCodeGenInfo::isStructReturnInRegABI(
   return false;
 }
 
-bool
-PPC32TargetCodeGenInfo::initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
-                                                llvm::Value *Address) const {
+bool PPC32TargetCodeGenInfo::initDwarfEHRegSizeTable(
+    CodeGen::CodeGenFunction &CGF, llvm::Value *Address) const {
   return PPC_initDwarfEHRegSizeTable(CGF, Address, /*Is64Bit*/ false,
                                      /*IsAIX*/ false);
 }
@@ -696,12 +695,11 @@ public:
   bool initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
                                llvm::Value *Address) const override;
 };
-}
+} // namespace
 
 // Return true if the ABI requires Ty to be passed sign- or zero-
 // extended to 64 bits.
-bool
-PPC64_SVR4_ABIInfo::isPromotableTypeForABI(QualType Ty) const {
+bool PPC64_SVR4_ABIInfo::isPromotableTypeForABI(QualType Ty) const {
   // Treat an enum type as its underlying type.
   if (const EnumType *EnumTy = Ty->getAs<EnumType>())
     Ty = EnumTy->getDecl()->getIntegerType();
@@ -735,7 +733,7 @@ CharUnits PPC64_SVR4_ABIInfo::getParamTypeAlignment(QualType Ty) const {
   if (const ComplexType *CTy = Ty->getAs<ComplexType>())
     Ty = CTy->getElementType();
 
-  auto FloatUsesVector = [this](QualType Ty){
+  auto FloatUsesVector = [this](QualType Ty) {
     return Ty->isRealFloatingType() && &getContext().getFloatTypeSemantics(
                                            Ty) == &llvm::APFloat::IEEEquad();
   };
@@ -743,7 +741,8 @@ CharUnits PPC64_SVR4_ABIInfo::getParamTypeAlignment(QualType Ty) const {
   // Only vector types of size 16 bytes need alignment (larger types are
   // passed via reference, smaller types are not aligned).
   if (Ty->isVectorType()) {
-    return CharUnits::fromQuantity(getContext().getTypeSize(Ty) == 128 ? 16 : 8);
+    return CharUnits::fromQuantity(getContext().getTypeSize(Ty) == 128 ? 16
+                                                                       : 8);
   } else if (FloatUsesVector(Ty)) {
     // According to ABI document section 'Optional Save Areas': If extended
     // precision floating-point values in IEEE BINARY 128 QUADRUPLE PRECISION
@@ -811,18 +810,17 @@ bool PPC64_SVR4_ABIInfo::isHomogeneousAggregateSmallEnough(
     const Type *Base, uint64_t Members) const {
   // Vector and fp128 types require one register, other floating point types
   // require one or two registers depending on their size.
-  uint32_t NumRegs =
-      ((getContext().getTargetInfo().hasFloat128Type() &&
-          Base->isFloat128Type()) ||
-        Base->isVectorType()) ? 1
-                              : (getContext().getTypeSize(Base) + 63) / 64;
+  uint32_t NumRegs = ((getContext().getTargetInfo().hasFloat128Type() &&
+                       Base->isFloat128Type()) ||
+                      Base->isVectorType())
+                         ? 1
+                         : (getContext().getTypeSize(Base) + 63) / 64;
 
   // Homogeneous Aggregates may occupy at most 8 registers.
   return Members * NumRegs <= 8;
 }
 
-ABIArgInfo
-PPC64_SVR4_ABIInfo::classifyArgumentType(QualType Ty) const {
+ABIArgInfo PPC64_SVR4_ABIInfo::classifyArgumentType(QualType Ty) const {
   Ty = useFirstFieldIfTransparentUnion(Ty);
 
   if (Ty->isAnyComplexType())
@@ -897,8 +895,7 @@ PPC64_SVR4_ABIInfo::classifyArgumentType(QualType Ty) const {
               : ABIArgInfo::getDirect());
 }
 
-ABIArgInfo
-PPC64_SVR4_ABIInfo::classifyReturnType(QualType RetTy) const {
+ABIArgInfo PPC64_SVR4_ABIInfo::classifyReturnType(QualType RetTy) const {
   if (RetTy->isVoidType())
     return ABIArgInfo::getIgnore();
 
@@ -994,10 +991,8 @@ RValue PPC64_SVR4_ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
                           /*ForceRightAdjust*/ true);
 }
 
-bool
-PPC64_SVR4_TargetCodeGenInfo::initDwarfEHRegSizeTable(
-  CodeGen::CodeGenFunction &CGF,
-  llvm::Value *Address) const {
+bool PPC64_SVR4_TargetCodeGenInfo::initDwarfEHRegSizeTable(
+    CodeGen::CodeGenFunction &CGF, llvm::Value *Address) const {
   return PPC_initDwarfEHRegSizeTable(CGF, Address, /*Is64Bit*/ true,
                                      /*IsAIX*/ false);
 }
@@ -1020,9 +1015,8 @@ void PPC64_SVR4_TargetCodeGenInfo::emitTargetMetadata(
   }
 }
 
-bool
-PPC64TargetCodeGenInfo::initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
-                                                llvm::Value *Address) const {
+bool PPC64TargetCodeGenInfo::initDwarfEHRegSizeTable(
+    CodeGen::CodeGenFunction &CGF, llvm::Value *Address) const {
   return PPC_initDwarfEHRegSizeTable(CGF, Address, /*Is64Bit*/ true,
                                      /*IsAIX*/ false);
 }
diff --git a/clang/lib/CodeGen/Targets/RISCV.cpp b/clang/lib/CodeGen/Targets/RISCV.cpp
index 2c48ba37f..5e9a90a9d 100644
--- a/clang/lib/CodeGen/Targets/RISCV.cpp
+++ b/clang/lib/CodeGen/Targets/RISCV.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -332,7 +332,8 @@ ABIArgInfo RISCVABIInfo::coerceAndExpandFPCCEligibleStruct(
 
   CharUnits Field2Align =
       CharUnits::fromQuantity(getDataLayout().getABITypeAlign(Field2Ty));
-  CharUnits Field1End = Field1Off +
+  CharUnits Field1End =
+      Field1Off +
       CharUnits::fromQuantity(getDataLayout().getTypeStoreSize(Field1Ty));
   CharUnits Field2OffNoPadNoPack = Field1End.alignTo(Field2Align);
 
@@ -422,8 +423,8 @@ ABIArgInfo RISCVABIInfo::classifyArgumentType(QualType Ty, bool IsFixed,
     return ABIArgInfo::getIgnore();
 
   // Pass floating point values via FPRs if possible.
-  if (IsFixed && Ty->isFloatingType() && !Ty->isComplexType() &&
-      FLen >= Size && ArgFPRsLeft) {
+  if (IsFixed && Ty->isFloatingType() && !Ty->isComplexType() && FLen >= Size &&
+      ArgFPRsLeft) {
     ArgFPRsLeft--;
     return ABIArgInfo::getDirect();
   }
@@ -586,7 +587,8 @@ public:
   void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
                            CodeGen::CodeGenModule &CGM) const override {
     const auto *FD = dyn_cast_or_null<FunctionDecl>(D);
-    if (!FD) return;
+    if (!FD)
+      return;
 
     auto *Fn = cast<llvm::Function>(GV);
 
@@ -599,8 +601,12 @@ public:
 
     const char *Kind;
     switch (Attr->getInterrupt()) {
-    case RISCVInterruptAttr::supervisor: Kind = "supervisor"; break;
-    case RISCVInterruptAttr::machine: Kind = "machine"; break;
+    case RISCVInterruptAttr::supervisor:
+      Kind = "supervisor";
+      break;
+    case RISCVInterruptAttr::machine:
+      Kind = "machine";
+      break;
     }
 
     Fn->addFnAttr("interrupt", Kind);
diff --git a/clang/lib/CodeGen/Targets/SPIR.cpp b/clang/lib/CodeGen/Targets/SPIR.cpp
index 5c75e985e..b5edeef07 100644
--- a/clang/lib/CodeGen/Targets/SPIR.cpp
+++ b/clang/lib/CodeGen/Targets/SPIR.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -208,8 +208,8 @@ void computeSPIRKernelABIInfo(CodeGenModule &CGM, CGFunctionInfo &FI) {
   else
     CommonSPIRABIInfo(CGM.getTypes()).computeInfo(FI);
 }
-}
-}
+} // namespace CodeGen
+} // namespace clang
 
 unsigned CommonSPIRTargetCodeGenInfo::getOpenCLKernelCallingConv() const {
   return llvm::CallingConv::SPIR_KERNEL;
@@ -339,8 +339,8 @@ llvm::Type *CommonSPIRTargetCodeGenInfo::getOpenCLType(CodeGenModule &CGM,
     enum AccessQualifier : unsigned { AQ_ro = 0, AQ_wo = 1, AQ_rw = 2 };
     switch (BuiltinTy->getKind()) {
 #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix)                   \
-    case BuiltinType::Id:                                                      \
-      return getSPIRVImageType(Ctx, "spirv.Image", #ImgType, AQ_##Suffix);
+  case BuiltinType::Id:                                                        \
+    return getSPIRVImageType(Ctx, "spirv.Image", #ImgType, AQ_##Suffix);
 #include "clang/Basic/OpenCLImageTypes.def"
     case BuiltinType::OCLSampler:
       return llvm::TargetExtType::get(Ctx, "spirv.Sampler");
@@ -353,8 +353,8 @@ llvm::Type *CommonSPIRTargetCodeGenInfo::getOpenCLType(CodeGenModule &CGM,
     case BuiltinType::OCLReserveID:
       return llvm::TargetExtType::get(Ctx, "spirv.ReserveId");
 #define INTEL_SUBGROUP_AVC_TYPE(Name, Id)                                      \
-    case BuiltinType::OCLIntelSubgroupAVC##Id:                                 \
-      return llvm::TargetExtType::get(Ctx, "spirv.Avc" #Id "INTEL");
+  case BuiltinType::OCLIntelSubgroupAVC##Id:                                   \
+    return llvm::TargetExtType::get(Ctx, "spirv.Avc" #Id "INTEL");
 #include "clang/Basic/OpenCLExtensionTypes.def"
     default:
       return nullptr;
diff --git a/clang/lib/CodeGen/Targets/Sparc.cpp b/clang/lib/CodeGen/Targets/Sparc.cpp
index da8c7219b..5b663eb83 100644
--- a/clang/lib/CodeGen/Targets/Sparc.cpp
+++ b/clang/lib/CodeGen/Targets/Sparc.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -29,13 +29,10 @@ private:
 };
 } // end anonymous namespace
 
-
-ABIArgInfo
-SparcV8ABIInfo::classifyReturnType(QualType Ty) const {
+ABIArgInfo SparcV8ABIInfo::classifyReturnType(QualType Ty) const {
   if (Ty->isAnyComplexType()) {
     return ABIArgInfo::getDirect();
-  }
-  else {
+  } else {
     return DefaultABIInfo::classifyReturnType(Ty);
   }
 }
@@ -128,12 +125,12 @@ private:
   struct CoerceBuilder {
     llvm::LLVMContext &Context;
     const llvm::DataLayout &DL;
-    SmallVector<llvm::Type*, 8> Elems;
+    SmallVector<llvm::Type *, 8> Elems;
     uint64_t Size;
     bool InReg;
 
     CoerceBuilder(llvm::LLVMContext &c, const llvm::DataLayout &dl)
-      : Context(c), DL(dl), Size(0), InReg(false) {}
+        : Context(c), DL(dl), Size(0), InReg(false) {}
 
     // Pad Elems with integers until Size is ToSize.
     void pad(uint64_t ToSize) {
@@ -222,8 +219,7 @@ private:
 };
 } // end anonymous namespace
 
-ABIArgInfo
-SparcV9ABIInfo::classifyType(QualType Ty, unsigned SizeLimit) const {
+ABIArgInfo SparcV9ABIInfo::classifyType(QualType Ty, unsigned SizeLimit) const {
   if (Ty->isVoidType())
     return ABIArgInfo::getIgnore();
 
@@ -369,9 +365,8 @@ public:
 };
 } // end anonymous namespace
 
-bool
-SparcV9TargetCodeGenInfo::initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
-                                                llvm::Value *Address) const {
+bool SparcV9TargetCodeGenInfo::initDwarfEHRegSizeTable(
+    CodeGen::CodeGenFunction &CGF, llvm::Value *Address) const {
   // This is calculated from the LLVM and GCC tables and verified
   // against gcc output.  AFAIK all ABIs use the same encoding.
 
diff --git a/clang/lib/CodeGen/Targets/SystemZ.cpp b/clang/lib/CodeGen/Targets/SystemZ.cpp
index 23c96fa5c..26bdd2bfa 100644
--- a/clang/lib/CodeGen/Targets/SystemZ.cpp
+++ b/clang/lib/CodeGen/Targets/SystemZ.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "clang/Basic/Builtins.h"
 #include "llvm/IR/IntrinsicsS390.h"
 
@@ -60,7 +60,7 @@ public:
   SystemZTargetCodeGenInfo(CodeGenTypes &CGT, bool HasVector, bool SoftFloatABI)
       : TargetCodeGenInfo(
             std::make_unique<SystemZABIInfo>(CGT, HasVector, SoftFloatABI)),
-            Ctx(CGT.getContext()) {
+        Ctx(CGT.getContext()) {
     SwiftInfo =
         std::make_unique<SwiftABIInfo>(CGT, /*SwiftErrorInRegister=*/false);
   }
@@ -89,12 +89,11 @@ public:
     if (const auto *VD = dyn_cast<VarDecl>(D)) {
       if (VD->isExternallyVisible())
         handleExternallyVisibleObjABI(VD->getType().getTypePtr(), M,
-                                      /*IsParam*/false);
-    }
-    else if (const FunctionDecl *FD = dyn_cast<FunctionDecl>(D)) {
+                                      /*IsParam*/ false);
+    } else if (const FunctionDecl *FD = dyn_cast<FunctionDecl>(D)) {
       if (FD->isExternallyVisible())
         handleExternallyVisibleObjABI(FD->getType().getTypePtr(), M,
-                                      /*IsParam*/false);
+                                      /*IsParam*/ false);
     }
   }
 
@@ -140,7 +139,7 @@ public:
     return nullptr;
   }
 };
-}
+} // namespace
 
 bool SystemZABIInfo::isPromotableIntegerTypeForABI(QualType Ty) const {
   // Treat an enum type as its underlying type.
@@ -168,14 +167,12 @@ bool SystemZABIInfo::isPromotableIntegerTypeForABI(QualType Ty) const {
 }
 
 bool SystemZABIInfo::isCompoundType(QualType Ty) const {
-  return (Ty->isAnyComplexType() ||
-          Ty->isVectorType() ||
+  return (Ty->isAnyComplexType() || Ty->isVectorType() ||
           isAggregateTypeForABI(Ty));
 }
 
 bool SystemZABIInfo::isVectorArgumentType(QualType Ty) const {
-  return (HasVector &&
-          Ty->isVectorType() &&
+  return (HasVector && Ty->isVectorType() &&
           getContext().getTypeSize(Ty) <= 128);
 }
 
@@ -270,7 +267,7 @@ RValue SystemZABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
   CharUnits UnpaddedSize;
   CharUnits DirectAlign;
   SZCGI.handleExternallyVisibleObjABI(Ty.getTypePtr(), CGT.getCGM(),
-                                      /*IsParam*/true);
+                                      /*IsParam*/ true);
   if (IsIndirect) {
     DirectTy = llvm::PointerType::getUnqual(DirectTy);
     UnpaddedSize = DirectAlign = CharUnits::fromQuantity(8);
@@ -291,7 +288,7 @@ RValue SystemZABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
 
   llvm::Type *IndexTy = CGF.Int64Ty;
   llvm::Value *PaddedSizeV =
-    llvm::ConstantInt::get(IndexTy, PaddedSize.getQuantity());
+      llvm::ConstantInt::get(IndexTy, PaddedSize.getQuantity());
 
   if (IsVector) {
     // Work out the address of a vector argument on the stack.
@@ -318,14 +315,14 @@ RValue SystemZABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
   unsigned MaxRegs, RegCountField, RegSaveIndex;
   CharUnits RegPadding;
   if (InFPRs) {
-    MaxRegs = 4; // Maximum of 4 FPR arguments
-    RegCountField = 1; // __fpr
-    RegSaveIndex = 16; // save offset for f0
+    MaxRegs = 4;              // Maximum of 4 FPR arguments
+    RegCountField = 1;        // __fpr
+    RegSaveIndex = 16;        // save offset for f0
     RegPadding = CharUnits(); // floats are passed in the high bits of an FPR
   } else {
-    MaxRegs = 5; // Maximum of 5 GPR arguments
-    RegCountField = 0; // __gpr
-    RegSaveIndex = 2; // save offset for r2
+    MaxRegs = 5;          // Maximum of 5 GPR arguments
+    RegCountField = 0;    // __gpr
+    RegSaveIndex = 2;     // save offset for r2
     RegPadding = Padding; // values are passed in the low bits of a GPR
   }
 
@@ -333,8 +330,8 @@ RValue SystemZABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
       CGF.Builder.CreateStructGEP(VAListAddr, RegCountField, "reg_count_ptr");
   llvm::Value *RegCount = CGF.Builder.CreateLoad(RegCountPtr, "reg_count");
   llvm::Value *MaxRegsV = llvm::ConstantInt::get(IndexTy, MaxRegs);
-  llvm::Value *InRegs = CGF.Builder.CreateICmpULT(RegCount, MaxRegsV,
-                                                 "fits_in_regs");
+  llvm::Value *InRegs =
+      CGF.Builder.CreateICmpULT(RegCount, MaxRegsV, "fits_in_regs");
 
   llvm::BasicBlock *InRegBlock = CGF.createBasicBlock("vaarg.in_reg");
   llvm::BasicBlock *InMemBlock = CGF.createBasicBlock("vaarg.in_mem");
@@ -346,12 +343,12 @@ RValue SystemZABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
 
   // Work out the address of an argument register.
   llvm::Value *ScaledRegCount =
-    CGF.Builder.CreateMul(RegCount, PaddedSizeV, "scaled_reg_count");
+      CGF.Builder.CreateMul(RegCount, PaddedSizeV, "scaled_reg_count");
   llvm::Value *RegBase =
-    llvm::ConstantInt::get(IndexTy, RegSaveIndex * PaddedSize.getQuantity()
-                                      + RegPadding.getQuantity());
+      llvm::ConstantInt::get(IndexTy, RegSaveIndex * PaddedSize.getQuantity() +
+                                          RegPadding.getQuantity());
   llvm::Value *RegOffset =
-    CGF.Builder.CreateAdd(ScaledRegCount, RegBase, "reg_offset");
+      CGF.Builder.CreateAdd(ScaledRegCount, RegBase, "reg_offset");
   Address RegSaveAreaPtr =
       CGF.Builder.CreateStructGEP(VAListAddr, 3, "reg_save_area_ptr");
   llvm::Value *RegSaveArea =
@@ -363,8 +360,7 @@ RValue SystemZABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
 
   // Update the register count
   llvm::Value *One = llvm::ConstantInt::get(IndexTy, 1);
-  llvm::Value *NewRegCount =
-    CGF.Builder.CreateAdd(RegCount, One, "reg_count");
+  llvm::Value *NewRegCount = CGF.Builder.CreateAdd(RegCount, One, "reg_count");
   CGF.Builder.CreateStore(NewRegCount, RegCountPtr);
   CGF.EmitBranch(ContBlock);
 
@@ -478,7 +474,7 @@ void SystemZABIInfo::computeInfo(CGFunctionInfo &FI) const {
       // vector ABI becomes visible as the va_list could be passed on to
       // other functions.
       SZCGI.handleExternallyVisibleObjABI(I.type.getTypePtr(), CGT.getCGM(),
-                                          /*IsParam*/true);
+                                          /*IsParam*/ true);
   }
 }
 
@@ -495,8 +491,9 @@ bool SystemZTargetCodeGenInfo::isVectorTypeBased(const Type *Ty,
     const Type *SingleEltTy = getABIInfo<SystemZABIInfo>()
                                   .GetSingleElementType(QualType(Ty, 0))
                                   .getTypePtr();
-    bool SingleVecEltStruct = SingleEltTy != Ty && SingleEltTy->isVectorType() &&
-      Ctx.getTypeSize(SingleEltTy) == Ctx.getTypeSize(Ty);
+    bool SingleVecEltStruct =
+        SingleEltTy != Ty && SingleEltTy->isVectorType() &&
+        Ctx.getTypeSize(SingleEltTy) == Ctx.getTypeSize(Ty);
     if (Ty->isVectorType() || SingleVecEltStruct)
       return Ctx.getTypeSize(Ty) / 8 <= 16;
   }
@@ -507,26 +504,26 @@ bool SystemZTargetCodeGenInfo::isVectorTypeBased(const Type *Ty,
 
   // Vectors >= 16 bytes expose the ABI through alignment requirements.
   if (Ty->isVectorType() && Ctx.getTypeSize(Ty) / 8 >= 16)
-      return true;
+    return true;
 
   if (const auto *RecordTy = Ty->getAs<RecordType>()) {
     const RecordDecl *RD = RecordTy->getDecl();
     if (const CXXRecordDecl *CXXRD = dyn_cast<CXXRecordDecl>(RD))
       if (CXXRD->hasDefinition())
         for (const auto &I : CXXRD->bases())
-          if (isVectorTypeBased(I.getType().getTypePtr(), /*IsParam*/false))
+          if (isVectorTypeBased(I.getType().getTypePtr(), /*IsParam*/ false))
             return true;
     for (const auto *FD : RD->fields())
-      if (isVectorTypeBased(FD->getType().getTypePtr(), /*IsParam*/false))
+      if (isVectorTypeBased(FD->getType().getTypePtr(), /*IsParam*/ false))
         return true;
   }
 
   if (const auto *FT = Ty->getAs<FunctionType>())
-    if (isVectorTypeBased(FT->getReturnType().getTypePtr(), /*IsParam*/true))
+    if (isVectorTypeBased(FT->getReturnType().getTypePtr(), /*IsParam*/ true))
       return true;
   if (const FunctionProtoType *Proto = Ty->getAs<FunctionProtoType>())
     for (const auto &ParamType : Proto->getParamTypes())
-      if (isVectorTypeBased(ParamType.getTypePtr(), /*IsParam*/true))
+      if (isVectorTypeBased(ParamType.getTypePtr(), /*IsParam*/ true))
         return true;
 
   return false;
diff --git a/clang/lib/CodeGen/Targets/TCE.cpp b/clang/lib/CodeGen/Targets/TCE.cpp
index d7178b4b8..a994a8fda 100644
--- a/clang/lib/CodeGen/Targets/TCE.cpp
+++ b/clang/lib/CodeGen/Targets/TCE.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -34,7 +34,8 @@ void TCETargetCodeGenInfo::setTargetAttributes(
   if (GV->isDeclaration())
     return;
   const FunctionDecl *FD = dyn_cast_or_null<FunctionDecl>(D);
-  if (!FD) return;
+  if (!FD)
+    return;
 
   llvm::Function *F = cast<llvm::Function>(GV);
 
@@ -74,7 +75,7 @@ void TCETargetCodeGenInfo::setTargetAttributes(
   }
 }
 
-}
+} // namespace
 
 std::unique_ptr<TargetCodeGenInfo>
 CodeGen::createTCETargetCodeGenInfo(CodeGenModule &CGM) {
diff --git a/clang/lib/CodeGen/Targets/VE.cpp b/clang/lib/CodeGen/Targets/VE.cpp
index a7acc249c..558fee3ee 100644
--- a/clang/lib/CodeGen/Targets/VE.cpp
+++ b/clang/lib/CodeGen/Targets/VE.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
diff --git a/clang/lib/CodeGen/Targets/WebAssembly.cpp b/clang/lib/CodeGen/Targets/WebAssembly.cpp
index 70a968fe9..71d13dde7 100644
--- a/clang/lib/CodeGen/Targets/WebAssembly.cpp
+++ b/clang/lib/CodeGen/Targets/WebAssembly.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
diff --git a/clang/lib/CodeGen/Targets/X86.cpp b/clang/lib/CodeGen/Targets/X86.cpp
index 5ee5179dd..0ba5b3449 100644
--- a/clang/lib/CodeGen/Targets/X86.cpp
+++ b/clang/lib/CodeGen/Targets/X86.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 #include "clang/Basic/DiagnosticFrontend.h"
 #include "llvm/ADT/SmallBitVector.h"
 
@@ -20,8 +20,8 @@ namespace {
 bool IsX86_MMXType(llvm::Type *IRType) {
   // Return true if the type is an MMX type <2 x i32>, <4 x i16>, or <8 x i8>.
   return IRType->isVectorTy() && IRType->getPrimitiveSizeInBits() == 64 &&
-    cast<llvm::VectorType>(IRType)->getElementType()->isIntegerTy() &&
-    IRType->getScalarSizeInBits() != 64;
+         cast<llvm::VectorType>(IRType)->getElementType()->isIntegerTy() &&
+         IRType->getScalarSizeInBits() != 64;
 }
 
 static llvm::Type *X86AdjustInlineAsmType(CodeGen::CodeGenFunction &CGF,
@@ -65,7 +65,7 @@ static bool isX86VectorCallAggregateSmallEnough(uint64_t NumMembers) {
 }
 
 /// Returns a Homogeneous Vector Aggregate ABIArgInfo, used in X86.
-static ABIArgInfo getDirectX86Hva(llvm::Type* T = nullptr) {
+static ABIArgInfo getDirectX86Hva(llvm::Type *T = nullptr) {
   auto AI = ABIArgInfo::getDirect(T);
   AI.setInReg(true);
   AI.setCanBeFlattened(false);
@@ -80,7 +80,7 @@ static ABIArgInfo getDirectX86Hva(llvm::Type* T = nullptr) {
 struct CCState {
   CCState(CGFunctionInfo &FI)
       : IsPreassigned(FI.arg_size()), CC(FI.getCallingConvention()),
-	Required(FI.getRequiredArgs()), IsDelegateCall(FI.isDelegateCall()) {}
+        Required(FI.getRequiredArgs()), IsDelegateCall(FI.isDelegateCall()) {}
 
   llvm::SmallBitVector IsPreassigned;
   unsigned CC = CallingConv::CC_C;
@@ -92,10 +92,7 @@ struct CCState {
 
 /// X86_32ABIInfo - The X86-32 ABI information.
 class X86_32ABIInfo : public ABIInfo {
-  enum Class {
-    Integer,
-    Float
-  };
+  enum Class { Integer, Float };
 
   static const unsigned MinABIStackAlignInBytes = 4;
 
@@ -158,7 +155,6 @@ class X86_32ABIInfo : public ABIInfo {
   void runVectorCallFirstPass(CGFunctionInfo &FI, CCState &State) const;
 
 public:
-
   void computeInfo(CGFunctionInfo &FI) const override;
   RValue EmitVAArg(CodeGenFunction &CGF, Address VAListAddr, QualType Ty,
                    AggValueSlot Slot) const override;
@@ -201,24 +197,25 @@ public:
     SwiftInfo = std::make_unique<X86_32SwiftABIInfo>(CGT);
   }
 
-  static bool isStructReturnInRegABI(
-      const llvm::Triple &Triple, const CodeGenOptions &Opts);
+  static bool isStructReturnInRegABI(const llvm::Triple &Triple,
+                                     const CodeGenOptions &Opts);
 
   void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
                            CodeGen::CodeGenModule &CGM) const override;
 
   int getDwarfEHStackPointer(CodeGen::CodeGenModule &CGM) const override {
     // Darwin uses different dwarf register numbers for EH.
-    if (CGM.getTarget().getTriple().isOSDarwin()) return 5;
+    if (CGM.getTarget().getTriple().isOSDarwin())
+      return 5;
     return 4;
   }
 
   bool initDwarfEHRegSizeTable(CodeGen::CodeGenFunction &CGF,
                                llvm::Value *Address) const override;
 
-  llvm::Type* adjustInlineAsmType(CodeGen::CodeGenFunction &CGF,
+  llvm::Type *adjustInlineAsmType(CodeGen::CodeGenFunction &CGF,
                                   StringRef Constraint,
-                                  llvm::Type* Ty) const override {
+                                  llvm::Type *Ty) const override {
     return X86AdjustInlineAsmType(CGF, Constraint, Ty);
   }
 
@@ -236,7 +233,7 @@ public:
   }
 };
 
-}
+} // namespace
 
 /// Rewrite input constraint references after adding some output constraints.
 /// In the case where there is one output and one input and we add one output,
@@ -329,7 +326,7 @@ bool X86_32ABIInfo::shouldReturnTypeInRegister(QualType Ty,
   // For i386, type must be register sized.
   // For the MCU ABI, it only needs to be <= 8-byte
   if ((IsMCUABI && Size > 64) || (!IsMCUABI && !isRegisterSize(Size)))
-   return false;
+    return false;
 
   if (Ty->isVectorType()) {
     // 64- and 128- bit vectors inside structures are not returned in
@@ -353,7 +350,8 @@ bool X86_32ABIInfo::shouldReturnTypeInRegister(QualType Ty,
 
   // Otherwise, it must be a record type.
   const RecordType *RT = Ty->getAs<RecordType>();
-  if (!RT) return false;
+  if (!RT)
+    return false;
 
   // FIXME: Traverse bases here too.
 
@@ -453,7 +451,8 @@ bool X86_32ABIInfo::canExpandIndirectArgument(QualType Ty) const {
   return Size == getContext().getTypeSize(Ty);
 }
 
-ABIArgInfo X86_32ABIInfo::getIndirectReturnResult(QualType RetTy, CCState &State) const {
+ABIArgInfo X86_32ABIInfo::getIndirectReturnResult(QualType RetTy,
+                                                  CCState &State) const {
   // If the return value is indirect, then the hidden argument is consuming one
   // integer register.
   if (State.CC != llvm::CallingConv::X86_FastCall &&
@@ -495,8 +494,8 @@ ABIArgInfo X86_32ABIInfo::classifyReturnType(QualType RetTy,
       // register, or if it is 64 bits and has a single element.
       if ((Size == 8 || Size == 16 || Size == 32) ||
           (Size == 64 && VT->getNumElements() == 1))
-        return ABIArgInfo::getDirect(llvm::IntegerType::get(getVMContext(),
-                                                            Size));
+        return ABIArgInfo::getDirect(
+            llvm::IntegerType::get(getVMContext(), Size));
 
       return getIndirectReturnResult(RetTy, State);
     }
@@ -538,13 +537,14 @@ ABIArgInfo X86_32ABIInfo::classifyReturnType(QualType RetTy,
       // We apply a similar transformation for pointer types to improve the
       // quality of the generated IR.
       if (const Type *SeltTy = isSingleElementStruct(RetTy, getContext()))
-        if ((!IsWin32StructABI && SeltTy->isRealFloatingType())
-            || SeltTy->hasPointerRepresentation())
+        if ((!IsWin32StructABI && SeltTy->isRealFloatingType()) ||
+            SeltTy->hasPointerRepresentation())
           return ABIArgInfo::getDirect(CGT.ConvertType(QualType(SeltTy, 0)));
 
       // FIXME: We should be able to narrow this integer in cases with dead
       // padding.
-      return ABIArgInfo::getDirect(llvm::IntegerType::get(getVMContext(),Size));
+      return ABIArgInfo::getDirect(
+          llvm::IntegerType::get(getVMContext(), Size));
     }
 
     return getIndirectReturnResult(RetTy, State);
@@ -708,7 +708,8 @@ bool X86_32ABIInfo::shouldPrimitiveUseInReg(QualType Ty, CCState &State) const {
   return !IsMCUABI;
 }
 
-void X86_32ABIInfo::runVectorCallFirstPass(CGFunctionInfo &FI, CCState &State) const {
+void X86_32ABIInfo::runVectorCallFirstPass(CGFunctionInfo &FI,
+                                           CCState &State) const {
   // Vectorcall x86 works subtly different than in x64, so the format is
   // a bit different than the x64 version.  First, all vector types (not HVAs)
   // are assigned, with the first 6 ending up in the [XYZ]MM0-5 registers.
@@ -805,7 +806,7 @@ ABIArgInfo X86_32ABIInfo::classifyArgumentType(QualType Ty, CCState &State,
     bool InReg;
     if (shouldAggregateUseDirect(Ty, State, InReg, NeedsPadding)) {
       unsigned SizeInRegs = (TI.Width + 31) / 32;
-      SmallVector<llvm::Type*, 3> Elements(SizeInRegs, Int32);
+      SmallVector<llvm::Type *, 3> Elements(SizeInRegs, Int32);
       llvm::Type *Result = llvm::StructType::get(LLVMContext, Elements);
       if (InReg)
         return ABIArgInfo::getDirectInReg(Result);
@@ -823,7 +824,7 @@ ABIArgInfo X86_32ABIInfo::classifyArgumentType(QualType Ty, CCState &State,
       unsigned AlignInBits = 0;
       if (RT) {
         const ASTRecordLayout &Layout =
-          getContext().getASTRecordLayout(RT->getDecl());
+            getContext().getASTRecordLayout(RT->getDecl());
         AlignInBits = getContext().toBits(Layout.getRequiredAlignment());
       } else if (TI.isAlignRequired()) {
         AlignInBits = TI.Align;
@@ -873,7 +874,6 @@ ABIArgInfo X86_32ABIInfo::classifyArgumentType(QualType Ty, CCState &State,
     return ABIArgInfo::getDirect();
   }
 
-
   if (const EnumType *EnumTy = Ty->getAs<EnumType>())
     Ty = EnumTy->getDecl()->getIntegerType();
 
@@ -928,7 +928,7 @@ void X86_32ABIInfo::computeInfo(CGFunctionInfo &FI) const {
     // The C++ ABI is not aware of register usage, so we have to check if the
     // return value was sret and put it in a register ourselves if appropriate.
     if (State.FreeRegs) {
-      --State.FreeRegs;  // The sret parameter consumes a register.
+      --State.FreeRegs; // The sret parameter consumes a register.
       if (!IsMCUABI)
         FI.getReturnInfo().setInReg(true);
     }
@@ -950,8 +950,7 @@ void X86_32ABIInfo::computeInfo(CGFunctionInfo &FI) const {
     if (State.IsPreassigned.test(I))
       continue;
 
-    Args[I].info =
-        classifyArgumentType(Args[I].type, State, I);
+    Args[I].info = classifyArgumentType(Args[I].type, State, I);
     UsedInAlloca |= (Args[I].info.getKind() == ABIArgInfo::InAlloca);
   }
 
@@ -961,10 +960,9 @@ void X86_32ABIInfo::computeInfo(CGFunctionInfo &FI) const {
     rewriteWithInAlloca(FI);
 }
 
-void
-X86_32ABIInfo::addFieldToArgStruct(SmallVector<llvm::Type *, 6> &FrameFields,
-                                   CharUnits &StackOffset, ABIArgInfo &Info,
-                                   QualType Type) const {
+void X86_32ABIInfo::addFieldToArgStruct(
+    SmallVector<llvm::Type *, 6> &FrameFields, CharUnits &StackOffset,
+    ABIArgInfo &Info, QualType Type) const {
   // Arguments are always 4-byte-aligned.
   CharUnits WordSize = CharUnits::fromQuantity(4);
   assert(StackOffset.isMultipleOf(WordSize) && "unaligned inalloca struct");
@@ -1074,7 +1072,7 @@ RValue X86_32ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
   // Just messing with TypeInfo like this works because we never pass
   // anything indirectly.
   TypeInfo.Align = CharUnits::fromQuantity(
-                getTypeStackAlignInBytes(Ty, TypeInfo.Align.getQuantity()));
+      getTypeStackAlignInBytes(Ty, TypeInfo.Align.getQuantity()));
 
   return emitVoidPtrVAArg(CGF, VAListAddr, Ty, /*Indirect*/ false, TypeInfo,
                           CharUnits::fromQuantity(4),
@@ -1088,9 +1086,9 @@ bool X86_32TargetCodeGenInfo::isStructReturnInRegABI(
   switch (Opts.getStructReturnConvention()) {
   case CodeGenOptions::SRCK_Default:
     break;
-  case CodeGenOptions::SRCK_OnStack:  // -fpcc-struct-return
+  case CodeGenOptions::SRCK_OnStack: // -fpcc-struct-return
     return false;
-  case CodeGenOptions::SRCK_InRegs:  // -freg-struct-return
+  case CodeGenOptions::SRCK_InRegs: // -freg-struct-return
     return true;
   }
 
@@ -1120,8 +1118,8 @@ static void addX86InterruptAttrs(const FunctionDecl *FD, llvm::GlobalValue *GV,
 
   auto PtrTy = cast<PointerType>(FD->getParamDecl(0)->getType());
   llvm::Type *ByValTy = CGM.getTypes().ConvertType(PtrTy->getPointeeType());
-  llvm::Attribute NewAttr = llvm::Attribute::getWithByValType(
-    Fn->getContext(), ByValTy);
+  llvm::Attribute NewAttr =
+      llvm::Attribute::getWithByValType(Fn->getContext(), ByValTy);
   Fn->addParamAttr(0, NewAttr);
 }
 
@@ -1140,8 +1138,7 @@ void X86_32TargetCodeGenInfo::setTargetAttributes(
 }
 
 bool X86_32TargetCodeGenInfo::initDwarfEHRegSizeTable(
-                                               CodeGen::CodeGenFunction &CGF,
-                                               llvm::Value *Address) const {
+    CodeGen::CodeGenFunction &CGF, llvm::Value *Address) const {
   CodeGen::CGBuilderTy &Builder = CGF.Builder;
 
   llvm::Value *Four8 = llvm::ConstantInt::get(CGF.Int8Ty, 4);
@@ -1163,7 +1160,7 @@ bool X86_32TargetCodeGenInfo::initDwarfEHRegSizeTable(
     // reason.
     Builder.CreateAlignedStore(
         Four8, Builder.CreateConstInBoundsGEP1_32(CGF.Int8Ty, Address, 9),
-                               CharUnits::One());
+        CharUnits::One());
 
     // 11-16 are st(0..5).  Not sure why we stop at 5.
     // These have size 12, which is sizeof(long double) on
@@ -1179,7 +1176,6 @@ bool X86_32TargetCodeGenInfo::initDwarfEHRegSizeTable(
 // X86-64 ABI Implementation
 //===----------------------------------------------------------------------===//
 
-
 namespace {
 
 /// \p returns the size in bits of the largest (native) vector for \p AVXLevel.
@@ -1265,11 +1261,11 @@ class X86_64ABIInfo : public ABIInfo {
                 bool isNamedArg, bool IsRegCall = false) const;
 
   llvm::Type *GetByteVectorType(QualType Ty) const;
-  llvm::Type *GetSSETypeAtOffset(llvm::Type *IRType,
-                                 unsigned IROffset, QualType SourceTy,
+  llvm::Type *GetSSETypeAtOffset(llvm::Type *IRType, unsigned IROffset,
+                                 QualType SourceTy,
                                  unsigned SourceOffset) const;
-  llvm::Type *GetINTEGERTypeAtOffset(llvm::Type *IRType,
-                                     unsigned IROffset, QualType SourceTy,
+  llvm::Type *GetINTEGERTypeAtOffset(llvm::Type *IRType, unsigned IROffset,
+                                     QualType SourceTy,
                                      unsigned SourceOffset) const;
 
   /// getIndirectResult - Give a source type \arg Ty, return a suitable result
@@ -1348,7 +1344,7 @@ public:
     unsigned neededInt, neededSSE;
     // The freeIntRegs argument doesn't matter here.
     ABIArgInfo info = classifyArgumentType(type, 0, neededInt, neededSSE,
-                                           /*isNamedArg*/true);
+                                           /*isNamedArg*/ true);
     if (info.isDirect()) {
       llvm::Type *ty = info.getCoerceToType();
       if (llvm::VectorType *vectorTy = dyn_cast_or_null<llvm::VectorType>(ty))
@@ -1364,9 +1360,7 @@ public:
   RValue EmitMSVAArg(CodeGenFunction &CGF, Address VAListAddr, QualType Ty,
                      AggValueSlot Slot) const override;
 
-  bool has64BitPointers() const {
-    return Has64BitPointers;
-  }
+  bool has64BitPointers() const { return Has64BitPointers; }
 };
 
 /// WinX86_64ABIInfo - The Windows X86_64 ABI information.
@@ -1429,9 +1423,9 @@ public:
     return false;
   }
 
-  llvm::Type* adjustInlineAsmType(CodeGen::CodeGenFunction &CGF,
+  llvm::Type *adjustInlineAsmType(CodeGen::CodeGenFunction &CGF,
                                   StringRef Constraint,
-                                  llvm::Type* Ty) const override {
+                                  llvm::Type *Ty) const override {
     return X86AdjustInlineAsmType(CGF, Constraint, Ty);
   }
 
@@ -1445,8 +1439,8 @@ public:
     // defines varargs anyway.
     if (fnType->getCallConv() == CC_C) {
       bool HasAVXType = false;
-      for (CallArgList::const_iterator
-             it = args.begin(), ie = args.end(); it != ie; ++it) {
+      for (CallArgList::const_iterator it = args.begin(), ie = args.end();
+           it != ie; ++it) {
         if (getABIInfo<X86_64ABIInfo>().isPassedUsingAVXType(it->Ty)) {
           HasAVXType = true;
           break;
@@ -1618,11 +1612,11 @@ std::string TargetCodeGenInfo::qualifyWindowsLibrary(StringRef Lib) {
 namespace {
 class WinX86_32TargetCodeGenInfo : public X86_32TargetCodeGenInfo {
 public:
-  WinX86_32TargetCodeGenInfo(CodeGen::CodeGenTypes &CGT,
-        bool DarwinVectorABI, bool RetSmallStructInRegABI, bool Win32StructABI,
-        unsigned NumRegisterParameters)
-    : X86_32TargetCodeGenInfo(CGT, DarwinVectorABI, RetSmallStructInRegABI,
-        Win32StructABI, NumRegisterParameters, false) {}
+  WinX86_32TargetCodeGenInfo(CodeGen::CodeGenTypes &CGT, bool DarwinVectorABI,
+                             bool RetSmallStructInRegABI, bool Win32StructABI,
+                             unsigned NumRegisterParameters)
+      : X86_32TargetCodeGenInfo(CGT, DarwinVectorABI, RetSmallStructInRegABI,
+                                Win32StructABI, NumRegisterParameters, false) {}
 
   void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
                            CodeGen::CodeGenModule &CGM) const override;
@@ -1633,8 +1627,7 @@ public:
     Opt += qualifyWindowsLibrary(Lib);
   }
 
-  void getDetectMismatchOption(llvm::StringRef Name,
-                               llvm::StringRef Value,
+  void getDetectMismatchOption(llvm::StringRef Name, llvm::StringRef Value,
                                llvm::SmallString<32> &Opt) const override {
     Opt = "/FAILIFMISMATCH:\"" + Name.str() + "=" + Value.str() + "\"";
   }
@@ -1682,8 +1675,7 @@ public:
     Opt += qualifyWindowsLibrary(Lib);
   }
 
-  void getDetectMismatchOption(llvm::StringRef Name,
-                               llvm::StringRef Value,
+  void getDetectMismatchOption(llvm::StringRef Name, llvm::StringRef Value,
                                llvm::SmallString<32> &Opt) const override {
     Opt = "/FAILIFMISMATCH:\"" + Name.str() + "=" + Value.str() + "\"";
   }
@@ -1774,8 +1766,8 @@ X86_64ABIInfo::Class X86_64ABIInfo::merge(Class Accum, Class Field) {
     return Field;
   if (Accum == Integer || Field == Integer)
     return Integer;
-  if (Field == X87 || Field == X87Up || Field == ComplexX87 ||
-      Accum == X87 || Accum == X87Up)
+  if (Field == X87 || Field == X87Up || Field == ComplexX87 || Accum == X87 ||
+      Accum == X87Up)
     return Memory;
   return SSE;
 }
@@ -2009,7 +2001,8 @@ void X86_64ABIInfo::classify(QualType Ty, uint64_t OffsetBase, Class &Lo,
         (Size != EltSize || Size > getNativeVectorSizeForAVXABI(AVXLevel)))
       return;
 
-    for (uint64_t i=0, Offset=OffsetBase; i<ArraySize; ++i, Offset += EltSize) {
+    for (uint64_t i = 0, Offset = OffsetBase; i < ArraySize;
+         ++i, Offset += EltSize) {
       Class FieldLo, FieldHi;
       classify(AT->getElementType(), Offset, FieldLo, FieldHi, isNamedArg);
       Lo = merge(Lo, FieldLo);
@@ -2063,7 +2056,7 @@ void X86_64ABIInfo::classify(QualType Ty, uint64_t OffsetBase, Class &Lo,
         // initialized to class NO_CLASS.
         Class FieldLo, FieldHi;
         uint64_t Offset =
-          OffsetBase + getContext().toBits(Layout.getBaseClassOffset(Base));
+            OffsetBase + getContext().toBits(Layout.getBaseClassOffset(Base));
         classify(I.getType(), Offset, FieldLo, FieldHi, isNamedArg);
         Lo = merge(Lo, FieldLo);
         Hi = merge(Hi, FieldHi);
@@ -2082,7 +2075,7 @@ void X86_64ABIInfo::classify(QualType Ty, uint64_t OffsetBase, Class &Lo,
     bool IsUnion = RT->isUnionType() && !UseClang11Compat;
 
     for (RecordDecl::field_iterator i = RD->field_begin(), e = RD->field_end();
-           i != e; ++i, ++idx) {
+         i != e; ++i, ++idx) {
       uint64_t Offset = OffsetBase + Layout.getFieldOffset(idx);
       bool BitField = i->isBitField();
 
@@ -2243,8 +2236,8 @@ ABIArgInfo X86_64ABIInfo::getIndirectResult(QualType Ty,
     // If this type fits in an eightbyte, coerce it into the matching integral
     // type, which will end up on the stack (with alignment 8).
     if (Align == 8 && Size <= 64)
-      return ABIArgInfo::getDirect(llvm::IntegerType::get(getVMContext(),
-                                                          Size));
+      return ABIArgInfo::getDirect(
+          llvm::IntegerType::get(getVMContext(), Size));
   }
 
   return ABIArgInfo::getIndirect(CharUnits::fromQuantity(Align));
@@ -2280,7 +2273,6 @@ llvm::Type *X86_64ABIInfo::GetByteVectorType(QualType Ty) const {
   uint64_t Size = getContext().getTypeSize(Ty);
   assert((Size == 128 || Size == 256 || Size == 512) && "Invalid type found!");
 
-
   // Return a LLVM IR vector type based on the size of 'Ty'.
   return llvm::FixedVectorType::get(llvm::Type::getDoubleTy(getVMContext()),
                                     Size / 64);
@@ -2309,12 +2301,13 @@ static bool BitsContainNoUserData(QualType Ty, unsigned StartBit,
     // Check each element to see if the element overlaps with the queried range.
     for (unsigned i = 0; i != NumElts; ++i) {
       // If the element is after the span we care about, then we're done..
-      unsigned EltOffset = i*EltSize;
-      if (EltOffset >= EndBit) break;
+      unsigned EltOffset = i * EltSize;
+      if (EltOffset >= EndBit)
+        break;
 
-      unsigned EltStart = EltOffset < StartBit ? StartBit-EltOffset :0;
+      unsigned EltStart = EltOffset < StartBit ? StartBit - EltOffset : 0;
       if (!BitsContainNoUserData(AT->getElementType(), EltStart,
-                                 EndBit-EltOffset, Context))
+                                 EndBit - EltOffset, Context))
         return false;
     }
     // If it overlaps no elements, then it is safe to process as padding.
@@ -2335,11 +2328,12 @@ static bool BitsContainNoUserData(QualType Ty, unsigned StartBit,
 
         // If the base is after the span we care about, ignore it.
         unsigned BaseOffset = Context.toBits(Layout.getBaseClassOffset(Base));
-        if (BaseOffset >= EndBit) continue;
+        if (BaseOffset >= EndBit)
+          continue;
 
-        unsigned BaseStart = BaseOffset < StartBit ? StartBit-BaseOffset :0;
-        if (!BitsContainNoUserData(I.getType(), BaseStart,
-                                   EndBit-BaseOffset, Context))
+        unsigned BaseStart = BaseOffset < StartBit ? StartBit - BaseOffset : 0;
+        if (!BitsContainNoUserData(I.getType(), BaseStart, EndBit - BaseOffset,
+                                   Context))
           return false;
       }
     }
@@ -2354,10 +2348,11 @@ static bool BitsContainNoUserData(QualType Ty, unsigned StartBit,
       unsigned FieldOffset = (unsigned)Layout.getFieldOffset(idx);
 
       // If we found a field after the region we care about, then we're done.
-      if (FieldOffset >= EndBit) break;
+      if (FieldOffset >= EndBit)
+        break;
 
-      unsigned FieldStart = FieldOffset < StartBit ? StartBit-FieldOffset :0;
-      if (!BitsContainNoUserData(i->getType(), FieldStart, EndBit-FieldOffset,
+      unsigned FieldStart = FieldOffset < StartBit ? StartBit - FieldOffset : 0;
+      if (!BitsContainNoUserData(i->getType(), FieldStart, EndBit - FieldOffset,
                                  Context))
         return false;
     }
@@ -2400,9 +2395,10 @@ static llvm::Type *getFPTypeAtOffset(llvm::Type *IRType, unsigned IROffset,
 
 /// GetSSETypeAtOffset - Return a type that will be passed by the backend in the
 /// low 8 bytes of an XMM register, corresponding to the SSE class.
-llvm::Type *X86_64ABIInfo::
-GetSSETypeAtOffset(llvm::Type *IRType, unsigned IROffset,
-                   QualType SourceTy, unsigned SourceOffset) const {
+llvm::Type *X86_64ABIInfo::GetSSETypeAtOffset(llvm::Type *IRType,
+                                              unsigned IROffset,
+                                              QualType SourceTy,
+                                              unsigned SourceOffset) const {
   const llvm::DataLayout &TD = getDataLayout();
   unsigned SourceSize =
       (unsigned)getContext().getTypeSize(SourceTy) / 8 - SourceOffset;
@@ -2414,10 +2410,10 @@ GetSSETypeAtOffset(llvm::Type *IRType, unsigned IROffset,
   llvm::Type *T1 = nullptr;
   unsigned T0Size = TD.getTypeAllocSize(T0);
   if (SourceSize > T0Size)
-      T1 = getFPTypeAtOffset(IRType, IROffset + T0Size, TD);
+    T1 = getFPTypeAtOffset(IRType, IROffset + T0Size, TD);
   if (T1 == nullptr) {
-    // Check if IRType is a half/bfloat + float. float type will be in IROffset+4 due
-    // to its alignment.
+    // Check if IRType is a half/bfloat + float. float type will be in
+    // IROffset+4 due to its alignment.
     if (T0->is16bitFPTy() && SourceSize > 4)
       T1 = getFPTypeAtOffset(IRType, IROffset + 4, TD);
     // If we can't get a second FP type, return a simple half or float.
@@ -2445,7 +2441,6 @@ GetSSETypeAtOffset(llvm::Type *IRType, unsigned IROffset,
   return llvm::Type::getDoubleTy(getVMContext());
 }
 
-
 /// GetINTEGERTypeAtOffset - The ABI specifies that a value should be passed in
 /// an 8-byte GPR.  This means that we either have a scalar or we are talking
 /// about the high or low part of an up-to-16-byte struct.  This routine picks
@@ -2460,9 +2455,10 @@ GetSSETypeAtOffset(llvm::Type *IRType, unsigned IROffset,
 /// SourceTy is the source-level type for the entire argument.  SourceOffset is
 /// an offset into this that we're processing (which is always either 0 or 8).
 ///
-llvm::Type *X86_64ABIInfo::
-GetINTEGERTypeAtOffset(llvm::Type *IRType, unsigned IROffset,
-                       QualType SourceTy, unsigned SourceOffset) const {
+llvm::Type *X86_64ABIInfo::GetINTEGERTypeAtOffset(llvm::Type *IRType,
+                                                  unsigned IROffset,
+                                                  QualType SourceTy,
+                                                  unsigned SourceOffset) const {
   // If we're dealing with an un-offset LLVM IR type, then it means that we're
   // returning an 8-byte unit starting with it.  See if we can safely use it.
   if (IROffset == 0) {
@@ -2480,11 +2476,12 @@ GetINTEGERTypeAtOffset(llvm::Type *IRType, unsigned IROffset,
     if (IRType->isIntegerTy(8) || IRType->isIntegerTy(16) ||
         IRType->isIntegerTy(32) ||
         (isa<llvm::PointerType>(IRType) && !Has64BitPointers)) {
-      unsigned BitWidth = isa<llvm::PointerType>(IRType) ? 32 :
-          cast<llvm::IntegerType>(IRType)->getBitWidth();
+      unsigned BitWidth = isa<llvm::PointerType>(IRType)
+                              ? 32
+                              : cast<llvm::IntegerType>(IRType)->getBitWidth();
 
-      if (BitsContainNoUserData(SourceTy, SourceOffset*8+BitWidth,
-                                SourceOffset*8+64, getContext()))
+      if (BitsContainNoUserData(SourceTy, SourceOffset * 8 + BitWidth,
+                                SourceOffset * 8 + 64, getContext()))
         return IRType;
     }
   }
@@ -2504,33 +2501,31 @@ GetINTEGERTypeAtOffset(llvm::Type *IRType, unsigned IROffset,
   if (llvm::ArrayType *ATy = dyn_cast<llvm::ArrayType>(IRType)) {
     llvm::Type *EltTy = ATy->getElementType();
     unsigned EltSize = getDataLayout().getTypeAllocSize(EltTy);
-    unsigned EltOffset = IROffset/EltSize*EltSize;
-    return GetINTEGERTypeAtOffset(EltTy, IROffset-EltOffset, SourceTy,
+    unsigned EltOffset = IROffset / EltSize * EltSize;
+    return GetINTEGERTypeAtOffset(EltTy, IROffset - EltOffset, SourceTy,
                                   SourceOffset);
   }
 
   // Okay, we don't have any better idea of what to pass, so we pass this in an
   // integer register that isn't too big to fit the rest of the struct.
   unsigned TySizeInBytes =
-    (unsigned)getContext().getTypeSizeInChars(SourceTy).getQuantity();
+      (unsigned)getContext().getTypeSizeInChars(SourceTy).getQuantity();
 
   assert(TySizeInBytes != SourceOffset && "Empty field?");
 
   // It is always safe to classify this as an integer type up to i64 that
   // isn't larger than the structure.
   return llvm::IntegerType::get(getVMContext(),
-                                std::min(TySizeInBytes-SourceOffset, 8U)*8);
+                                std::min(TySizeInBytes - SourceOffset, 8U) * 8);
 }
 
-
 /// GetX86_64ByValArgumentPair - Given a high and low type that can ideally
 /// be used as elements of a two register pair to pass or return, return a
 /// first class aggregate to represent them.  For example, if the low part of
 /// a by-value argument should be passed as i32* and the high part as float,
 /// return {i32*, float}.
-static llvm::Type *
-GetX86_64ByValArgumentPair(llvm::Type *Lo, llvm::Type *Hi,
-                           const llvm::DataLayout &TD) {
+static llvm::Type *GetX86_64ByValArgumentPair(llvm::Type *Lo, llvm::Type *Hi,
+                                              const llvm::DataLayout &TD) {
   // In order to correctly satisfy the ABI, we need to the high part to start
   // at offset 8.  If the high and low parts we inferred are both 4-byte types
   // (e.g. i32 and i32) then the resultant struct type ({i32,i32}) won't have
@@ -2553,8 +2548,8 @@ GetX86_64ByValArgumentPair(llvm::Type *Lo, llvm::Type *Hi,
     if (Lo->isHalfTy() || Lo->isFloatTy())
       Lo = llvm::Type::getDoubleTy(Lo->getContext());
     else {
-      assert((Lo->isIntegerTy() || Lo->isPointerTy())
-             && "Invalid/unknown lo type");
+      assert((Lo->isIntegerTy() || Lo->isPointerTy()) &&
+             "Invalid/unknown lo type");
       Lo = llvm::Type::getInt64Ty(Lo->getContext());
     }
   }
@@ -2567,8 +2562,7 @@ GetX86_64ByValArgumentPair(llvm::Type *Lo, llvm::Type *Hi,
   return Result;
 }
 
-ABIArgInfo X86_64ABIInfo::
-classifyReturnType(QualType RetTy) const {
+ABIArgInfo X86_64ABIInfo::classifyReturnType(QualType RetTy) const {
   // AMD64-ABI 3.2.3p4: Rule 1. Classify the return type with the
   // classification algorithm.
   X86_64ABIInfo::Class Lo, Hi;
@@ -2652,12 +2646,12 @@ classifyReturnType(QualType RetTy) const {
 
   case Integer:
     HighPart = GetINTEGERTypeAtOffset(CGT.ConvertType(RetTy), 8, RetTy, 8);
-    if (Lo == NoClass)  // Return HighPart at offset 8 in memory.
+    if (Lo == NoClass) // Return HighPart at offset 8 in memory.
       return ABIArgInfo::getDirect(HighPart, 8);
     break;
   case SSE:
     HighPart = GetSSETypeAtOffset(CGT.ConvertType(RetTy), 8, RetTy, 8);
-    if (Lo == NoClass)  // Return HighPart at offset 8 in memory.
+    if (Lo == NoClass) // Return HighPart at offset 8 in memory.
       return ABIArgInfo::getDirect(HighPart, 8);
     break;
 
@@ -2680,7 +2674,7 @@ classifyReturnType(QualType RetTy) const {
     // extra bits in an SSE reg.
     if (Lo != X87) {
       HighPart = GetSSETypeAtOffset(CGT.ConvertType(RetTy), 8, RetTy, 8);
-      if (Lo == NoClass)  // Return HighPart at offset 8 in memory.
+      if (Lo == NoClass) // Return HighPart at offset 8 in memory.
         return ABIArgInfo::getDirect(HighPart, 8);
     }
     break;
@@ -2782,14 +2776,15 @@ X86_64ABIInfo::classifyArgumentType(QualType Ty, unsigned freeIntRegs,
   case ComplexX87:
     llvm_unreachable("Invalid classification for hi word.");
 
-  case NoClass: break;
+  case NoClass:
+    break;
 
   case Integer:
     ++neededInt;
     // Pick an 8-byte type based on the preferred type.
     HighPart = GetINTEGERTypeAtOffset(CGT.ConvertType(Ty), 8, Ty, 8);
 
-    if (Lo == NoClass)  // Pass HighPart at offset 8 in memory.
+    if (Lo == NoClass) // Pass HighPart at offset 8 in memory.
       return ABIArgInfo::getDirect(HighPart, 8);
     break;
 
@@ -2800,7 +2795,7 @@ X86_64ABIInfo::classifyArgumentType(QualType Ty, unsigned freeIntRegs,
     ++neededSSE;
     HighPart = GetSSETypeAtOffset(CGT.ConvertType(Ty), 8, Ty, 8);
 
-    if (Lo == NoClass)  // Pass HighPart at offset 8 in memory.
+    if (Lo == NoClass) // Pass HighPart at offset 8 in memory.
       return ABIArgInfo::getDirect(HighPart, 8);
     break;
 
@@ -2980,7 +2975,7 @@ static Address EmitX86_64VAArgFromMemory(CodeGenFunction &CGF,
   Address overflow_arg_area_p =
       CGF.Builder.CreateStructGEP(VAListAddr, 2, "overflow_arg_area_p");
   llvm::Value *overflow_arg_area =
-    CGF.Builder.CreateLoad(overflow_arg_area_p, "overflow_arg_area");
+      CGF.Builder.CreateLoad(overflow_arg_area_p, "overflow_arg_area");
 
   // AMD64-ABI 3.5.7p5: Step 7. Align l->overflow_arg_area upwards to a 16
   // byte boundary if alignment needed by type exceeds 8 byte boundary.
@@ -2988,8 +2983,8 @@ static Address EmitX86_64VAArgFromMemory(CodeGenFunction &CGF,
   // alignment greater than 16 where necessary.
   CharUnits Align = CGF.getContext().getTypeAlignInChars(Ty);
   if (Align > CharUnits::fromQuantity(8)) {
-    overflow_arg_area = emitRoundPointerUpToAlignment(CGF, overflow_arg_area,
-                                                      Align);
+    overflow_arg_area =
+        emitRoundPointerUpToAlignment(CGF, overflow_arg_area, Align);
   }
 
   // AMD64-ABI 3.5.7p5: Step 8. Fetch type from l->overflow_arg_area.
@@ -3003,7 +2998,7 @@ static Address EmitX86_64VAArgFromMemory(CodeGenFunction &CGF,
 
   uint64_t SizeInBytes = (CGF.getContext().getTypeSize(Ty) + 7) / 8;
   llvm::Value *Offset =
-      llvm::ConstantInt::get(CGF.Int32Ty, (SizeInBytes + 7)  & ~7);
+      llvm::ConstantInt::get(CGF.Int32Ty, (SizeInBytes + 7) & ~7);
   overflow_arg_area = CGF.Builder.CreateGEP(CGF.Int8Ty, overflow_arg_area,
                                             Offset, "overflow_arg_area.next");
   CGF.Builder.CreateStore(overflow_arg_area, overflow_arg_area_p);
@@ -3025,7 +3020,7 @@ RValue X86_64ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
 
   Ty = getContext().getCanonicalType(Ty);
   ABIArgInfo AI = classifyArgumentType(Ty, 0, neededInt, neededSSE,
-                                       /*isNamedArg*/false);
+                                       /*isNamedArg*/ false);
 
   // Empty records are ignored for parameter passing purposes.
   if (AI.isIgnore())
@@ -3063,7 +3058,7 @@ RValue X86_64ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
     fp_offset_p = CGF.Builder.CreateStructGEP(VAListAddr, 1, "fp_offset_p");
     fp_offset = CGF.Builder.CreateLoad(fp_offset_p, "fp_offset");
     llvm::Value *FitsInFP =
-      llvm::ConstantInt::get(CGF.Int32Ty, 176 - neededSSE * 16);
+        llvm::ConstantInt::get(CGF.Int32Ty, 176 - neededSSE * 16);
     FitsInFP = CGF.Builder.CreateICmpULE(fp_offset, FitsInFP, "fits_in_fp");
     InRegs = InRegs ? CGF.Builder.CreateAnd(InRegs, FitsInFP) : FitsInFP;
   }
@@ -3189,12 +3184,11 @@ RValue X86_64ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
     // to assume that the slots are 16-byte aligned, since the stack is
     // naturally 16-byte aligned and the prologue is expected to store
     // all the SSE registers to the RSA.
-    Address RegAddrLo = Address(CGF.Builder.CreateGEP(CGF.Int8Ty, RegSaveArea,
-                                                      fp_offset),
-                                CGF.Int8Ty, CharUnits::fromQuantity(16));
-    Address RegAddrHi =
-      CGF.Builder.CreateConstInBoundsByteGEP(RegAddrLo,
-                                             CharUnits::fromQuantity(16));
+    Address RegAddrLo =
+        Address(CGF.Builder.CreateGEP(CGF.Int8Ty, RegSaveArea, fp_offset),
+                CGF.Int8Ty, CharUnits::fromQuantity(16));
+    Address RegAddrHi = CGF.Builder.CreateConstInBoundsByteGEP(
+        RegAddrLo, CharUnits::fromQuantity(16));
     llvm::Type *ST = AI.canHaveCoerceToType()
                          ? AI.getCoerceToType()
                          : llvm::StructType::get(CGF.DoubleTy, CGF.DoubleTy);
@@ -3234,8 +3228,8 @@ RValue X86_64ABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
   // Return the appropriate result.
 
   CGF.EmitBlock(ContBlock);
-  Address ResAddr = emitMergePHI(CGF, RegAddr, InRegBlock, MemAddr, InMemBlock,
-                                 "vaarg.addr");
+  Address ResAddr =
+      emitMergePHI(CGF, RegAddr, InRegBlock, MemAddr, InMemBlock, "vaarg.addr");
   return CGF.EmitLoadOfAnyValue(CGF.MakeAddrLValue(ResAddr, Ty), Slot);
 }
 
@@ -3288,7 +3282,6 @@ ABIArgInfo WinX86_64ABIInfo::classify(QualType Ty, unsigned &FreeSSERegs,
 
     if (RT->getDecl()->hasFlexibleArrayMember())
       return getNaturalAlignIndirect(Ty, /*ByVal=*/false);
-
   }
 
   const Type *Base = nullptr;
diff --git a/clang/lib/CodeGen/Targets/XCore.cpp b/clang/lib/CodeGen/Targets/XCore.cpp
index ced4981fd..76a383fde 100644
--- a/clang/lib/CodeGen/Targets/XCore.cpp
+++ b/clang/lib/CodeGen/Targets/XCore.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "ABIInfoImpl.h"
-#include "TargetInfo.h"
+#include "../ABIInfoImpl.h"
+#include "../TargetInfo.h"
 
 using namespace clang;
 using namespace clang::CodeGen;
@@ -77,7 +77,7 @@ typedef llvm::SmallString<128> SmallStringEnc;
 ///   been exited too soon for the encoding to be correct for the member.
 ///
 class TypeStringCache {
-  enum Status {NonRecursive, Recursive, Incomplete, IncompleteUsed};
+  enum Status { NonRecursive, Recursive, Incomplete, IncompleteUsed };
   struct Entry {
     std::string Str;     // The encoded TypeString for the type.
     enum Status State;   // Information about the encoding in 'Str'.
@@ -91,8 +91,7 @@ public:
   TypeStringCache() : IncompleteCount(0), IncompleteUsedCount(0) {}
   void addIncomplete(const IdentifierInfo *ID, std::string StubEnc);
   bool removeIncomplete(const IdentifierInfo *ID);
-  void addIfComplete(const IdentifierInfo *ID, StringRef Str,
-                     bool IsRecursive);
+  void addIfComplete(const IdentifierInfo *ID, StringRef Str, bool IsRecursive);
   StringRef lookupStr(const IdentifierInfo *ID);
 };
 
@@ -101,11 +100,13 @@ public:
 class FieldEncoding {
   bool HasName;
   std::string Enc;
+
 public:
   FieldEncoding(bool b, SmallStringEnc &e) : HasName(b), Enc(e.c_str()) {}
   StringRef str() { return Enc; }
   bool operator<(const FieldEncoding &rhs) const {
-    if (HasName != rhs.HasName) return HasName;
+    if (HasName != rhs.HasName)
+      return HasName;
     return Enc < rhs.Enc;
   }
 };
@@ -196,7 +197,7 @@ void TypeStringCache::addIncomplete(const IdentifierInfo *ID,
   if (!ID)
     return;
   Entry &E = Map[ID];
-  assert( (E.Str.empty() || E.State == Recursive) &&
+  assert((E.Str.empty() || E.State == Recursive) &&
          "Incorrectly use of addIncomplete");
   assert(!StubEnc.empty() && "Passing an empty string to addIncomplete()");
   E.Swapped.swap(E.Str); // swap out the Recursive
@@ -215,8 +216,7 @@ bool TypeStringCache::removeIncomplete(const IdentifierInfo *ID) {
   auto I = Map.find(ID);
   assert(I != Map.end() && "Entry not present");
   Entry &E = I->second;
-  assert( (E.State == Incomplete ||
-           E.State == IncompleteUsed) &&
+  assert((E.State == Incomplete || E.State == IncompleteUsed) &&
          "Entry must be an incomplete type");
   bool IsRecursive = false;
   if (E.State == IncompleteUsed) {
@@ -244,7 +244,7 @@ void TypeStringCache::addIfComplete(const IdentifierInfo *ID, StringRef Str,
     return; // No key or it is an incomplete sub-type so don't add.
   Entry &E = Map[ID];
   if (IsRecursive && !E.Str.empty()) {
-    assert(E.State==Recursive && E.Str.size() == Str.size() &&
+    assert(E.State == Recursive && E.Str.size() == Str.size() &&
            "This is not the same Recursive entry");
     // The parent container was not recursive after all, so we could have used
     // this Recursive sub-member entry after all, but we assumed the worse when
@@ -253,7 +253,7 @@ void TypeStringCache::addIfComplete(const IdentifierInfo *ID, StringRef Str,
   }
   assert(E.Str.empty() && "Entry already present");
   E.Str = Str.str();
-  E.State = IsRecursive? Recursive : NonRecursive;
+  E.State = IsRecursive ? Recursive : NonRecursive;
 }
 
 /// Return a cached TypeString encoding for the ID. If there isn't one, or we
@@ -261,13 +261,13 @@ void TypeStringCache::addIfComplete(const IdentifierInfo *ID, StringRef Str,
 /// encoding is Recursive, return an empty StringRef.
 StringRef TypeStringCache::lookupStr(const IdentifierInfo *ID) {
   if (!ID)
-    return StringRef();   // We have no key.
+    return StringRef(); // We have no key.
   auto I = Map.find(ID);
   if (I == Map.end())
-    return StringRef();   // We have no encoding.
+    return StringRef(); // We have no encoding.
   Entry &E = I->second;
   if (E.State == Recursive && IncompleteCount)
-    return StringRef();   // We don't use Recursive encodings for member types.
+    return StringRef(); // We don't use Recursive encodings for member types.
 
   if (E.State == Incomplete) {
     // The incomplete type is being used to break out of recursion.
@@ -303,7 +303,7 @@ void XCoreTargetCodeGenInfo::emitTargetMD(
     llvm::Metadata *MDVals[] = {llvm::ConstantAsMetadata::get(GV),
                                 llvm::MDString::get(Ctx, Enc.str())};
     llvm::NamedMDNode *MD =
-      CGM.getModule().getOrInsertNamedMetadata("xcore.typestrings");
+        CGM.getModule().getOrInsertNamedMetadata("xcore.typestrings");
     MD->addOperand(llvm::MDNode::get(Ctx, MDVals));
   }
 }
@@ -325,8 +325,7 @@ void XCoreTargetCodeGenInfo::emitTargetMetadata(
 }
 
 static bool appendType(SmallStringEnc &Enc, QualType QType,
-                       const CodeGen::CodeGenModule &CGM,
-                       TypeStringCache &TSC);
+                       const CodeGen::CodeGenModule &CGM, TypeStringCache &TSC);
 
 /// Helper function for appendRecordType().
 /// Builds a SmallVector containing the encoded field types in declaration
@@ -371,7 +370,7 @@ static bool appendRecordType(SmallStringEnc &Enc, const RecordType *RT,
 
   // Start to emit an incomplete TypeString.
   size_t Start = Enc.size();
-  Enc += (RT->isUnionType()? 'u' : 's');
+  Enc += (RT->isUnionType() ? 'u' : 's');
   Enc += '(';
   if (ID)
     Enc += ID->getName();
@@ -386,10 +385,10 @@ static bool appendRecordType(SmallStringEnc &Enc, const RecordType *RT,
     // complete TypeString for this RecordType.
     SmallVector<FieldEncoding, 16> FE;
     std::string StubEnc(Enc.substr(Start).str());
-    StubEnc += '}';  // StubEnc now holds a valid incomplete TypeString.
+    StubEnc += '}'; // StubEnc now holds a valid incomplete TypeString.
     TSC.addIncomplete(ID, std::move(StubEnc));
     if (!extractFieldType(FE, RD, CGM, TSC)) {
-      (void) TSC.removeIncomplete(ID);
+      (void)TSC.removeIncomplete(ID);
       return false;
     }
     IsRecursive = TSC.removeIncomplete(ID);
@@ -412,8 +411,7 @@ static bool appendRecordType(SmallStringEnc &Enc, const RecordType *RT,
 
 /// Appends enum types to Enc and adds the encoding to the cache.
 static bool appendEnumType(SmallStringEnc &Enc, const EnumType *ET,
-                           TypeStringCache &TSC,
-                           const IdentifierInfo *ID) {
+                           TypeStringCache &TSC, const IdentifierInfo *ID) {
   // Append the cached TypeString if we have one.
   StringRef TypeString = TSC.lookupStr(ID);
   if (!TypeString.empty()) {
@@ -457,14 +455,15 @@ static bool appendEnumType(SmallStringEnc &Enc, const EnumType *ET,
 /// This is done prior to appending the type's encoding.
 static void appendQualifier(SmallStringEnc &Enc, QualType QT) {
   // Qualifiers are emitted in alphabetical order.
-  static const char *const Table[]={"","c:","r:","cr:","v:","cv:","rv:","crv:"};
+  static const char *const Table[] = {
+      "", "c:", "r:", "cr:", "v:", "cv:", "rv:", "crv:"};
   int Lookup = 0;
   if (QT.isConstQualified())
-    Lookup += 1<<0;
+    Lookup += 1 << 0;
   if (QT.isRestrictQualified())
-    Lookup += 1<<1;
+    Lookup += 1 << 1;
   if (QT.isVolatileQualified())
-    Lookup += 1<<2;
+    Lookup += 1 << 2;
   Enc += Table[Lookup];
 }
 
@@ -472,56 +471,56 @@ static void appendQualifier(SmallStringEnc &Enc, QualType QT) {
 static bool appendBuiltinType(SmallStringEnc &Enc, const BuiltinType *BT) {
   const char *EncType;
   switch (BT->getKind()) {
-    case BuiltinType::Void:
-      EncType = "0";
-      break;
-    case BuiltinType::Bool:
-      EncType = "b";
-      break;
-    case BuiltinType::Char_U:
-      EncType = "uc";
-      break;
-    case BuiltinType::UChar:
-      EncType = "uc";
-      break;
-    case BuiltinType::SChar:
-      EncType = "sc";
-      break;
-    case BuiltinType::UShort:
-      EncType = "us";
-      break;
-    case BuiltinType::Short:
-      EncType = "ss";
-      break;
-    case BuiltinType::UInt:
-      EncType = "ui";
-      break;
-    case BuiltinType::Int:
-      EncType = "si";
-      break;
-    case BuiltinType::ULong:
-      EncType = "ul";
-      break;
-    case BuiltinType::Long:
-      EncType = "sl";
-      break;
-    case BuiltinType::ULongLong:
-      EncType = "ull";
-      break;
-    case BuiltinType::LongLong:
-      EncType = "sll";
-      break;
-    case BuiltinType::Float:
-      EncType = "ft";
-      break;
-    case BuiltinType::Double:
-      EncType = "d";
-      break;
-    case BuiltinType::LongDouble:
-      EncType = "ld";
-      break;
-    default:
-      return false;
+  case BuiltinType::Void:
+    EncType = "0";
+    break;
+  case BuiltinType::Bool:
+    EncType = "b";
+    break;
+  case BuiltinType::Char_U:
+    EncType = "uc";
+    break;
+  case BuiltinType::UChar:
+    EncType = "uc";
+    break;
+  case BuiltinType::SChar:
+    EncType = "sc";
+    break;
+  case BuiltinType::UShort:
+    EncType = "us";
+    break;
+  case BuiltinType::Short:
+    EncType = "ss";
+    break;
+  case BuiltinType::UInt:
+    EncType = "ui";
+    break;
+  case BuiltinType::Int:
+    EncType = "si";
+    break;
+  case BuiltinType::ULong:
+    EncType = "ul";
+    break;
+  case BuiltinType::Long:
+    EncType = "sl";
+    break;
+  case BuiltinType::ULongLong:
+    EncType = "ull";
+    break;
+  case BuiltinType::LongLong:
+    EncType = "sll";
+    break;
+  case BuiltinType::Float:
+    EncType = "ft";
+    break;
+  case BuiltinType::Double:
+    EncType = "d";
+    break;
+  case BuiltinType::LongDouble:
+    EncType = "ld";
+    break;
+  default:
+    return false;
   }
   Enc += EncType;
   return true;
@@ -562,8 +561,8 @@ static bool appendArrayType(SmallStringEnc &Enc, QualType QT,
 /// Appends a function encoding to Enc, calling appendType for the return type
 /// and the arguments.
 static bool appendFunctionType(SmallStringEnc &Enc, const FunctionType *FT,
-                             const CodeGen::CodeGenModule &CGM,
-                             TypeStringCache &TSC) {
+                               const CodeGen::CodeGenModule &CGM,
+                               TypeStringCache &TSC) {
   Enc += "f{";
   if (!appendType(Enc, FT->getReturnType(), CGM, TSC))
     return false;
diff --git a/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp b/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp
index 2ffbc1a22..0e06d21cc 100644
--- a/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp
+++ b/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp
@@ -6,7 +6,7 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "DirectoryScanner.h"
+#include "../DirectoryScanner.h"
 #include "clang/DirectoryWatcher/DirectoryWatcher.h"
 
 #include "llvm/ADT/STLExtras.h"
diff --git a/clang/lib/DirectoryWatcher/mac/DirectoryWatcher-mac.cpp b/clang/lib/DirectoryWatcher/mac/DirectoryWatcher-mac.cpp
index b8788bae8..07e9ecbe5 100644
--- a/clang/lib/DirectoryWatcher/mac/DirectoryWatcher-mac.cpp
+++ b/clang/lib/DirectoryWatcher/mac/DirectoryWatcher-mac.cpp
@@ -6,7 +6,7 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "DirectoryScanner.h"
+#include "../DirectoryScanner.h"
 #include "clang/DirectoryWatcher/DirectoryWatcher.h"
 
 #include "llvm/ADT/STLExtras.h"
diff --git a/clang/lib/DirectoryWatcher/windows/DirectoryWatcher-windows.cpp b/clang/lib/DirectoryWatcher/windows/DirectoryWatcher-windows.cpp
index 110d40243..1b6a3e54d 100644
--- a/clang/lib/DirectoryWatcher/windows/DirectoryWatcher-windows.cpp
+++ b/clang/lib/DirectoryWatcher/windows/DirectoryWatcher-windows.cpp
@@ -6,7 +6,7 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "DirectoryScanner.h"
+#include "../DirectoryScanner.h"
 #include "clang/DirectoryWatcher/DirectoryWatcher.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/Support/ConvertUTF.h"
diff --git a/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp b/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp
index 3f0b3f2d8..37625b9d6 100644
--- a/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp
+++ b/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp
@@ -9,7 +9,7 @@
 #include "AMDGPUOpenMP.h"
 #include "AMDGPU.h"
 #include "CommonArgs.h"
-#include "ToolChains/ROCm.h"
+#include "ROCm.h"
 #include "clang/Basic/DiagnosticDriver.h"
 #include "clang/Driver/Compilation.h"
 #include "clang/Driver/Driver.h"
diff --git a/clang/lib/Driver/ToolChains/Arch/CSKY.cpp b/clang/lib/Driver/ToolChains/Arch/CSKY.cpp
index e94ea12f4..2b318305a 100644
--- a/clang/lib/Driver/ToolChains/Arch/CSKY.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/CSKY.cpp
@@ -7,7 +7,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "CSKY.h"
-#include "ToolChains/CommonArgs.h"
+#include "../CommonArgs.h"
 #include "clang/Basic/CharInfo.h"
 #include "clang/Driver/Driver.h"
 #include "clang/Driver/DriverDiagnostic.h"
diff --git a/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp b/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp
index 0575a1ebe..d558db3e5 100644
--- a/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp
@@ -8,7 +8,7 @@
 
 #include "LoongArch.h"
 #include "../Clang.h"
-#include "ToolChains/CommonArgs.h"
+#include "../CommonArgs.h"
 #include "clang/Basic/DiagnosticDriver.h"
 #include "clang/Driver/Driver.h"
 #include "clang/Driver/DriverDiagnostic.h"
@@ -284,8 +284,8 @@ void loongarch::getLoongArchTargetFeatures(const Driver &D,
                    options::OPT_mno_lamcas, "lamcas");
   AddTargetFeature(Args, Features, options::OPT_mld_seq_sa,
                    options::OPT_mno_ld_seq_sa, "ld-seq-sa");
-  AddTargetFeature(Args, Features, options::OPT_mdiv32,
-                   options::OPT_mno_div32, "div32");
+  AddTargetFeature(Args, Features, options::OPT_mdiv32, options::OPT_mno_div32,
+                   "div32");
   AddTargetFeature(Args, Features, options::OPT_mscq, options::OPT_mno_scq,
                    "scq");
 }
diff --git a/clang/lib/Driver/ToolChains/Arch/M68k.cpp b/clang/lib/Driver/ToolChains/Arch/M68k.cpp
index 963f7a187..f71cdbf1c 100644
--- a/clang/lib/Driver/ToolChains/Arch/M68k.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/M68k.cpp
@@ -7,7 +7,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "M68k.h"
-#include "ToolChains/CommonArgs.h"
+#include "../CommonArgs.h"
 #include "clang/Driver/Driver.h"
 #include "clang/Driver/DriverDiagnostic.h"
 #include "clang/Driver/Options.h"
diff --git a/clang/lib/Driver/ToolChains/Arch/Mips.cpp b/clang/lib/Driver/ToolChains/Arch/Mips.cpp
index ca0745fc2..abdf96215 100644
--- a/clang/lib/Driver/ToolChains/Arch/Mips.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/Mips.cpp
@@ -7,7 +7,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "Mips.h"
-#include "ToolChains/CommonArgs.h"
+#include "../CommonArgs.h"
 #include "clang/Driver/Driver.h"
 #include "clang/Driver/DriverDiagnostic.h"
 #include "clang/Driver/Options.h"
@@ -232,9 +232,8 @@ void mips::getMIPSTargetFeatures(const Driver &D, const llvm::Triple &Triple,
     NonPIC =
         (O.matches(options::OPT_fno_PIC) || O.matches(options::OPT_fno_pic) ||
          O.matches(options::OPT_fno_PIE) || O.matches(options::OPT_fno_pie));
-    IsPIC =
-        (O.matches(options::OPT_fPIC) || O.matches(options::OPT_fpic) ||
-         O.matches(options::OPT_fPIE) || O.matches(options::OPT_fpie));
+    IsPIC = (O.matches(options::OPT_fPIC) || O.matches(options::OPT_fpic) ||
+             O.matches(options::OPT_fPIE) || O.matches(options::OPT_fpie));
   }
 
   bool UseAbiCalls = false;
diff --git a/clang/lib/Driver/ToolChains/Arch/PPC.cpp b/clang/lib/Driver/ToolChains/Arch/PPC.cpp
index 57baa186a..ad914d4c9 100644
--- a/clang/lib/Driver/ToolChains/Arch/PPC.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/PPC.cpp
@@ -7,7 +7,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "PPC.h"
-#include "ToolChains/CommonArgs.h"
+#include "../CommonArgs.h"
 #include "clang/Driver/Driver.h"
 #include "clang/Driver/DriverDiagnostic.h"
 #include "clang/Driver/Options.h"
@@ -74,7 +74,8 @@ void ppc::getPPCTargetFeatures(const Driver &D, const llvm::Triple &Triple,
   }
 }
 
-ppc::ReadGOTPtrMode ppc::getPPCReadGOTPtrMode(const Driver &D, const llvm::Triple &Triple,
+ppc::ReadGOTPtrMode ppc::getPPCReadGOTPtrMode(const Driver &D,
+                                              const llvm::Triple &Triple,
                                               const ArgList &Args) {
   if (Args.getLastArg(options::OPT_msecure_plt))
     return ppc::ReadGOTPtrMode::SecurePlt;
diff --git a/clang/lib/Driver/ToolChains/Arch/RISCV.cpp b/clang/lib/Driver/ToolChains/Arch/RISCV.cpp
index 6935904a2..344f8f683 100644
--- a/clang/lib/Driver/ToolChains/Arch/RISCV.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/RISCV.cpp
@@ -8,7 +8,7 @@
 
 #include "RISCV.h"
 #include "../Clang.h"
-#include "ToolChains/CommonArgs.h"
+#include "../CommonArgs.h"
 #include "clang/Basic/CharInfo.h"
 #include "clang/Driver/Driver.h"
 #include "clang/Driver/DriverDiagnostic.h"
@@ -54,8 +54,7 @@ static bool getArchFeatures(const Driver &D, StringRef Arch,
 
 // Get features except standard extension feature
 static void getRISCFeaturesFromMcpu(const Driver &D, const Arg *A,
-                                    const llvm::Triple &Triple,
-                                    StringRef Mcpu,
+                                    const llvm::Triple &Triple, StringRef Mcpu,
                                     std::vector<StringRef> &Features) {
   bool Is64Bit = Triple.isRISCV64();
   if (!llvm::RISCV::parseCPU(Mcpu, Is64Bit)) {
diff --git a/clang/lib/Driver/ToolChains/Arch/X86.cpp b/clang/lib/Driver/ToolChains/Arch/X86.cpp
index 47c2c3e23..2afff722d 100644
--- a/clang/lib/Driver/ToolChains/Arch/X86.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/X86.cpp
@@ -7,7 +7,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "X86.h"
-#include "ToolChains/CommonArgs.h"
+#include "../CommonArgs.h"
 #include "clang/Driver/Driver.h"
 #include "clang/Driver/DriverDiagnostic.h"
 #include "clang/Driver/Options.h"
diff --git a/clang/lib/Sema/SemaDeclAttr.cpp b/clang/lib/Sema/SemaDeclAttr.cpp
index f351663c6..1c7ec3cb0 100644
--- a/clang/lib/Sema/SemaDeclAttr.cpp
+++ b/clang/lib/Sema/SemaDeclAttr.cpp
@@ -73,11 +73,7 @@ using namespace clang;
 using namespace sema;
 
 namespace AttributeLangSupport {
-  enum LANG {
-    C,
-    Cpp,
-    ObjC
-  };
+enum LANG { C, Cpp, ObjC };
 } // end namespace AttributeLangSupport
 
 static unsigned getNumAttributeArgs(const ParsedAttr &AL) {
@@ -91,8 +87,9 @@ SourceLocation Sema::getAttrLoc(const ParsedAttr &AL) { return AL.getLoc(); }
 /// that the result will fit into a regular (signed) int. All args have the same
 /// purpose as they do in checkUInt32Argument.
 template <typename AttrInfo>
-static bool checkPositiveIntArgument(Sema &S, const AttrInfo &AI, const Expr *Expr,
-                                     int &Val, unsigned Idx = UINT_MAX) {
+static bool checkPositiveIntArgument(Sema &S, const AttrInfo &AI,
+                                     const Expr *Expr, int &Val,
+                                     unsigned Idx = UINT_MAX) {
   uint32_t UVal;
   if (!S.checkUInt32Argument(AI, Expr, UVal, Idx))
     return false;
@@ -163,10 +160,9 @@ static bool isIntOrBool(Expr *Exp) {
   return QT->isBooleanType() || QT->isIntegerType();
 }
 
-
 // Check to see if the type is a smart pointer of some kind.  We assume
 // it's a smart pointer if it defines both operator-> and operator*.
-static bool threadSafetyCheckIsSmartPointer(Sema &S, const RecordType* RT) {
+static bool threadSafetyCheckIsSmartPointer(Sema &S, const RecordType *RT) {
   auto IsOverloadedOperatorPresent = [&S](const RecordDecl *Record,
                                           OverloadedOperatorKind Op) {
     DeclContextLookupResult Result =
@@ -399,10 +395,10 @@ static void checkAttrArgsAreCapabilityObjs(Sema &S, Decl *D,
     const RecordType *RT = getRecordType(ArgTy);
 
     // Now check if we index into a record type function param.
-    if(!RT && ParamIdxOk) {
+    if (!RT && ParamIdxOk) {
       const auto *FD = dyn_cast<FunctionDecl>(D);
       const auto *IL = dyn_cast<IntegerLiteral>(ArgExp);
-      if(FD && IL) {
+      if (FD && IL) {
         unsigned int NumParams = FD->getNumParams();
         llvm::APInt ArgValue = IL->getValue();
         uint64_t ParamIdxFromOne = ArgValue.getZExtValue();
@@ -639,7 +635,7 @@ static bool checkTryLockFunAttrCommon(Sema &S, Decl *D, const ParsedAttr &AL,
 
 static void handleSharedTrylockFunctionAttr(Sema &S, Decl *D,
                                             const ParsedAttr &AL) {
-  SmallVector<Expr*, 2> Args;
+  SmallVector<Expr *, 2> Args;
   if (!checkTryLockFunAttrCommon(S, D, AL, Args))
     return;
 
@@ -649,7 +645,7 @@ static void handleSharedTrylockFunctionAttr(Sema &S, Decl *D,
 
 static void handleExclusiveTrylockFunctionAttr(Sema &S, Decl *D,
                                                const ParsedAttr &AL) {
-  SmallVector<Expr*, 2> Args;
+  SmallVector<Expr *, 2> Args;
   if (!checkTryLockFunAttrCommon(S, D, AL, Args))
     return;
 
@@ -659,7 +655,7 @@ static void handleExclusiveTrylockFunctionAttr(Sema &S, Decl *D,
 
 static void handleLockReturnedAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   // check that the argument is lockable object
-  SmallVector<Expr*, 1> Args;
+  SmallVector<Expr *, 1> Args;
   checkAttrArgsAreCapabilityObjs(S, D, AL, Args);
   unsigned Size = Args.size();
   if (Size == 0)
@@ -677,7 +673,7 @@ static void handleLocksExcludedAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     return;
 
   // check that all arguments are lockable objects
-  SmallVector<Expr*, 1> Args;
+  SmallVector<Expr *, 1> Args;
   checkAttrArgsAreCapabilityObjs(S, D, AL, Args);
   unsigned Size = Args.size();
   if (Size == 0)
@@ -791,7 +787,7 @@ public:
     return true;
   }
 };
-}
+} // namespace
 
 static void handleDiagnoseAsBuiltinAttr(Sema &S, Decl *D,
                                         const ParsedAttr &AL) {
@@ -1002,8 +998,8 @@ static void handleConsumableAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     IdentifierLoc *IL = AL.getArgAsIdent(0);
     if (!ConsumableAttr::ConvertStrToConsumedState(IL->Ident->getName(),
                                                    DefaultState)) {
-      S.Diag(IL->Loc, diag::warn_attribute_type_not_supported) << AL
-                                                               << IL->Ident;
+      S.Diag(IL->Loc, diag::warn_attribute_type_not_supported)
+          << AL << IL->Ident;
       return;
     }
   } else {
@@ -1087,10 +1083,10 @@ static void handleParamTypestateAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   // FIXME: This check is currently being done in the analysis.  It can be
   //        enabled here only after the parser propagates attributes at
   //        template specialization definition, not declaration.
-  //QualType ReturnType = cast<ParmVarDecl>(D)->getType();
-  //const CXXRecordDecl *RD = ReturnType->getAsCXXRecordDecl();
+  // QualType ReturnType = cast<ParmVarDecl>(D)->getType();
+  // const CXXRecordDecl *RD = ReturnType->getAsCXXRecordDecl();
   //
-  //if (!RD || !RD->hasAttr<ConsumableAttr>()) {
+  // if (!RD || !RD->hasAttr<ConsumableAttr>()) {
   //    S.Diag(AL.getLoc(), diag::warn_return_state_for_unconsumable_type) <<
   //      ReturnType.getAsString();
   //    return;
@@ -1106,8 +1102,8 @@ static void handleReturnTypestateAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     IdentifierLoc *IL = AL.getArgAsIdent(0);
     if (!ReturnTypestateAttr::ConvertStrToConsumedState(IL->Ident->getName(),
                                                         ReturnState)) {
-      S.Diag(IL->Loc, diag::warn_attribute_type_not_supported) << AL
-                                                               << IL->Ident;
+      S.Diag(IL->Loc, diag::warn_attribute_type_not_supported)
+          << AL << IL->Ident;
       return;
     }
   } else {
@@ -1153,8 +1149,8 @@ static void handleSetTypestateAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     IdentifierLoc *Ident = AL.getArgAsIdent(0);
     StringRef Param = Ident->Ident->getName();
     if (!SetTypestateAttr::ConvertStrToConsumedState(Param, NewState)) {
-      S.Diag(Ident->Loc, diag::warn_attribute_type_not_supported) << AL
-                                                                  << Param;
+      S.Diag(Ident->Loc, diag::warn_attribute_type_not_supported)
+          << AL << Param;
       return;
     }
   } else {
@@ -1175,8 +1171,8 @@ static void handleTestTypestateAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     IdentifierLoc *Ident = AL.getArgAsIdent(0);
     StringRef Param = Ident->Ident->getName();
     if (!TestTypestateAttr::ConvertStrToConsumedState(Param, TestState)) {
-      S.Diag(Ident->Loc, diag::warn_attribute_type_not_supported) << AL
-                                                                  << Param;
+      S.Diag(Ident->Loc, diag::warn_attribute_type_not_supported)
+          << AL << Param;
       return;
     }
   } else {
@@ -1197,10 +1193,10 @@ static void handlePackedAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   if (auto *TD = dyn_cast<TagDecl>(D))
     TD->addAttr(::new (S.Context) PackedAttr(S.Context, AL));
   else if (auto *FD = dyn_cast<FieldDecl>(D)) {
-    bool BitfieldByteAligned = (!FD->getType()->isDependentType() &&
-                                !FD->getType()->isIncompleteType() &&
-                                FD->isBitField() &&
-                                S.Context.getTypeAlign(FD->getType()) <= 8);
+    bool BitfieldByteAligned =
+        (!FD->getType()->isDependentType() &&
+         !FD->getType()->isIncompleteType() && FD->isBitField() &&
+         S.Context.getTypeAlign(FD->getType()) <= 8);
 
     if (S.getASTContext().getTargetInfo().getTriple().isPS()) {
       if (BitfieldByteAligned)
@@ -1358,7 +1354,7 @@ static void handleNonNullAttrParameter(Sema &S, ParmVarDecl *D,
       handleNonNullAttr(S, D, AL);
     } else {
       S.Diag(AL.getLoc(), diag::warn_attribute_nonnull_parm_no_args)
-        << D->getSourceRange();
+          << D->getSourceRange();
     }
     return;
   }
@@ -1425,18 +1421,17 @@ void Sema::AddAssumeAlignedAttr(Decl *D, const AttributeCommonInfo &CI, Expr *E,
     if (!(I = E->getIntegerConstantExpr(Context))) {
       if (OE)
         Diag(AttrLoc, diag::err_attribute_argument_n_type)
-          << &TmpAttr << 1 << AANT_ArgumentIntegerConstant
-          << E->getSourceRange();
+            << &TmpAttr << 1 << AANT_ArgumentIntegerConstant
+            << E->getSourceRange();
       else
         Diag(AttrLoc, diag::err_attribute_argument_type)
-          << &TmpAttr << AANT_ArgumentIntegerConstant
-          << E->getSourceRange();
+            << &TmpAttr << AANT_ArgumentIntegerConstant << E->getSourceRange();
       return;
     }
 
     if (!I->isPowerOf2()) {
       Diag(AttrLoc, diag::err_alignment_not_power_of_two)
-        << E->getSourceRange();
+          << E->getSourceRange();
       return;
     }
 
@@ -1557,21 +1552,21 @@ static void handleOwnershipAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
 
     // Is the function argument a pointer type?
     QualType T = getFunctionOrMethodParamType(D, Idx.getASTIndex());
-    int Err = -1;  // No error
+    int Err = -1; // No error
     switch (K) {
-      case OwnershipAttr::Takes:
-      case OwnershipAttr::Holds:
-        if (!T->isAnyPointerType() && !T->isBlockPointerType())
-          Err = 0;
-        break;
-      case OwnershipAttr::Returns:
-        if (!T->isIntegerType())
-          Err = 1;
-        break;
+    case OwnershipAttr::Takes:
+    case OwnershipAttr::Holds:
+      if (!T->isAnyPointerType() && !T->isBlockPointerType())
+        Err = 0;
+      break;
+    case OwnershipAttr::Returns:
+      if (!T->isIntegerType())
+        Err = 1;
+      break;
     }
     if (-1 != Err) {
-      S.Diag(AL.getLoc(), diag::err_ownership_type) << AL << Err
-                                                    << Ex->getSourceRange();
+      S.Diag(AL.getLoc(), diag::err_ownership_type)
+          << AL << Err << Ex->getSourceRange();
       return;
     }
 
@@ -1580,11 +1575,11 @@ static void handleOwnershipAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
       // Cannot have two ownership attributes of different kinds for the same
       // index.
       if (I->getOwnKind() != K && llvm::is_contained(I->args(), Idx)) {
-          S.Diag(AL.getLoc(), diag::err_attributes_are_not_compatible)
-              << AL << I
-              << (AL.isRegularKeywordAttribute() ||
-                  I->isRegularKeywordAttribute());
-          return;
+        S.Diag(AL.getLoc(), diag::err_attributes_are_not_compatible)
+            << AL << I
+            << (AL.isRegularKeywordAttribute() ||
+                I->isRegularKeywordAttribute());
+        return;
       } else if (K == OwnershipAttr::Returns &&
                  I->getOwnKind() == OwnershipAttr::Returns) {
         // A returns attribute conflicts with any other returns attribute using
@@ -1768,8 +1763,8 @@ static void handleTLSModelAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     return;
 
   // Check that the value.
-  if (Model != "global-dynamic" && Model != "local-dynamic"
-      && Model != "initial-exec" && Model != "local-exec") {
+  if (Model != "global-dynamic" && Model != "local-dynamic" &&
+      Model != "initial-exec" && Model != "local-exec") {
     S.Diag(LiteralLoc, diag::err_attr_tlsmodel_arg);
     return;
   }
@@ -1894,7 +1889,8 @@ static void handleNakedAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
 }
 
 static void handleNoReturnAttr(Sema &S, Decl *D, const ParsedAttr &Attrs) {
-  if (hasDeclarator(D)) return;
+  if (hasDeclarator(D))
+    return;
 
   if (!isa<ObjCMethodDecl>(D)) {
     S.Diag(Attrs.getLoc(), diag::warn_attribute_wrong_decl_type)
@@ -2032,8 +2028,7 @@ static void handleDependencyAttr(Sema &S, Scope *Scope, Decl *D,
     // [[carries_dependency]] can only be applied to a parameter if it is a
     // parameter of a function declaration or lambda.
     if (!(Scope->getFlags() & clang::Scope::FunctionDeclarationScope)) {
-      S.Diag(AL.getLoc(),
-             diag::err_carries_dependency_param_not_function_decl);
+      S.Diag(AL.getLoc(), diag::err_carries_dependency_param_not_function_decl);
       return;
     }
   }
@@ -2089,8 +2084,8 @@ static bool checkAvailabilityAttr(Sema &S, SourceRange Range,
                                   VersionTuple Introduced,
                                   VersionTuple Deprecated,
                                   VersionTuple Obsoleted) {
-  StringRef PlatformName
-    = AvailabilityAttr::getPrettyPlatformName(Platform->getName());
+  StringRef PlatformName =
+      AvailabilityAttr::getPrettyPlatformName(Platform->getName());
   if (PlatformName.empty())
     PlatformName = Platform->getName();
 
@@ -2099,24 +2094,22 @@ static bool checkAvailabilityAttr(Sema &S, SourceRange Range,
   if (!Introduced.empty() && !Deprecated.empty() &&
       !(Introduced <= Deprecated)) {
     S.Diag(Range.getBegin(), diag::warn_availability_version_ordering)
-      << 1 << PlatformName << Deprecated.getAsString()
-      << 0 << Introduced.getAsString();
+        << 1 << PlatformName << Deprecated.getAsString() << 0
+        << Introduced.getAsString();
     return true;
   }
 
-  if (!Introduced.empty() && !Obsoleted.empty() &&
-      !(Introduced <= Obsoleted)) {
+  if (!Introduced.empty() && !Obsoleted.empty() && !(Introduced <= Obsoleted)) {
     S.Diag(Range.getBegin(), diag::warn_availability_version_ordering)
-      << 2 << PlatformName << Obsoleted.getAsString()
-      << 0 << Introduced.getAsString();
+        << 2 << PlatformName << Obsoleted.getAsString() << 0
+        << Introduced.getAsString();
     return true;
   }
 
-  if (!Deprecated.empty() && !Obsoleted.empty() &&
-      !(Deprecated <= Obsoleted)) {
+  if (!Deprecated.empty() && !Obsoleted.empty() && !(Deprecated <= Obsoleted)) {
     S.Diag(Range.getBegin(), diag::warn_availability_version_ordering)
-      << 2 << PlatformName << Obsoleted.getAsString()
-      << 1 << Deprecated.getAsString();
+        << 2 << PlatformName << Obsoleted.getAsString() << 1
+        << Deprecated.getAsString();
     return true;
   }
 
@@ -2220,7 +2213,8 @@ AvailabilityAttr *Sema::mergeAvailabilityAttr(
             Which = 0;
             FirstVersion = OldIntroduced;
             SecondVersion = Introduced;
-          } else if (!versionsMatch(Deprecated, OldDeprecated, OverrideOrImpl)) {
+          } else if (!versionsMatch(Deprecated, OldDeprecated,
+                                    OverrideOrImpl)) {
             Which = 1;
             FirstVersion = Deprecated;
             SecondVersion = OldDeprecated;
@@ -2233,8 +2227,8 @@ AvailabilityAttr *Sema::mergeAvailabilityAttr(
           if (Which == -1) {
             Diag(OldAA->getLocation(),
                  diag::warn_mismatched_availability_override_unavail)
-              << AvailabilityAttr::getPrettyPlatformName(Platform->getName())
-              << (AMK == AMK_Override);
+                << AvailabilityAttr::getPrettyPlatformName(Platform->getName())
+                << (AMK == AMK_Override);
           } else if (Which != 1 && AMK == AMK_OptionalProtocolImplementation) {
             // Allow different 'introduced' / 'obsoleted' availability versions
             // on a method that implements an optional protocol requirement. It
@@ -2246,10 +2240,10 @@ AvailabilityAttr *Sema::mergeAvailabilityAttr(
           } else {
             Diag(OldAA->getLocation(),
                  diag::warn_mismatched_availability_override)
-              << Which
-              << AvailabilityAttr::getPrettyPlatformName(Platform->getName())
-              << FirstVersion.getAsString() << SecondVersion.getAsString()
-              << (AMK == AMK_Override);
+                << Which
+                << AvailabilityAttr::getPrettyPlatformName(Platform->getName())
+                << FirstVersion.getAsString() << SecondVersion.getAsString()
+                << (AMK == AMK_Override);
           }
           if (AMK == AMK_Override)
             Diag(CI.getLoc(), diag::note_overridden_method);
@@ -2291,10 +2285,8 @@ AvailabilityAttr *Sema::mergeAvailabilityAttr(
     }
   }
 
-  if (FoundAny &&
-      MergedIntroduced == Introduced &&
-      MergedDeprecated == Deprecated &&
-      MergedObsoleted == Obsoleted)
+  if (FoundAny && MergedIntroduced == Introduced &&
+      MergedDeprecated == Deprecated && MergedObsoleted == Obsoleted)
     return nullptr;
 
   // Only create a new attribute if !OverrideOrImpl, but we want to do
@@ -2326,7 +2318,7 @@ static void handleAvailabilityAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   IdentifierInfo *II = Platform->Ident;
   if (AvailabilityAttr::getPrettyPlatformName(II->getName()).empty())
     S.Diag(Platform->Loc, diag::warn_availability_unknown_platform)
-      << Platform->Ident;
+        << Platform->Ident;
 
   auto *ND = dyn_cast<NamedDecl>(D);
   if (!ND) // We warned about this already, so just return.
@@ -2646,8 +2638,8 @@ static void handleVisibilityAttr(Sema &S, Decl *D, const ParsedAttr &AL,
 
   VisibilityAttr::VisibilityType type;
   if (!VisibilityAttr::ConvertStrToVisibilityType(TypeStr, type)) {
-    S.Diag(LiteralLoc, diag::warn_attribute_type_not_supported) << AL
-                                                                << TypeStr;
+    S.Diag(LiteralLoc, diag::warn_attribute_type_not_supported)
+        << AL << TypeStr;
     return;
   }
 
@@ -2683,7 +2675,7 @@ static void handleSentinelAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
 
     if (Idx->isSigned() && Idx->isNegative()) {
       S.Diag(AL.getLoc(), diag::err_attribute_sentinel_less_than_zero)
-        << E->getSourceRange();
+          << E->getSourceRange();
       return;
     }
 
@@ -2705,7 +2697,7 @@ static void handleSentinelAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
       // FIXME: This error message could be improved, it would be nice
       // to say what the bounds actually are.
       S.Diag(AL.getLoc(), diag::err_attribute_sentinel_not_zero_or_one)
-        << E->getSourceRange();
+          << E->getSourceRange();
       return;
     }
   }
@@ -2818,7 +2810,7 @@ static void handleWeakImportAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   if (!D->canBeWeakImported(isDef)) {
     if (isDef)
       S.Diag(AL.getLoc(), diag::warn_attribute_invalid_on_definition)
-        << "weak_import";
+          << "weak_import";
     else if (isa<ObjCPropertyDecl>(D) || isa<ObjCMethodDecl>(D) ||
              (S.Context.getTargetInfo().getTriple().isOSDarwin() &&
               (isa<ObjCInterfaceDecl>(D) || isa<EnumDecl>(D)))) {
@@ -2850,9 +2842,9 @@ static void handleWorkGroupSize(Sema &S, Decl *D, const ParsedAttr &AL) {
   }
 
   WorkGroupAttr *Existing = D->getAttr<WorkGroupAttr>();
-  if (Existing && !(Existing->getXDim() == WGSize[0] &&
-                    Existing->getYDim() == WGSize[1] &&
-                    Existing->getZDim() == WGSize[2]))
+  if (Existing &&
+      !(Existing->getXDim() == WGSize[0] && Existing->getYDim() == WGSize[1] &&
+        Existing->getZDim() == WGSize[2]))
     S.Diag(AL.getLoc(), diag::warn_duplicate_attribute) << AL;
 
   D->addAttr(::new (S.Context)
@@ -2899,7 +2891,7 @@ SectionAttr *Sema::mergeSectionAttr(Decl *D, const AttributeCommonInfo &CI,
     if (ExistingAttr->getName() == Name)
       return nullptr;
     Diag(ExistingAttr->getLocation(), diag::warn_mismatched_section)
-         << 1 /*section*/;
+        << 1 /*section*/;
     Diag(CI.getLoc(), diag::note_previous_attribute);
     return nullptr;
   }
@@ -2990,7 +2982,7 @@ CodeSegAttr *Sema::mergeCodeSegAttr(Decl *D, const AttributeCommonInfo &CI,
     if (ExistingAttr->getName() == Name)
       return nullptr;
     Diag(ExistingAttr->getLocation(), diag::warn_mismatched_section)
-         << 0 /*codeseg*/;
+        << 0 /*codeseg*/;
     Diag(CI.getLoc(), diag::note_previous_attribute);
     return nullptr;
   }
@@ -3006,10 +2998,9 @@ static void handleCodeSegAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     return;
   if (const auto *ExistingAttr = D->getAttr<CodeSegAttr>()) {
     if (!ExistingAttr->isImplicit()) {
-      S.Diag(AL.getLoc(),
-             ExistingAttr->getName() == Str
-             ? diag::warn_duplicate_codeseg_attribute
-             : diag::err_conflicting_codeseg_attribute);
+      S.Diag(AL.getLoc(), ExistingAttr->getName() == Str
+                              ? diag::warn_duplicate_codeseg_attribute
+                              : diag::err_conflicting_codeseg_attribute);
       return;
     }
     D->dropAttr<CodeSegAttr>();
@@ -3420,8 +3411,8 @@ static void handleCleanupAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     FD = dyn_cast<FunctionDecl>(DRE->getDecl());
     NI = DRE->getNameInfo();
     if (!FD) {
-      S.Diag(Loc, diag::err_attribute_cleanup_arg_not_function) << 1
-        << NI.getName();
+      S.Diag(Loc, diag::err_attribute_cleanup_arg_not_function)
+          << 1 << NI.getName();
       return;
     }
   } else if (auto *ULE = dyn_cast<UnresolvedLookupExpr>(E)) {
@@ -3430,8 +3421,8 @@ static void handleCleanupAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     FD = S.ResolveSingleFunctionTemplateSpecialization(ULE, true);
     NI = ULE->getNameInfo();
     if (!FD) {
-      S.Diag(Loc, diag::err_attribute_cleanup_arg_not_function) << 2
-        << NI.getName();
+      S.Diag(Loc, diag::err_attribute_cleanup_arg_not_function)
+          << 2 << NI.getName();
       if (ULE->getType() == S.Context.OverloadTy)
         S.NoteAllOverloadCandidates(ULE);
       return;
@@ -3443,7 +3434,7 @@ static void handleCleanupAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
 
   if (FD->getNumParams() != 1) {
     S.Diag(Loc, diag::err_attribute_cleanup_func_must_take_one_arg)
-      << NI.getName();
+        << NI.getName();
     return;
   }
 
@@ -3451,10 +3442,10 @@ static void handleCleanupAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   // If this ever proves to be a problem it should be easy to fix.
   QualType Ty = S.Context.getPointerType(cast<VarDecl>(D)->getType());
   QualType ParamTy = FD->getParamDecl(0)->getType();
-  if (S.CheckAssignmentConstraints(FD->getParamDecl(0)->getLocation(),
-                                   ParamTy, Ty) != Sema::Compatible) {
+  if (S.CheckAssignmentConstraints(FD->getParamDecl(0)->getLocation(), ParamTy,
+                                   Ty) != Sema::Compatible) {
     S.Diag(Loc, diag::err_attribute_cleanup_func_arg_incompatible_type)
-      << NI.getName() << ParamTy << Ty;
+        << NI.getName() << ParamTy << Ty;
     return;
   }
   VarDecl *VD = cast<VarDecl>(D);
@@ -3652,8 +3643,7 @@ FormatAttr *Sema::mergeFormatAttr(Decl *D, const AttributeCommonInfo &CI,
                                   int FirstArg) {
   // Check whether we already have an equivalent format attribute.
   for (auto *F : D->specific_attrs<FormatAttr>()) {
-    if (F->getType() == Format &&
-        F->getFormatIdx() == FormatIdx &&
+    if (F->getType() == Format && F->getFormatIdx() == FormatIdx &&
         F->getFirstArg() == FirstArg) {
       // If we don't have a valid location for this attribute, adopt the
       // location.
@@ -3719,7 +3709,7 @@ static void handleFormatAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     if (ArgIdx == 0) {
       S.Diag(AL.getLoc(),
              diag::err_format_attribute_implicit_this_format_string)
-        << IdxExpr->getSourceRange();
+          << IdxExpr->getSourceRange();
       return;
     }
     ArgIdx--;
@@ -3732,7 +3722,8 @@ static void handleFormatAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
       (!Ty->isPointerType() ||
        !Ty->castAs<PointerType>()->getPointeeType()->isCharType())) {
     S.Diag(AL.getLoc(), diag::err_format_attribute_not)
-      << IdxExpr->getSourceRange() << getFunctionOrMethodParamRange(D, ArgIdx);
+        << IdxExpr->getSourceRange()
+        << getFunctionOrMethodParamRange(D, ArgIdx);
     return;
   }
 
@@ -4076,7 +4067,7 @@ static void handleTransparentUnionAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   }
 
   RecordDecl::field_iterator Field = RD->field_begin(),
-                          FieldEnd = RD->field_end();
+                             FieldEnd = RD->field_end();
   if (Field == FieldEnd) {
     S.Diag(AL.getLoc(), diag::warn_transparent_union_attribute_zero_fields);
     return;
@@ -4087,7 +4078,7 @@ static void handleTransparentUnionAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   if (FirstType->hasFloatingRepresentation() || FirstType->isVectorType()) {
     S.Diag(FirstField->getLocation(),
            diag::warn_transparent_union_attribute_floating)
-      << FirstType->isVectorType() << FirstType;
+        << FirstType->isVectorType() << FirstType;
     return;
   }
 
@@ -4150,7 +4141,7 @@ void Sema::AddAlignValueAttr(Decl *D, const AttributeCommonInfo &CI, Expr *E) {
   if (!T->isDependentType() && !T->isAnyPointerType() &&
       !T->isReferenceType() && !T->isMemberPointerType()) {
     Diag(AttrLoc, diag::warn_attribute_pointer_or_reference_only)
-      << &TmpAttr << T << D->getSourceRange();
+        << &TmpAttr << T << D->getSourceRange();
     return;
   }
 
@@ -4163,7 +4154,7 @@ void Sema::AddAlignValueAttr(Decl *D, const AttributeCommonInfo &CI, Expr *E) {
 
     if (!Alignment.isPowerOf2()) {
       Diag(AttrLoc, diag::err_alignment_not_power_of_two)
-        << E->getSourceRange();
+          << E->getSourceRange();
       return;
     }
 
@@ -4321,7 +4312,7 @@ void Sema::AddAlignedAttr(Decl *D, const AttributeCommonInfo &CI, Expr *E,
   if (!(TmpAttr.isAlignas() && !Alignment)) {
     if (!llvm::isPowerOf2_64(AlignVal)) {
       Diag(AttrLoc, diag::err_alignment_not_power_of_two)
-        << E->getSourceRange();
+          << E->getSourceRange();
       return;
     }
   }
@@ -4443,7 +4434,7 @@ void Sema::CheckAlignasUnderalignment(Decl *D) {
     CharUnits NaturalAlign = Context.getTypeAlignInChars(UnderlyingTy);
     if (NaturalAlign > RequestedAlign)
       Diag(AlignasAttr->getLocation(), diag::err_alignas_underaligned)
-        << DiagTy << (unsigned)NaturalAlign.getQuantity();
+          << DiagTy << (unsigned)NaturalAlign.getQuantity();
   }
 }
 
@@ -4937,7 +4928,8 @@ static void handleGNUInlineAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
 }
 
 static void handleCallConvAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
-  if (hasDeclarator(D)) return;
+  if (hasDeclarator(D))
+    return;
 
   // Diagnostic is emitted elsewhere: here we store the (valid) AL
   // in the Decl node for syntactic reasoning, e.g., pretty-printing.
@@ -5145,7 +5137,7 @@ bool Sema::CheckCallingConvAttr(const ParsedAttr &Attrs, CallingConv &CC,
     return true;
 
   if (Attrs.hasProcessingCache()) {
-    CC = (CallingConv) Attrs.getProcessingCache();
+    CC = (CallingConv)Attrs.getProcessingCache();
     return false;
   }
 
@@ -5194,12 +5186,11 @@ bool Sema::CheckCallingConvAttr(const ParsedAttr &Attrs, CallingConv &CC,
     CC = CC_X86RegCall;
     break;
   case ParsedAttr::AT_MSABI:
-    CC = Context.getTargetInfo().getTriple().isOSWindows() ? CC_C :
-                                                             CC_Win64;
+    CC = Context.getTargetInfo().getTriple().isOSWindows() ? CC_C : CC_Win64;
     break;
   case ParsedAttr::AT_SysVABI:
-    CC = Context.getTargetInfo().getTriple().isOSWindows() ? CC_X86_64SysV :
-                                                             CC_C;
+    CC = Context.getTargetInfo().getTriple().isOSWindows() ? CC_X86_64SysV
+                                                           : CC_C;
     break;
   case ParsedAttr::AT_Pcs: {
     StringRef StrRef;
@@ -5237,7 +5228,8 @@ bool Sema::CheckCallingConvAttr(const ParsedAttr &Attrs, CallingConv &CC,
   case ParsedAttr::AT_RISCVVectorCC:
     CC = CC_RISCVVectorCall;
     break;
-  default: llvm_unreachable("unexpected attribute kind");
+  default:
+    llvm_unreachable("unexpected attribute kind");
   }
 
   TargetInfo::CallingConvCheckResult A = TargetInfo::CCCR_OK;
@@ -5309,7 +5301,7 @@ bool Sema::CheckCallingConvAttr(const ParsedAttr &Attrs, CallingConv &CC,
   }
   }
 
-  Attrs.setProcessingCache((unsigned) CC);
+  Attrs.setProcessingCache((unsigned)CC);
   return false;
 }
 
@@ -5331,7 +5323,7 @@ bool Sema::CheckRegparmAttr(const ParsedAttr &AL, unsigned &numParams) {
 
   if (Context.getTargetInfo().getRegParmMax() == 0) {
     Diag(AL.getLoc(), diag::err_attribute_regparm_wrong_platform)
-      << NumParamsExpr->getSourceRange();
+        << NumParamsExpr->getSourceRange();
     AL.setInvalid();
     return true;
   }
@@ -5339,7 +5331,8 @@ bool Sema::CheckRegparmAttr(const ParsedAttr &AL, unsigned &numParams) {
   numParams = NP;
   if (numParams > Context.getTargetInfo().getRegParmMax()) {
     Diag(AL.getLoc(), diag::err_attribute_regparm_invalid_number)
-      << Context.getTargetInfo().getRegParmMax() << NumParamsExpr->getSourceRange();
+        << Context.getTargetInfo().getRegParmMax()
+        << NumParamsExpr->getSourceRange();
     AL.setInvalid();
     return true;
   }
@@ -5892,10 +5885,10 @@ static void handleDLLAttr(Sema &S, Decl *D, const ParsedAttr &A) {
     D->addAttr(NewAttr);
 }
 
-MSInheritanceAttr *
-Sema::mergeMSInheritanceAttr(Decl *D, const AttributeCommonInfo &CI,
-                             bool BestCase,
-                             MSInheritanceModel Model) {
+MSInheritanceAttr *Sema::mergeMSInheritanceAttr(Decl *D,
+                                                const AttributeCommonInfo &CI,
+                                                bool BestCase,
+                                                MSInheritanceModel Model) {
   if (MSInheritanceAttr *IA = D->getAttr<MSInheritanceAttr>()) {
     if (IA->getInheritanceModel() == Model)
       return nullptr;
@@ -5946,7 +5939,7 @@ static void handleCapabilityAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
 }
 
 static void handleAssertCapabilityAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
-  SmallVector<Expr*, 1> Args;
+  SmallVector<Expr *, 1> Args;
   if (!checkLockFunAttrCommon(S, D, AL, Args))
     return;
 
@@ -5960,7 +5953,7 @@ static void handleAcquireCapabilityAttr(Sema &S, Decl *D,
       ParmDecl && !checkFunParamsAreScopedLockable(S, ParmDecl, AL))
     return;
 
-  SmallVector<Expr*, 1> Args;
+  SmallVector<Expr *, 1> Args;
   if (!checkLockFunAttrCommon(S, D, AL, Args))
     return;
 
@@ -5970,7 +5963,7 @@ static void handleAcquireCapabilityAttr(Sema &S, Decl *D,
 
 static void handleTryAcquireCapabilityAttr(Sema &S, Decl *D,
                                            const ParsedAttr &AL) {
-  SmallVector<Expr*, 2> Args;
+  SmallVector<Expr *, 2> Args;
   if (!checkTryLockFunAttrCommon(S, D, AL, Args))
     return;
 
@@ -6001,7 +5994,7 @@ static void handleRequiresCapabilityAttr(Sema &S, Decl *D,
     return;
 
   // check that all arguments are lockable objects
-  SmallVector<Expr*, 1> Args;
+  SmallVector<Expr *, 1> Args;
   checkAttrArgsAreCapabilityObjs(S, D, AL, Args);
   if (Args.empty())
     return;
@@ -6075,7 +6068,8 @@ static void handleNoSanitizeAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
             SanitizerMask() &&
         SanitizerName != "coverage")
       S.Diag(LiteralLoc, diag::warn_unknown_sanitizer_ignored) << SanitizerName;
-    else if (isGlobalVar(D) && !isSanitizerAttributeAllowedOnGlobals(SanitizerName))
+    else if (isGlobalVar(D) &&
+             !isSanitizerAttributeAllowedOnGlobals(SanitizerName))
       S.Diag(D->getLocation(), diag::warn_attribute_type_not_supported_global)
           << AL << SanitizerName;
     Sanitizers.push_back(SanitizerName);
@@ -6290,7 +6284,7 @@ static void handleAcquireHandleAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   D->addAttr(AcquireHandleAttr::Create(S.Context, Argument, AL));
 }
 
-template<typename Attr>
+template <typename Attr>
 static void handleHandleAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   StringRef Argument;
   if (!S.checkStringLiteralArgumentAttr(AL, 0, Argument))
@@ -6298,7 +6292,7 @@ static void handleHandleAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   D->addAttr(Attr::Create(S.Context, Argument, AL));
 }
 
-template<typename Attr>
+template <typename Attr>
 static void handleUnsafeBufferUsage(Sema &S, Decl *D, const ParsedAttr &AL) {
   D->addAttr(Attr::Create(S.Context, AL));
 }
@@ -6322,14 +6316,11 @@ static void handleCFGuardAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   D->addAttr(::new (S.Context) CFGuardAttr(S.Context, AL, Arg));
 }
 
-
 template <typename AttrTy>
 static const AttrTy *findEnforceTCBAttrByName(Decl *D, StringRef Name) {
   auto Attrs = D->specific_attrs<AttrTy>();
-  auto I = llvm::find_if(Attrs,
-                         [Name](const AttrTy *A) {
-                           return A->getTCBName() == Name;
-                         });
+  auto I = llvm::find_if(
+      Attrs, [Name](const AttrTy *A) { return A->getTCBName() == Name; });
   return I == Attrs.end() ? nullptr : *I;
 }
 
@@ -6341,12 +6332,12 @@ static void handleEnforceTCBAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
 
   // A function cannot be have both regular and leaf membership in the same TCB.
   if (const ConflictingAttrTy *ConflictingAttr =
-      findEnforceTCBAttrByName<ConflictingAttrTy>(D, Argument)) {
+          findEnforceTCBAttrByName<ConflictingAttrTy>(D, Argument)) {
     // We could attach a note to the other attribute but in this case
     // there's no need given how the two are very close to each other.
     S.Diag(AL.getLoc(), diag::err_tcb_conflicting_attributes)
-      << AL.getAttrName()->getName() << ConflictingAttr->getAttrName()->getName()
-      << Argument;
+        << AL.getAttrName()->getName()
+        << ConflictingAttr->getAttrName()->getName() << Argument;
 
     // Error recovery: drop the non-leaf attribute so that to suppress
     // all future warnings caused by erroneous attributes. The leaf attribute
@@ -6363,10 +6354,10 @@ static AttrTy *mergeEnforceTCBAttrImpl(Sema &S, Decl *D, const AttrTy &AL) {
   // Check if the new redeclaration has different leaf-ness in the same TCB.
   StringRef TCBName = AL.getTCBName();
   if (const ConflictingAttrTy *ConflictingAttr =
-      findEnforceTCBAttrByName<ConflictingAttrTy>(D, TCBName)) {
+          findEnforceTCBAttrByName<ConflictingAttrTy>(D, TCBName)) {
     S.Diag(ConflictingAttr->getLoc(), diag::err_tcb_conflicting_attributes)
-      << ConflictingAttr->getAttrName()->getName()
-      << AL.getAttrName()->getName() << TCBName;
+        << ConflictingAttr->getAttrName()->getName()
+        << AL.getAttrName()->getName() << TCBName;
 
     // Add a note so that the user could easily find the conflicting attribute.
     S.Diag(AL.getLoc(), diag::note_conflicting_attribute);
@@ -6377,18 +6368,51 @@ static AttrTy *mergeEnforceTCBAttrImpl(Sema &S, Decl *D, const AttrTy &AL) {
   }
 
   ASTContext &Context = S.getASTContext();
-  return ::new(Context) AttrTy(Context, AL, AL.getTCBName());
+  return ::new (Context) AttrTy(Context, AL, AL.getTCBName());
 }
 
 EnforceTCBAttr *Sema::mergeEnforceTCBAttr(Decl *D, const EnforceTCBAttr &AL) {
-  return mergeEnforceTCBAttrImpl<EnforceTCBAttr, EnforceTCBLeafAttr>(
-      *this, D, AL);
+  return mergeEnforceTCBAttrImpl<EnforceTCBAttr, EnforceTCBLeafAttr>(*this, D,
+                                                                     AL);
+}
+
+EnforceTCBLeafAttr *
+Sema::mergeEnforceTCBLeafAttr(Decl *D, const EnforceTCBLeafAttr &AL) {
+  return mergeEnforceTCBAttrImpl<EnforceTCBLeafAttr, EnforceTCBAttr>(*this, D,
+                                                                     AL);
+}
+
+/// Handle Silica main 'reflect' attribute.
+static void handleSilicaReflect(Sema &S, Decl *D, const ParsedAttr &AL) {
+  SmallVector<SilicaReflectAttr::Option, 4> Options;
+
+  for (unsigned I = 0, E = AL.getNumArgs(); I != E; ++I) {
+    StringRef OptStr;
+    if (!S.checkStringLiteralArgumentAttr(AL, I, OptStr))
+      continue;
+
+    SilicaReflectAttr::Option Option;
+    if (!SilicaReflectAttr::ConvertStrToOption(OptStr, Option)) {
+      S.Diag(AL.getLoc(), diag::warn_attribute_type_not_supported)
+          << AL << "'" + std::string(OptStr) + "'";
+      continue;
+    }
+
+    Options.push_back(Option);
+  }
+
+  D->addAttr(::new (S.Context)
+                 SilicaReflectAttr(S.Context, AL, Options.data(), Options.size()));
 }
 
-EnforceTCBLeafAttr *Sema::mergeEnforceTCBLeafAttr(
-    Decl *D, const EnforceTCBLeafAttr &AL) {
-  return mergeEnforceTCBAttrImpl<EnforceTCBLeafAttr, EnforceTCBAttr>(
-      *this, D, AL);
+/// Handle Silica 'alias' attribute.
+static void handleSilicaAlias(Sema &S, Decl *D, const ParsedAttr &AL) {
+  StringRef Name;
+
+  if (!S.checkStringLiteralArgumentAttr(AL, 0, Name))
+    return;
+
+  D->addAttr(::new (S.Context) SilicaAliasAttr(S.Context, AL, Name));
 }
 
 static void handleVTablePointerAuthentication(Sema &S, Decl *D,
@@ -6585,7 +6609,8 @@ ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D, const ParsedAttr &AL,
 
   switch (AL.getKind()) {
   default:
-    if (AL.getInfo().handleDeclAttribute(S, D, AL) != ParsedAttrInfo::NotHandled)
+    if (AL.getInfo().handleDeclAttribute(S, D, AL) !=
+        ParsedAttrInfo::NotHandled)
       break;
     if (!AL.isStmtAttr()) {
       assert(AL.isTypeAttr() && "Non-type attribute not handled");
@@ -6762,13 +6787,13 @@ ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D, const ParsedAttr &AL,
     handlePassObjectSizeAttr(S, D, AL);
     break;
   case ParsedAttr::AT_Constructor:
-      handleConstructorAttr(S, D, AL);
+    handleConstructorAttr(S, D, AL);
     break;
   case ParsedAttr::AT_Deprecated:
     handleDeprecatedAttr(S, D, AL);
     break;
   case ParsedAttr::AT_Destructor:
-      handleDestructorAttr(S, D, AL);
+    handleDestructorAttr(S, D, AL);
     break;
   case ParsedAttr::AT_EnableIf:
     handleEnableIfAttr(S, D, AL);
@@ -6972,7 +6997,7 @@ ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D, const ParsedAttr &AL,
     handleVecTypeHint(S, D, AL);
     break;
   case ParsedAttr::AT_InitPriority:
-      handleInitPriorityAttr(S, D, AL);
+    handleInitPriorityAttr(S, D, AL);
     break;
   case ParsedAttr::AT_Packed:
     handlePackedAttr(S, D, AL);
@@ -7410,6 +7435,18 @@ ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D, const ParsedAttr &AL,
   case ParsedAttr::AT_VTablePointerAuthentication:
     handleVTablePointerAuthentication(S, D, AL);
     break;
+
+  case ParsedAttr::AT_SilicaReflect:
+    handleSilicaReflect(S, D, AL);
+    break;
+
+  case ParsedAttr::AT_SilicaAlias:
+    handleSilicaAlias(S, D, AL);
+    break;
+
+  case ParsedAttr::AT_SilicaIgnore:
+    handleSimpleAttribute<SilicaIgnoreAttr>(S, D, AL);
+    break;
   }
 }
 
@@ -7538,8 +7575,8 @@ static void checkUnusedDeclAttributes(Sema &S, const ParsedAttributesView &A) {
       S.Diag(AL.getLoc(), diag::warn_unknown_attribute_ignored)
           << AL << AL.getRange();
     } else {
-      S.Diag(AL.getLoc(), diag::warn_attribute_not_on_decl) << AL
-                                                            << AL.getRange();
+      S.Diag(AL.getLoc(), diag::warn_attribute_not_on_decl)
+          << AL << AL.getRange();
     }
   }
 }
@@ -7577,7 +7614,7 @@ NamedDecl *Sema::DeclClonePragmaWeak(NamedDecl *ND, const IdentifierInfo *II,
     // a typedef.
     QualType FDTy = FD->getType();
     if (const auto *FT = FDTy->getAs<FunctionProtoType>()) {
-      SmallVector<ParmVarDecl*, 16> Params;
+      SmallVector<ParmVarDecl *, 16> Params;
       for (const auto &AI : FT->param_types()) {
         ParmVarDecl *Param = BuildParmVarDeclForTypedef(NewFD, Loc, AI);
         Param->setScopeInfo(0, Params.size());
@@ -7713,8 +7750,7 @@ static bool isForbiddenTypeAllowed(Sema &S, Decl *D,
   // Private ivars are always okay.  Unfortunately, people don't
   // always properly make their ivars private, even in system headers.
   // Plus we need to make fields okay, too.
-  if (!isa<FieldDecl>(D) && !isa<ObjCPropertyDecl>(D) &&
-      !isa<FunctionDecl>(D))
+  if (!isa<FieldDecl>(D) && !isa<ObjCPropertyDecl>(D) && !isa<FunctionDecl>(D))
     return false;
 
   // Silently accept unsupported uses of __weak in both user and system
@@ -7767,7 +7803,6 @@ static void handleDelayedForbiddenType(Sema &S, DelayedDiagnostic &DD,
   DD.Triggered = true;
 }
 
-
 void Sema::PopParsingDeclaration(ParsingDeclState state, Decl *decl) {
   assert(DelayedDiagnostics.getCurrentPool());
   DelayedDiagnosticPool &poppedPool = *DelayedDiagnostics.getCurrentPool();
@@ -7776,7 +7811,8 @@ void Sema::PopParsingDeclaration(ParsingDeclState state, Decl *decl) {
   // When delaying diagnostics to run in the context of a parsed
   // declaration, we only want to actually emit anything if parsing
   // succeeds.
-  if (!decl) return;
+  if (!decl)
+    return;
 
   // We emit all the active diagnostics in this pool or any of its
   // parents.  In general, we'll get one pool for the decl spec
@@ -7788,10 +7824,11 @@ void Sema::PopParsingDeclaration(ParsingDeclState state, Decl *decl) {
   const DelayedDiagnosticPool *pool = &poppedPool;
   do {
     bool AnyAccessFailures = false;
-    for (DelayedDiagnosticPool::pool_iterator
-           i = pool->pool_begin(), e = pool->pool_end(); i != e; ++i) {
+    for (DelayedDiagnosticPool::pool_iterator i = pool->pool_begin(),
+                                              e = pool->pool_end();
+         i != e; ++i) {
       // This const_cast is a bit lame.  Really, Triggered should be mutable.
-      DelayedDiagnostic &diag = const_cast<DelayedDiagnostic&>(*i);
+      DelayedDiagnostic &diag = const_cast<DelayedDiagnostic &>(*i);
       if (diag.Triggered)
         continue;
 
diff --git a/clang/lib/Support/CMakeLists.txt b/clang/lib/Support/CMakeLists.txt
index de06271e9..3d0a7c861 100644
--- a/clang/lib/Support/CMakeLists.txt
+++ b/clang/lib/Support/CMakeLists.txt
@@ -15,9 +15,7 @@ set(clangSupport_sources
 
 add_clang_library(clangSupport ${clangSupport_sources})
 
-if (TARGET obj.clangSupport)
-  add_library(clangSupport_tablegen ALIAS obj.clangSupport)
-elseif (NOT LLVM_LINK_LLVM_DYLIB)
+if (NOT LLVM_LINK_LLVM_DYLIB)
   add_library(clangSupport_tablegen ALIAS clangSupport)
 else()
   # Build a version of the support library that does not link against
diff --git a/clang/tools/CMakeLists.txt b/clang/tools/CMakeLists.txt
index 98c018e96..917a47588 100644
--- a/clang/tools/CMakeLists.txt
+++ b/clang/tools/CMakeLists.txt
@@ -1,45 +1,5 @@
 create_subdirectory_options(CLANG TOOL)
 
-add_clang_subdirectory(diagtool)
-add_clang_subdirectory(driver)
-add_clang_subdirectory(apinotes-test)
-add_clang_subdirectory(clang-diff)
-add_clang_subdirectory(clang-format)
-add_clang_subdirectory(clang-fuzzer)
-add_clang_subdirectory(clang-import-test)
-add_clang_subdirectory(clang-linker-wrapper)
-add_clang_subdirectory(clang-nvlink-wrapper)
-add_clang_subdirectory(clang-offload-packager)
-add_clang_subdirectory(clang-offload-bundler)
-add_clang_subdirectory(clang-scan-deps)
-add_clang_subdirectory(clang-sycl-linker)
-add_clang_subdirectory(clang-installapi)
-if(HAVE_CLANG_REPL_SUPPORT)
-  add_clang_subdirectory(clang-repl)
-endif()
-
-add_clang_subdirectory(c-index-test)
-
-add_clang_subdirectory(clang-refactor)
-# For MinGW we only enable shared library if LLVM_LINK_LLVM_DYLIB=ON.
-# Without that option resulting library is too close to 2^16 DLL exports limit.
-if(UNIX OR (MSVC AND LLVM_BUILD_LLVM_DYLIB_VIS) OR (MINGW AND LLVM_LINK_LLVM_DYLIB))
-  add_clang_subdirectory(clang-shlib)
-endif()
-
-if(CLANG_ENABLE_ARCMT)
-  add_clang_subdirectory(arcmt-test)
-  add_clang_subdirectory(c-arcmt-test)
-endif()
-
-if(CLANG_ENABLE_STATIC_ANALYZER)
-  add_clang_subdirectory(clang-check)
-  add_clang_subdirectory(clang-extdef-mapping)
-  add_clang_subdirectory(scan-build)
-  add_clang_subdirectory(scan-build-py)
-  add_clang_subdirectory(scan-view)
-endif()
-
 # We support checking out the clang-tools-extra repository into the 'extra'
 # subdirectory. It contains tools developed as part of the Clang/LLVM project
 # on top of the Clang tooling platform. We keep them in a separate repository
diff --git a/clang/tools/libclang/CMakeLists.txt b/clang/tools/libclang/CMakeLists.txt
index 00a1223c0..b7a02410f 100644
--- a/clang/tools/libclang/CMakeLists.txt
+++ b/clang/tools/libclang/CMakeLists.txt
@@ -135,7 +135,7 @@ if (UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES "AIX")
     remove_definitions("-D_XOPEN_SOURCE=700")
 endif()
 
-add_clang_library(libclang ${ENABLE_SHARED} ${ENABLE_STATIC} INSTALL_WITH_TOOLCHAIN
+add_clang_library(libclang ${ENABLE_STATIC} INSTALL_WITH_TOOLCHAIN
   OUTPUT_NAME ${output_name}
   ${SOURCES}
 
@@ -154,7 +154,7 @@ add_clang_library(libclang ${ENABLE_SHARED} ${ENABLE_STATIC} INSTALL_WITH_TOOLCH
   )
 
 if(ENABLE_STATIC)
-  foreach(name libclang obj.libclang libclang_static)
+  foreach(name libclang libclang_static)
     if (TARGET ${name})
       target_compile_definitions(${name} PUBLIC CINDEX_NO_EXPORTS)
     endif()
diff --git a/llvm/CMakeLists.txt b/llvm/CMakeLists.txt
index f5293e866..f54cb5db4 100644
--- a/llvm/CMakeLists.txt
+++ b/llvm/CMakeLists.txt
@@ -512,7 +512,7 @@ set(LLVM_ALL_EXPERIMENTAL_TARGETS
 # List of targets with JIT support:
 set(LLVM_TARGETS_WITH_JIT X86 PowerPC AArch64 ARM Mips SystemZ)
 
-set(LLVM_TARGETS_TO_BUILD "all"
+set(LLVM_TARGETS_TO_BUILD ""
     CACHE STRING "Semicolon-separated list of targets to build, or \"all\".")
 
 set(LLVM_EXPERIMENTAL_TARGETS_TO_BUILD ""
diff --git a/llvm/cmake/modules/AddLLVM.cmake b/llvm/cmake/modules/AddLLVM.cmake
index d3e9377c8..5c2b1ef8b 100644
--- a/llvm/cmake/modules/AddLLVM.cmake
+++ b/llvm/cmake/modules/AddLLVM.cmake
@@ -850,35 +850,6 @@ function(add_llvm_install_targets target)
       list(APPEND file_dependencies ${dependency})
     endif()
   endforeach()
-
-  get_subproject_title(subproject_title)
-
-  add_custom_target(${target}
-                    DEPENDS ${file_dependencies}
-                    COMMAND "${CMAKE_COMMAND}"
-                            ${component_option}
-                            ${prefix_option}
-                            -P "${CMAKE_BINARY_DIR}/cmake_install.cmake"
-                    USES_TERMINAL)
-  set_target_properties(${target} PROPERTIES FOLDER "${subproject_title}/Installation")
-  add_custom_target(${target}-stripped
-                    DEPENDS ${file_dependencies}
-                    COMMAND "${CMAKE_COMMAND}"
-                            ${component_option}
-                            ${prefix_option}
-                            -DCMAKE_INSTALL_DO_STRIP=1
-                            -P "${CMAKE_BINARY_DIR}/cmake_install.cmake"
-                    USES_TERMINAL)
-  set_target_properties(${target}-stripped PROPERTIES FOLDER "${subproject_title}/Installation")
-  if(target_dependencies)
-    add_dependencies(${target} ${target_dependencies})
-    add_dependencies(${target}-stripped ${target_dependencies})
-  endif()
-
-  if(ARG_SYMLINK)
-    add_dependencies(${target} install-${ARG_SYMLINK})
-    add_dependencies(${target}-stripped install-${ARG_SYMLINK}-stripped)
-  endif()
 endfunction()
 
 # Define special targets that behave like a component group. They don't have any
diff --git a/llvm/include/llvm/CodeGenTypes/MachineValueType.h b/llvm/include/llvm/CodeGenTypes/MachineValueType.h
index c14abca02..87be980f8 100644
--- a/llvm/include/llvm/CodeGenTypes/MachineValueType.h
+++ b/llvm/include/llvm/CodeGenTypes/MachineValueType.h
@@ -25,563 +25,560 @@
 
 namespace llvm {
 
-  class Type;
-  struct fltSemantics;
-  class raw_ostream;
-
-  /// Machine Value Type. Every type that is supported natively by some
-  /// processor targeted by LLVM occurs here. This means that any legal value
-  /// type can be represented by an MVT.
-  class MVT {
-  public:
-    enum SimpleValueType : uint16_t {
-      // Simple value types that aren't explicitly part of this enumeration
-      // are considered extended value types.
-      INVALID_SIMPLE_VALUE_TYPE = 0,
-
-#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    Ty = n,
+class Type;
+struct fltSemantics;
+class raw_ostream;
+
+/// Machine Value Type. Every type that is supported natively by some
+/// processor targeted by LLVM occurs here. This means that any legal value
+/// type can be represented by an MVT.
+class MVT {
+public:
+  enum SimpleValueType : uint16_t {
+    // Simple value types that aren't explicitly part of this enumeration
+    // are considered extended value types.
+    INVALID_SIMPLE_VALUE_TYPE = 0,
+
+#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
+  Ty = n,
 #define GET_VT_RANGES
-#include "llvm/CodeGen/GenVT.inc"
+#include "GenVT.inc"
 #undef GET_VT_ATTR
 #undef GET_VT_RANGES
 
-      VALUETYPE_SIZE = LAST_VALUETYPE + 1,
-    };
+    VALUETYPE_SIZE = LAST_VALUETYPE + 1,
+  };
 
-    static_assert(FIRST_VALUETYPE > 0);
-    static_assert(LAST_VALUETYPE < token);
+  static_assert(FIRST_VALUETYPE > 0);
+  static_assert(LAST_VALUETYPE < token);
 
-    SimpleValueType SimpleTy = INVALID_SIMPLE_VALUE_TYPE;
+  SimpleValueType SimpleTy = INVALID_SIMPLE_VALUE_TYPE;
 
-    constexpr MVT() = default;
-    constexpr MVT(SimpleValueType SVT) : SimpleTy(SVT) {}
+  constexpr MVT() = default;
+  constexpr MVT(SimpleValueType SVT) : SimpleTy(SVT) {}
 
-    bool operator>(const MVT& S)  const { return SimpleTy >  S.SimpleTy; }
-    bool operator<(const MVT& S)  const { return SimpleTy <  S.SimpleTy; }
-    bool operator==(const MVT& S) const { return SimpleTy == S.SimpleTy; }
-    bool operator!=(const MVT& S) const { return SimpleTy != S.SimpleTy; }
-    bool operator>=(const MVT& S) const { return SimpleTy >= S.SimpleTy; }
-    bool operator<=(const MVT& S) const { return SimpleTy <= S.SimpleTy; }
+  bool operator>(const MVT &S) const { return SimpleTy > S.SimpleTy; }
+  bool operator<(const MVT &S) const { return SimpleTy < S.SimpleTy; }
+  bool operator==(const MVT &S) const { return SimpleTy == S.SimpleTy; }
+  bool operator!=(const MVT &S) const { return SimpleTy != S.SimpleTy; }
+  bool operator>=(const MVT &S) const { return SimpleTy >= S.SimpleTy; }
+  bool operator<=(const MVT &S) const { return SimpleTy <= S.SimpleTy; }
 
-    /// Support for debugging, callable in GDB: VT.dump()
-    void dump() const;
+  /// Support for debugging, callable in GDB: VT.dump()
+  void dump() const;
 
-    /// Implement operator<<.
-    void print(raw_ostream &OS) const;
+  /// Implement operator<<.
+  void print(raw_ostream &OS) const;
 
-    /// Return true if this is a valid simple valuetype.
-    bool isValid() const {
-      return (SimpleTy >= MVT::FIRST_VALUETYPE &&
-              SimpleTy <= MVT::LAST_VALUETYPE);
-    }
+  /// Return true if this is a valid simple valuetype.
+  bool isValid() const {
+    return (SimpleTy >= MVT::FIRST_VALUETYPE &&
+            SimpleTy <= MVT::LAST_VALUETYPE);
+  }
 
-    /// Return true if this is a FP or a vector FP type.
-    bool isFloatingPoint() const {
-      return ((SimpleTy >= MVT::FIRST_FP_VALUETYPE &&
-               SimpleTy <= MVT::LAST_FP_VALUETYPE) ||
-              (SimpleTy >= MVT::FIRST_FP_FIXEDLEN_VECTOR_VALUETYPE &&
-               SimpleTy <= MVT::LAST_FP_FIXEDLEN_VECTOR_VALUETYPE) ||
-              (SimpleTy >= MVT::FIRST_FP_SCALABLE_VECTOR_VALUETYPE &&
-               SimpleTy <= MVT::LAST_FP_SCALABLE_VECTOR_VALUETYPE));
-    }
+  /// Return true if this is a FP or a vector FP type.
+  bool isFloatingPoint() const {
+    return ((SimpleTy >= MVT::FIRST_FP_VALUETYPE &&
+             SimpleTy <= MVT::LAST_FP_VALUETYPE) ||
+            (SimpleTy >= MVT::FIRST_FP_FIXEDLEN_VECTOR_VALUETYPE &&
+             SimpleTy <= MVT::LAST_FP_FIXEDLEN_VECTOR_VALUETYPE) ||
+            (SimpleTy >= MVT::FIRST_FP_SCALABLE_VECTOR_VALUETYPE &&
+             SimpleTy <= MVT::LAST_FP_SCALABLE_VECTOR_VALUETYPE));
+  }
 
-    /// Return true if this is an integer or a vector integer type.
-    bool isInteger() const {
-      return ((SimpleTy >= MVT::FIRST_INTEGER_VALUETYPE &&
-               SimpleTy <= MVT::LAST_INTEGER_VALUETYPE) ||
-              (SimpleTy >= MVT::FIRST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE &&
-               SimpleTy <= MVT::LAST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE) ||
-              (SimpleTy >= MVT::FIRST_INTEGER_SCALABLE_VECTOR_VALUETYPE &&
-               SimpleTy <= MVT::LAST_INTEGER_SCALABLE_VECTOR_VALUETYPE));
-    }
+  /// Return true if this is an integer or a vector integer type.
+  bool isInteger() const {
+    return ((SimpleTy >= MVT::FIRST_INTEGER_VALUETYPE &&
+             SimpleTy <= MVT::LAST_INTEGER_VALUETYPE) ||
+            (SimpleTy >= MVT::FIRST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE &&
+             SimpleTy <= MVT::LAST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE) ||
+            (SimpleTy >= MVT::FIRST_INTEGER_SCALABLE_VECTOR_VALUETYPE &&
+             SimpleTy <= MVT::LAST_INTEGER_SCALABLE_VECTOR_VALUETYPE));
+  }
 
-    /// Return true if this is an integer, not including vectors.
-    bool isScalarInteger() const {
-      return (SimpleTy >= MVT::FIRST_INTEGER_VALUETYPE &&
-              SimpleTy <= MVT::LAST_INTEGER_VALUETYPE);
-    }
+  /// Return true if this is an integer, not including vectors.
+  bool isScalarInteger() const {
+    return (SimpleTy >= MVT::FIRST_INTEGER_VALUETYPE &&
+            SimpleTy <= MVT::LAST_INTEGER_VALUETYPE);
+  }
 
-    /// Return true if this is a vector value type.
-    bool isVector() const {
-      return (SimpleTy >= MVT::FIRST_VECTOR_VALUETYPE &&
-              SimpleTy <= MVT::LAST_VECTOR_VALUETYPE);
-    }
+  /// Return true if this is a vector value type.
+  bool isVector() const {
+    return (SimpleTy >= MVT::FIRST_VECTOR_VALUETYPE &&
+            SimpleTy <= MVT::LAST_VECTOR_VALUETYPE);
+  }
 
-    /// Return true if this is a vector value type where the
-    /// runtime length is machine dependent
-    bool isScalableVector() const {
-      return (SimpleTy >= MVT::FIRST_SCALABLE_VECTOR_VALUETYPE &&
-              SimpleTy <= MVT::LAST_SCALABLE_VECTOR_VALUETYPE);
-    }
+  /// Return true if this is a vector value type where the
+  /// runtime length is machine dependent
+  bool isScalableVector() const {
+    return (SimpleTy >= MVT::FIRST_SCALABLE_VECTOR_VALUETYPE &&
+            SimpleTy <= MVT::LAST_SCALABLE_VECTOR_VALUETYPE);
+  }
 
-    /// Return true if this is a RISCV vector tuple type where the
-    /// runtime length is machine dependent
-    bool isRISCVVectorTuple() const {
-      return (SimpleTy >= MVT::FIRST_RISCV_VECTOR_TUPLE_VALUETYPE &&
-              SimpleTy <= MVT::LAST_RISCV_VECTOR_TUPLE_VALUETYPE);
-    }
+  /// Return true if this is a RISCV vector tuple type where the
+  /// runtime length is machine dependent
+  bool isRISCVVectorTuple() const {
+    return (SimpleTy >= MVT::FIRST_RISCV_VECTOR_TUPLE_VALUETYPE &&
+            SimpleTy <= MVT::LAST_RISCV_VECTOR_TUPLE_VALUETYPE);
+  }
 
-    /// Return true if this is a custom target type that has a scalable size.
-    bool isScalableTargetExtVT() const {
-      return SimpleTy == MVT::aarch64svcount || isRISCVVectorTuple();
-    }
+  /// Return true if this is a custom target type that has a scalable size.
+  bool isScalableTargetExtVT() const {
+    return SimpleTy == MVT::aarch64svcount || isRISCVVectorTuple();
+  }
 
-    /// Return true if the type is a scalable type.
-    bool isScalableVT() const {
-      return isScalableVector() || isScalableTargetExtVT();
-    }
+  /// Return true if the type is a scalable type.
+  bool isScalableVT() const {
+    return isScalableVector() || isScalableTargetExtVT();
+  }
 
-    bool isFixedLengthVector() const {
-      return (SimpleTy >= MVT::FIRST_FIXEDLEN_VECTOR_VALUETYPE &&
-              SimpleTy <= MVT::LAST_FIXEDLEN_VECTOR_VALUETYPE);
-    }
+  bool isFixedLengthVector() const {
+    return (SimpleTy >= MVT::FIRST_FIXEDLEN_VECTOR_VALUETYPE &&
+            SimpleTy <= MVT::LAST_FIXEDLEN_VECTOR_VALUETYPE);
+  }
 
-    /// Return true if this is a 16-bit vector type.
-    bool is16BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 16);
-    }
+  /// Return true if this is a 16-bit vector type.
+  bool is16BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 16);
+  }
 
-    /// Return true if this is a 32-bit vector type.
-    bool is32BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 32);
-    }
+  /// Return true if this is a 32-bit vector type.
+  bool is32BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 32);
+  }
 
-    /// Return true if this is a 64-bit vector type.
-    bool is64BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 64);
-    }
+  /// Return true if this is a 64-bit vector type.
+  bool is64BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 64);
+  }
 
-    /// Return true if this is a 128-bit vector type.
-    bool is128BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 128);
-    }
+  /// Return true if this is a 128-bit vector type.
+  bool is128BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 128);
+  }
 
-    /// Return true if this is a 256-bit vector type.
-    bool is256BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 256);
-    }
+  /// Return true if this is a 256-bit vector type.
+  bool is256BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 256);
+  }
 
-    /// Return true if this is a 512-bit vector type.
-    bool is512BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 512);
-    }
+  /// Return true if this is a 512-bit vector type.
+  bool is512BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 512);
+  }
 
-    /// Return true if this is a 1024-bit vector type.
-    bool is1024BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 1024);
-    }
+  /// Return true if this is a 1024-bit vector type.
+  bool is1024BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 1024);
+  }
 
-    /// Return true if this is a 2048-bit vector type.
-    bool is2048BitVector() const {
-      return (isFixedLengthVector() && getFixedSizeInBits() == 2048);
-    }
+  /// Return true if this is a 2048-bit vector type.
+  bool is2048BitVector() const {
+    return (isFixedLengthVector() && getFixedSizeInBits() == 2048);
+  }
 
-    /// Return true if this is an overloaded type for TableGen.
-    bool isOverloaded() const {
-      switch (SimpleTy) {
-#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    case Ty:                                                                   \
-      return Any;
-#include "llvm/CodeGen/GenVT.inc"
+  /// Return true if this is an overloaded type for TableGen.
+  bool isOverloaded() const {
+    switch (SimpleTy) {
+#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
+  case Ty:                                                                     \
+    return Any;
+#include "GenVT.inc"
 #undef GET_VT_ATTR
-      default:
-        return false;
-      }
+    default:
+      return false;
     }
+  }
 
-    /// Return a vector with the same number of elements as this vector, but
-    /// with the element type converted to an integer type with the same
-    /// bitwidth.
-    MVT changeVectorElementTypeToInteger() const {
-      MVT EltTy = getVectorElementType();
-      MVT IntTy = MVT::getIntegerVT(EltTy.getSizeInBits());
-      MVT VecTy = MVT::getVectorVT(IntTy, getVectorElementCount());
-      assert(VecTy.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE &&
-             "Simple vector VT not representable by simple integer vector VT!");
-      return VecTy;
-    }
+  /// Return a vector with the same number of elements as this vector, but
+  /// with the element type converted to an integer type with the same
+  /// bitwidth.
+  MVT changeVectorElementTypeToInteger() const {
+    MVT EltTy = getVectorElementType();
+    MVT IntTy = MVT::getIntegerVT(EltTy.getSizeInBits());
+    MVT VecTy = MVT::getVectorVT(IntTy, getVectorElementCount());
+    assert(VecTy.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE &&
+           "Simple vector VT not representable by simple integer vector VT!");
+    return VecTy;
+  }
 
-    /// Return a VT for a vector type whose attributes match ourselves
-    /// with the exception of the element type that is chosen by the caller.
-    MVT changeVectorElementType(MVT EltVT) const {
-      MVT VecTy = MVT::getVectorVT(EltVT, getVectorElementCount());
-      assert(VecTy.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE &&
-             "Simple vector VT not representable by simple integer vector VT!");
-      return VecTy;
-    }
+  /// Return a VT for a vector type whose attributes match ourselves
+  /// with the exception of the element type that is chosen by the caller.
+  MVT changeVectorElementType(MVT EltVT) const {
+    MVT VecTy = MVT::getVectorVT(EltVT, getVectorElementCount());
+    assert(VecTy.SimpleTy != MVT::INVALID_SIMPLE_VALUE_TYPE &&
+           "Simple vector VT not representable by simple integer vector VT!");
+    return VecTy;
+  }
 
-    /// Return the type converted to an equivalently sized integer or vector
-    /// with integer element type. Similar to changeVectorElementTypeToInteger,
-    /// but also handles scalars.
-    MVT changeTypeToInteger() {
-      if (isVector())
-        return changeVectorElementTypeToInteger();
-      return MVT::getIntegerVT(getSizeInBits());
-    }
+  /// Return the type converted to an equivalently sized integer or vector
+  /// with integer element type. Similar to changeVectorElementTypeToInteger,
+  /// but also handles scalars.
+  MVT changeTypeToInteger() {
+    if (isVector())
+      return changeVectorElementTypeToInteger();
+    return MVT::getIntegerVT(getSizeInBits());
+  }
 
-    /// Return a VT for a vector type with the same element type but
-    /// half the number of elements.
-    MVT getHalfNumVectorElementsVT() const {
-      MVT EltVT = getVectorElementType();
-      auto EltCnt = getVectorElementCount();
-      assert(EltCnt.isKnownEven() && "Splitting vector, but not in half!");
-      return getVectorVT(EltVT, EltCnt.divideCoefficientBy(2));
-    }
+  /// Return a VT for a vector type with the same element type but
+  /// half the number of elements.
+  MVT getHalfNumVectorElementsVT() const {
+    MVT EltVT = getVectorElementType();
+    auto EltCnt = getVectorElementCount();
+    assert(EltCnt.isKnownEven() && "Splitting vector, but not in half!");
+    return getVectorVT(EltVT, EltCnt.divideCoefficientBy(2));
+  }
 
-    // Return a VT for a vector type with the same element type but
-    // double the number of elements.
-    MVT getDoubleNumVectorElementsVT() const {
-      MVT EltVT = getVectorElementType();
-      auto EltCnt = getVectorElementCount();
-      return MVT::getVectorVT(EltVT, EltCnt * 2);
-    }
+  // Return a VT for a vector type with the same element type but
+  // double the number of elements.
+  MVT getDoubleNumVectorElementsVT() const {
+    MVT EltVT = getVectorElementType();
+    auto EltCnt = getVectorElementCount();
+    return MVT::getVectorVT(EltVT, EltCnt * 2);
+  }
 
-    /// Returns true if the given vector is a power of 2.
-    bool isPow2VectorType() const {
-      unsigned NElts = getVectorMinNumElements();
-      return !(NElts & (NElts - 1));
-    }
+  /// Returns true if the given vector is a power of 2.
+  bool isPow2VectorType() const {
+    unsigned NElts = getVectorMinNumElements();
+    return !(NElts & (NElts - 1));
+  }
 
-    /// Widens the length of the given vector MVT up to the nearest power of 2
-    /// and returns that type.
-    MVT getPow2VectorType() const {
-      if (isPow2VectorType())
-        return *this;
+  /// Widens the length of the given vector MVT up to the nearest power of 2
+  /// and returns that type.
+  MVT getPow2VectorType() const {
+    if (isPow2VectorType())
+      return *this;
 
-      ElementCount NElts = getVectorElementCount();
-      unsigned NewMinCount = 1 << Log2_32_Ceil(NElts.getKnownMinValue());
-      NElts = ElementCount::get(NewMinCount, NElts.isScalable());
-      return MVT::getVectorVT(getVectorElementType(), NElts);
-    }
+    ElementCount NElts = getVectorElementCount();
+    unsigned NewMinCount = 1 << Log2_32_Ceil(NElts.getKnownMinValue());
+    NElts = ElementCount::get(NewMinCount, NElts.isScalable());
+    return MVT::getVectorVT(getVectorElementType(), NElts);
+  }
 
-    /// If this is a vector, return the element type, otherwise return this.
-    MVT getScalarType() const {
-      return isVector() ? getVectorElementType() : *this;
-    }
+  /// If this is a vector, return the element type, otherwise return this.
+  MVT getScalarType() const {
+    return isVector() ? getVectorElementType() : *this;
+  }
 
-    MVT getVectorElementType() const {
-      assert(SimpleTy >= FIRST_VALUETYPE && SimpleTy <= LAST_VALUETYPE);
-      static constexpr SimpleValueType EltTyTable[] = {
-#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    EltTy,
-#include "llvm/CodeGen/GenVT.inc"
+  MVT getVectorElementType() const {
+    assert(SimpleTy >= FIRST_VALUETYPE && SimpleTy <= LAST_VALUETYPE);
+    static constexpr SimpleValueType EltTyTable[] = {
+#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
+  EltTy,
+#include "GenVT.inc"
 #undef GET_VT_ATTR
-      };
-      SimpleValueType VT = EltTyTable[SimpleTy - FIRST_VALUETYPE];
-      assert(VT != INVALID_SIMPLE_VALUE_TYPE && "Not a vector MVT!");
-      return VT;
-    }
+    };
+    SimpleValueType VT = EltTyTable[SimpleTy - FIRST_VALUETYPE];
+    assert(VT != INVALID_SIMPLE_VALUE_TYPE && "Not a vector MVT!");
+    return VT;
+  }
 
-    /// Given a vector type, return the minimum number of elements it contains.
-    unsigned getVectorMinNumElements() const {
-      assert(SimpleTy >= FIRST_VALUETYPE && SimpleTy <= LAST_VALUETYPE);
-      static constexpr uint16_t NElemTable[] = {
-#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    NElem,
-#include "llvm/CodeGen/GenVT.inc"
+  /// Given a vector type, return the minimum number of elements it contains.
+  unsigned getVectorMinNumElements() const {
+    assert(SimpleTy >= FIRST_VALUETYPE && SimpleTy <= LAST_VALUETYPE);
+    static constexpr uint16_t NElemTable[] = {
+#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
+  NElem,
+#include "GenVT.inc"
 #undef GET_VT_ATTR
-      };
-      unsigned NElem = NElemTable[SimpleTy - FIRST_VALUETYPE];
-      assert(NElem != 0 && "Not a vector MVT!");
-      return NElem;
-    }
+    };
+    unsigned NElem = NElemTable[SimpleTy - FIRST_VALUETYPE];
+    assert(NElem != 0 && "Not a vector MVT!");
+    return NElem;
+  }
 
-    ElementCount getVectorElementCount() const {
-      return ElementCount::get(getVectorMinNumElements(), isScalableVector());
-    }
+  ElementCount getVectorElementCount() const {
+    return ElementCount::get(getVectorMinNumElements(), isScalableVector());
+  }
 
-    unsigned getVectorNumElements() const {
-      if (isScalableVector())
-        llvm::reportInvalidSizeRequest(
-            "Possible incorrect use of MVT::getVectorNumElements() for "
-            "scalable vector. Scalable flag may be dropped, use "
-            "MVT::getVectorElementCount() instead");
-      return getVectorMinNumElements();
-    }
+  unsigned getVectorNumElements() const {
+    if (isScalableVector())
+      llvm::reportInvalidSizeRequest(
+          "Possible incorrect use of MVT::getVectorNumElements() for "
+          "scalable vector. Scalable flag may be dropped, use "
+          "MVT::getVectorElementCount() instead");
+    return getVectorMinNumElements();
+  }
 
-    /// Returns the size of the specified MVT in bits.
-    ///
-    /// If the value type is a scalable vector type, the scalable property will
-    /// be set and the runtime size will be a positive integer multiple of the
-    /// base size.
-    TypeSize getSizeInBits() const {
-      static constexpr TypeSize SizeTable[] = {
-#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    TypeSize(Sz, Sc || Tup || Ty == aarch64svcount /* FIXME: Not in the td.    \
+  /// Returns the size of the specified MVT in bits.
+  ///
+  /// If the value type is a scalable vector type, the scalable property will
+  /// be set and the runtime size will be a positive integer multiple of the
+  /// base size.
+  TypeSize getSizeInBits() const {
+    static constexpr TypeSize SizeTable[] = {
+#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
+  TypeSize(Sz, Sc || Tup || Ty == aarch64svcount /* FIXME: Not in the td.    \
                                                     */),
-#include "llvm/CodeGen/GenVT.inc"
+#include "GenVT.inc"
 #undef GET_VT_ATTR
-      };
-
-      switch (SimpleTy) {
-      case INVALID_SIMPLE_VALUE_TYPE:
-        llvm_unreachable("getSizeInBits called on extended MVT.");
-      case Other:
-        llvm_unreachable("Value type is non-standard value, Other.");
-      case iPTR:
-        llvm_unreachable("Value type size is target-dependent. Ask TLI.");
-      case pAny:
-      case iAny:
-      case fAny:
-      case vAny:
-      case Any:
-        llvm_unreachable("Value type is overloaded.");
-      case token:
-        llvm_unreachable("Token type is a sentinel that cannot be used "
-                         "in codegen and has no size");
-      case Metadata:
-        llvm_unreachable("Value type is metadata.");
-      default:
-        assert(SimpleTy < VALUETYPE_SIZE && "Unexpected value type!");
-        return SizeTable[SimpleTy - FIRST_VALUETYPE];
-      }
-    }
-
-    /// Return the size of the specified fixed width value type in bits. The
-    /// function will assert if the type is scalable.
-    uint64_t getFixedSizeInBits() const {
-      return getSizeInBits().getFixedValue();
-    }
+    };
 
-    uint64_t getScalarSizeInBits() const {
-      return getScalarType().getSizeInBits().getFixedValue();
+    switch (SimpleTy) {
+    case INVALID_SIMPLE_VALUE_TYPE:
+      llvm_unreachable("getSizeInBits called on extended MVT.");
+    case Other:
+      llvm_unreachable("Value type is non-standard value, Other.");
+    case iPTR:
+      llvm_unreachable("Value type size is target-dependent. Ask TLI.");
+    case pAny:
+    case iAny:
+    case fAny:
+    case vAny:
+    case Any:
+      llvm_unreachable("Value type is overloaded.");
+    case token:
+      llvm_unreachable("Token type is a sentinel that cannot be used "
+                       "in codegen and has no size");
+    case Metadata:
+      llvm_unreachable("Value type is metadata.");
+    default:
+      assert(SimpleTy < VALUETYPE_SIZE && "Unexpected value type!");
+      return SizeTable[SimpleTy - FIRST_VALUETYPE];
     }
+  }
 
-    /// Return the number of bytes overwritten by a store of the specified value
-    /// type.
-    ///
-    /// If the value type is a scalable vector type, the scalable property will
-    /// be set and the runtime size will be a positive integer multiple of the
-    /// base size.
-    TypeSize getStoreSize() const {
-      TypeSize BaseSize = getSizeInBits();
-      return {(BaseSize.getKnownMinValue() + 7) / 8, BaseSize.isScalable()};
-    }
+  /// Return the size of the specified fixed width value type in bits. The
+  /// function will assert if the type is scalable.
+  uint64_t getFixedSizeInBits() const {
+    return getSizeInBits().getFixedValue();
+  }
 
-    // Return the number of bytes overwritten by a store of this value type or
-    // this value type's element type in the case of a vector.
-    uint64_t getScalarStoreSize() const {
-      return getScalarType().getStoreSize().getFixedValue();
-    }
+  uint64_t getScalarSizeInBits() const {
+    return getScalarType().getSizeInBits().getFixedValue();
+  }
 
-    /// Return the number of bits overwritten by a store of the specified value
-    /// type.
-    ///
-    /// If the value type is a scalable vector type, the scalable property will
-    /// be set and the runtime size will be a positive integer multiple of the
-    /// base size.
-    TypeSize getStoreSizeInBits() const {
-      return getStoreSize() * 8;
-    }
+  /// Return the number of bytes overwritten by a store of the specified value
+  /// type.
+  ///
+  /// If the value type is a scalable vector type, the scalable property will
+  /// be set and the runtime size will be a positive integer multiple of the
+  /// base size.
+  TypeSize getStoreSize() const {
+    TypeSize BaseSize = getSizeInBits();
+    return {(BaseSize.getKnownMinValue() + 7) / 8, BaseSize.isScalable()};
+  }
 
-    /// Returns true if the number of bits for the type is a multiple of an
-    /// 8-bit byte.
-    bool isByteSized() const { return getSizeInBits().isKnownMultipleOf(8); }
+  // Return the number of bytes overwritten by a store of this value type or
+  // this value type's element type in the case of a vector.
+  uint64_t getScalarStoreSize() const {
+    return getScalarType().getStoreSize().getFixedValue();
+  }
 
-    /// Return true if we know at compile time this has more bits than VT.
-    bool knownBitsGT(MVT VT) const {
-      return TypeSize::isKnownGT(getSizeInBits(), VT.getSizeInBits());
-    }
+  /// Return the number of bits overwritten by a store of the specified value
+  /// type.
+  ///
+  /// If the value type is a scalable vector type, the scalable property will
+  /// be set and the runtime size will be a positive integer multiple of the
+  /// base size.
+  TypeSize getStoreSizeInBits() const { return getStoreSize() * 8; }
+
+  /// Returns true if the number of bits for the type is a multiple of an
+  /// 8-bit byte.
+  bool isByteSized() const { return getSizeInBits().isKnownMultipleOf(8); }
+
+  /// Return true if we know at compile time this has more bits than VT.
+  bool knownBitsGT(MVT VT) const {
+    return TypeSize::isKnownGT(getSizeInBits(), VT.getSizeInBits());
+  }
 
-    /// Return true if we know at compile time this has more than or the same
-    /// bits as VT.
-    bool knownBitsGE(MVT VT) const {
-      return TypeSize::isKnownGE(getSizeInBits(), VT.getSizeInBits());
-    }
+  /// Return true if we know at compile time this has more than or the same
+  /// bits as VT.
+  bool knownBitsGE(MVT VT) const {
+    return TypeSize::isKnownGE(getSizeInBits(), VT.getSizeInBits());
+  }
 
-    /// Return true if we know at compile time this has fewer bits than VT.
-    bool knownBitsLT(MVT VT) const {
-      return TypeSize::isKnownLT(getSizeInBits(), VT.getSizeInBits());
-    }
+  /// Return true if we know at compile time this has fewer bits than VT.
+  bool knownBitsLT(MVT VT) const {
+    return TypeSize::isKnownLT(getSizeInBits(), VT.getSizeInBits());
+  }
 
-    /// Return true if we know at compile time this has fewer than or the same
-    /// bits as VT.
-    bool knownBitsLE(MVT VT) const {
-      return TypeSize::isKnownLE(getSizeInBits(), VT.getSizeInBits());
-    }
+  /// Return true if we know at compile time this has fewer than or the same
+  /// bits as VT.
+  bool knownBitsLE(MVT VT) const {
+    return TypeSize::isKnownLE(getSizeInBits(), VT.getSizeInBits());
+  }
 
-    /// Return true if this has more bits than VT.
-    bool bitsGT(MVT VT) const {
-      assert(isScalableVector() == VT.isScalableVector() &&
-             "Comparison between scalable and fixed types");
-      return knownBitsGT(VT);
-    }
+  /// Return true if this has more bits than VT.
+  bool bitsGT(MVT VT) const {
+    assert(isScalableVector() == VT.isScalableVector() &&
+           "Comparison between scalable and fixed types");
+    return knownBitsGT(VT);
+  }
 
-    /// Return true if this has no less bits than VT.
-    bool bitsGE(MVT VT) const {
-      assert(isScalableVector() == VT.isScalableVector() &&
-             "Comparison between scalable and fixed types");
-      return knownBitsGE(VT);
-    }
+  /// Return true if this has no less bits than VT.
+  bool bitsGE(MVT VT) const {
+    assert(isScalableVector() == VT.isScalableVector() &&
+           "Comparison between scalable and fixed types");
+    return knownBitsGE(VT);
+  }
 
-    /// Return true if this has less bits than VT.
-    bool bitsLT(MVT VT) const {
-      assert(isScalableVector() == VT.isScalableVector() &&
-             "Comparison between scalable and fixed types");
-      return knownBitsLT(VT);
-    }
+  /// Return true if this has less bits than VT.
+  bool bitsLT(MVT VT) const {
+    assert(isScalableVector() == VT.isScalableVector() &&
+           "Comparison between scalable and fixed types");
+    return knownBitsLT(VT);
+  }
 
-    /// Return true if this has no more bits than VT.
-    bool bitsLE(MVT VT) const {
-      assert(isScalableVector() == VT.isScalableVector() &&
-             "Comparison between scalable and fixed types");
-      return knownBitsLE(VT);
-    }
+  /// Return true if this has no more bits than VT.
+  bool bitsLE(MVT VT) const {
+    assert(isScalableVector() == VT.isScalableVector() &&
+           "Comparison between scalable and fixed types");
+    return knownBitsLE(VT);
+  }
 
-    static MVT getFloatingPointVT(unsigned BitWidth) {
-#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    if (FP == 3 && sz == BitWidth)                                             \
-      return Ty;
-#include "llvm/CodeGen/GenVT.inc"
+  static MVT getFloatingPointVT(unsigned BitWidth) {
+#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
+  if (FP == 3 && sz == BitWidth)                                               \
+    return Ty;
+#include "GenVT.inc"
 #undef GET_VT_ATTR
 
-      llvm_unreachable("Bad bit width!");
-    }
+    llvm_unreachable("Bad bit width!");
+  }
 
-    static MVT getIntegerVT(unsigned BitWidth) {
-#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    if (Int == 3 && sz == BitWidth)                                            \
-      return Ty;
-#include "llvm/CodeGen/GenVT.inc"
+  static MVT getIntegerVT(unsigned BitWidth) {
+#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
+  if (Int == 3 && sz == BitWidth)                                              \
+    return Ty;
+#include "GenVT.inc"
 #undef GET_VT_ATTR
 
-      return (MVT::SimpleValueType)(MVT::INVALID_SIMPLE_VALUE_TYPE);
-    }
+    return (MVT::SimpleValueType)(MVT::INVALID_SIMPLE_VALUE_TYPE);
+  }
 
-    static MVT getVectorVT(MVT VT, unsigned NumElements) {
-#define GET_VT_VECATTR(Ty, Sc, Tup, nElem, ElTy)                             \
-    if (!Sc && !Tup && VT.SimpleTy == ElTy && NumElements == nElem)            \
-      return Ty;
-#include "llvm/CodeGen/GenVT.inc"
+  static MVT getVectorVT(MVT VT, unsigned NumElements) {
+#define GET_VT_VECATTR(Ty, Sc, Tup, nElem, ElTy)                               \
+  if (!Sc && !Tup && VT.SimpleTy == ElTy && NumElements == nElem)              \
+    return Ty;
+#include "GenVT.inc"
 #undef GET_VT_VECATTR
 
-      return (MVT::SimpleValueType)(MVT::INVALID_SIMPLE_VALUE_TYPE);
-    }
+    return (MVT::SimpleValueType)(MVT::INVALID_SIMPLE_VALUE_TYPE);
+  }
 
-    static MVT getScalableVectorVT(MVT VT, unsigned NumElements) {
-#define GET_VT_VECATTR(Ty, Sc, Tup, nElem, ElTy)                             \
-    if (Sc && VT.SimpleTy == ElTy && NumElements == nElem)                     \
-      return Ty;
-#include "llvm/CodeGen/GenVT.inc"
+  static MVT getScalableVectorVT(MVT VT, unsigned NumElements) {
+#define GET_VT_VECATTR(Ty, Sc, Tup, nElem, ElTy)                               \
+  if (Sc && VT.SimpleTy == ElTy && NumElements == nElem)                       \
+    return Ty;
+#include "GenVT.inc"
 #undef GET_VT_VECATTR
 
-      return (MVT::SimpleValueType)(MVT::INVALID_SIMPLE_VALUE_TYPE);
-    }
+    return (MVT::SimpleValueType)(MVT::INVALID_SIMPLE_VALUE_TYPE);
+  }
 
-    static MVT getRISCVVectorTupleVT(unsigned Sz, unsigned NFields) {
-#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, nElem, EltTy) \
-    if (Tup && sz == Sz && NF == NFields)                                      \
-      return Ty;
-#include "llvm/CodeGen/GenVT.inc"
+  static MVT getRISCVVectorTupleVT(unsigned Sz, unsigned NFields) {
+#define GET_VT_ATTR(Ty, n, sz, Any, Int, FP, Vec, Sc, Tup, NF, nElem, EltTy)   \
+  if (Tup && sz == Sz && NF == NFields)                                        \
+    return Ty;
+#include "GenVT.inc"
 #undef GET_VT_ATTR
 
-      llvm_unreachable("Invalid RISCV vector tuple type");
-    }
+    llvm_unreachable("Invalid RISCV vector tuple type");
+  }
 
-    /// Given a RISC-V vector tuple type, return the num_fields.
-    unsigned getRISCVVectorTupleNumFields() const {
-      assert(isRISCVVectorTuple() && SimpleTy >= FIRST_VALUETYPE &&
-             SimpleTy <= LAST_VALUETYPE);
-      static constexpr uint8_t NFTable[] = {
-#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) \
-    NF,
-#include "llvm/CodeGen/GenVT.inc"
+  /// Given a RISC-V vector tuple type, return the num_fields.
+  unsigned getRISCVVectorTupleNumFields() const {
+    assert(isRISCVVectorTuple() && SimpleTy >= FIRST_VALUETYPE &&
+           SimpleTy <= LAST_VALUETYPE);
+    static constexpr uint8_t NFTable[] = {
+#define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy) NF,
+#include "GenVT.inc"
 #undef GET_VT_ATTR
-      };
-      return NFTable[SimpleTy - FIRST_VALUETYPE];
-    }
-
-    static MVT getVectorVT(MVT VT, unsigned NumElements, bool IsScalable) {
-      if (IsScalable)
-        return getScalableVectorVT(VT, NumElements);
-      return getVectorVT(VT, NumElements);
-    }
+    };
+    return NFTable[SimpleTy - FIRST_VALUETYPE];
+  }
 
-    static MVT getVectorVT(MVT VT, ElementCount EC) {
-      if (EC.isScalable())
-        return getScalableVectorVT(VT, EC.getKnownMinValue());
-      return getVectorVT(VT, EC.getKnownMinValue());
-    }
+  static MVT getVectorVT(MVT VT, unsigned NumElements, bool IsScalable) {
+    if (IsScalable)
+      return getScalableVectorVT(VT, NumElements);
+    return getVectorVT(VT, NumElements);
+  }
 
-    /// Return the value type corresponding to the specified type.
-    /// If HandleUnknown is true, unknown types are returned as Other,
-    /// otherwise they are invalid.
-    /// NB: This includes pointer types, which require a DataLayout to convert
-    /// to a concrete value type.
-    static MVT getVT(Type *Ty, bool HandleUnknown = false);
-
-    /// Returns an APFloat semantics tag appropriate for the value type. If this
-    /// is a vector type, the element semantics are returned.
-    const fltSemantics &getFltSemantics() const;
-
-  public:
-    /// SimpleValueType Iteration
-    /// @{
-    static auto all_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_VALUETYPE, MVT::LAST_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static MVT getVectorVT(MVT VT, ElementCount EC) {
+    if (EC.isScalable())
+      return getScalableVectorVT(VT, EC.getKnownMinValue());
+    return getVectorVT(VT, EC.getKnownMinValue());
+  }
 
-    static auto integer_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_INTEGER_VALUETYPE,
-                                MVT::LAST_INTEGER_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  /// Return the value type corresponding to the specified type.
+  /// If HandleUnknown is true, unknown types are returned as Other,
+  /// otherwise they are invalid.
+  /// NB: This includes pointer types, which require a DataLayout to convert
+  /// to a concrete value type.
+  static MVT getVT(Type *Ty, bool HandleUnknown = false);
+
+  /// Returns an APFloat semantics tag appropriate for the value type. If this
+  /// is a vector type, the element semantics are returned.
+  const fltSemantics &getFltSemantics() const;
+
+public:
+  /// SimpleValueType Iteration
+  /// @{
+  static auto all_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_VALUETYPE, MVT::LAST_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto fp_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_FP_VALUETYPE, MVT::LAST_FP_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static auto integer_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_INTEGER_VALUETYPE,
+                              MVT::LAST_INTEGER_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto vector_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_VECTOR_VALUETYPE,
-                                MVT::LAST_VECTOR_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static auto fp_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_FP_VALUETYPE, MVT::LAST_FP_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto fixedlen_vector_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_FIXEDLEN_VECTOR_VALUETYPE,
-                                MVT::LAST_FIXEDLEN_VECTOR_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static auto vector_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_VECTOR_VALUETYPE,
+                              MVT::LAST_VECTOR_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto scalable_vector_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_SCALABLE_VECTOR_VALUETYPE,
-                                MVT::LAST_SCALABLE_VECTOR_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static auto fixedlen_vector_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_FIXEDLEN_VECTOR_VALUETYPE,
+                              MVT::LAST_FIXEDLEN_VECTOR_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto integer_fixedlen_vector_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE,
-                                MVT::LAST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static auto scalable_vector_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_SCALABLE_VECTOR_VALUETYPE,
+                              MVT::LAST_SCALABLE_VECTOR_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto fp_fixedlen_vector_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_FP_FIXEDLEN_VECTOR_VALUETYPE,
-                                MVT::LAST_FP_FIXEDLEN_VECTOR_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static auto integer_fixedlen_vector_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE,
+                              MVT::LAST_INTEGER_FIXEDLEN_VECTOR_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto integer_scalable_vector_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_INTEGER_SCALABLE_VECTOR_VALUETYPE,
-                                MVT::LAST_INTEGER_SCALABLE_VECTOR_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
+  static auto fp_fixedlen_vector_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_FP_FIXEDLEN_VECTOR_VALUETYPE,
+                              MVT::LAST_FP_FIXEDLEN_VECTOR_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-    static auto fp_scalable_vector_valuetypes() {
-      return enum_seq_inclusive(MVT::FIRST_FP_SCALABLE_VECTOR_VALUETYPE,
-                                MVT::LAST_FP_SCALABLE_VECTOR_VALUETYPE,
-                                force_iteration_on_noniterable_enum);
-    }
-    /// @}
-  };
+  static auto integer_scalable_vector_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_INTEGER_SCALABLE_VECTOR_VALUETYPE,
+                              MVT::LAST_INTEGER_SCALABLE_VECTOR_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
+  }
 
-  inline raw_ostream &operator<<(raw_ostream &OS, const MVT &VT) {
-    VT.print(OS);
-    return OS;
+  static auto fp_scalable_vector_valuetypes() {
+    return enum_seq_inclusive(MVT::FIRST_FP_SCALABLE_VECTOR_VALUETYPE,
+                              MVT::LAST_FP_SCALABLE_VECTOR_VALUETYPE,
+                              force_iteration_on_noniterable_enum);
   }
+  /// @}
+};
+
+inline raw_ostream &operator<<(raw_ostream &OS, const MVT &VT) {
+  VT.print(OS);
+  return OS;
+}
 
 } // end namespace llvm
 
diff --git a/llvm/lib/CMakeLists.txt b/llvm/lib/CMakeLists.txt
index d0a2bc929..3ec909d95 100644
--- a/llvm/lib/CMakeLists.txt
+++ b/llvm/lib/CMakeLists.txt
@@ -24,7 +24,6 @@ add_subdirectory(Analysis)
 add_subdirectory(LTO)
 add_subdirectory(MC)
 add_subdirectory(MCA)
-add_subdirectory(ObjCopy)
 add_subdirectory(Object)
 add_subdirectory(ObjectYAML)
 add_subdirectory(Option)
@@ -44,7 +43,6 @@ add_subdirectory(TextAPI)
 if (LLVM_BUILD_TELEMETRY)
   add_subdirectory(Telemetry)
 endif()
-add_subdirectory(ToolDrivers)
 add_subdirectory(XRay)
 if (LLVM_INCLUDE_TESTS)
   add_subdirectory(Testing)
diff --git a/llvm/lib/CodeGen/ValueTypes.cpp b/llvm/lib/CodeGen/ValueTypes.cpp
index 2c80eee7c..3e9758ec8 100644
--- a/llvm/lib/CodeGen/ValueTypes.cpp
+++ b/llvm/lib/CodeGen/ValueTypes.cpp
@@ -177,19 +177,32 @@ std::string EVT::getEVTString() const {
     if (isFloatingPoint())
       return "f" + utostr(getSizeInBits());
     llvm_unreachable("Invalid EVT!");
-  case MVT::bf16:      return "bf16";
-  case MVT::ppcf128:   return "ppcf128";
-  case MVT::isVoid:    return "isVoid";
-  case MVT::Other:     return "ch";
-  case MVT::Glue:      return "glue";
-  case MVT::x86mmx:    return "x86mmx";
-  case MVT::x86amx:    return "x86amx";
-  case MVT::i64x8:     return "i64x8";
-  case MVT::Metadata:  return "Metadata";
-  case MVT::Untyped:   return "Untyped";
-  case MVT::funcref:   return "funcref";
-  case MVT::exnref:    return "exnref";
-  case MVT::externref: return "externref";
+  case MVT::bf16:
+    return "bf16";
+  case MVT::ppcf128:
+    return "ppcf128";
+  case MVT::isVoid:
+    return "isVoid";
+  case MVT::Other:
+    return "ch";
+  case MVT::Glue:
+    return "glue";
+  case MVT::x86mmx:
+    return "x86mmx";
+  case MVT::x86amx:
+    return "x86amx";
+  case MVT::i64x8:
+    return "i64x8";
+  case MVT::Metadata:
+    return "Metadata";
+  case MVT::Untyped:
+    return "Untyped";
+  case MVT::funcref:
+    return "funcref";
+  case MVT::exnref:
+    return "exnref";
+  case MVT::externref:
+    return "externref";
   case MVT::aarch64svcount:
     return "aarch64svcount";
   case MVT::spirvbuiltin:
@@ -223,7 +236,7 @@ Type *EVT::getTypeForEVT(LLVMContext &Context) const {
   case MVT::funcref: return Type::getWasm_FuncrefTy(Context);
   case MVT::Metadata: return Type::getMetadataTy(Context);
 #define GET_VT_EVT(Ty, EVT) case MVT::Ty: return EVT;
-#include "llvm/CodeGen/GenVT.inc"
+#include "GenVT.inc"
 #undef GET_VT_EVT
   }
   // clang-format on
@@ -234,20 +247,25 @@ Type *EVT::getTypeForEVT(LLVMContext &Context) const {
 /// they are invalid.
 /// NB: This includes pointer types, which require a DataLayout to convert
 /// to a concrete value type.
-MVT MVT::getVT(Type *Ty, bool HandleUnknown){
+MVT MVT::getVT(Type *Ty, bool HandleUnknown) {
   assert(Ty != nullptr && "Invalid type");
   switch (Ty->getTypeID()) {
   default:
-    if (HandleUnknown) return MVT(MVT::Other);
+    if (HandleUnknown)
+      return MVT(MVT::Other);
     llvm_unreachable("Unknown type!");
   case Type::VoidTyID:
     return MVT::isVoid;
   case Type::IntegerTyID:
     return getIntegerVT(cast<IntegerType>(Ty)->getBitWidth());
-  case Type::HalfTyID:      return MVT(MVT::f16);
-  case Type::BFloatTyID:    return MVT(MVT::bf16);
-  case Type::FloatTyID:     return MVT(MVT::f32);
-  case Type::DoubleTyID:    return MVT(MVT::f64);
+  case Type::HalfTyID:
+    return MVT(MVT::f16);
+  case Type::BFloatTyID:
+    return MVT(MVT::bf16);
+  case Type::FloatTyID:
+    return MVT(MVT::f32);
+  case Type::DoubleTyID:
+    return MVT(MVT::f64);
   case Type::X86_FP80TyID:
     return MVT(MVT::f80);
   case Type::TargetExtTyID: {
@@ -268,15 +286,17 @@ MVT MVT::getVT(Type *Ty, bool HandleUnknown){
       return MVT(MVT::Other);
     llvm_unreachable("Unknown target ext type!");
   }
-  case Type::X86_AMXTyID:   return MVT(MVT::x86amx);
-  case Type::FP128TyID:     return MVT(MVT::f128);
-  case Type::PPC_FP128TyID: return MVT(MVT::ppcf128);
+  case Type::X86_AMXTyID:
+    return MVT(MVT::x86amx);
+  case Type::FP128TyID:
+    return MVT(MVT::f128);
+  case Type::PPC_FP128TyID:
+    return MVT(MVT::ppcf128);
   case Type::FixedVectorTyID:
   case Type::ScalableVectorTyID: {
     VectorType *VTy = cast<VectorType>(Ty);
-    return getVectorVT(
-      getVT(VTy->getElementType(), /*HandleUnknown=*/ false),
-            VTy->getElementCount());
+    return getVectorVT(getVT(VTy->getElementType(), /*HandleUnknown=*/false),
+                       VTy->getElementCount());
   }
   }
 }
@@ -286,7 +306,7 @@ MVT MVT::getVT(Type *Ty, bool HandleUnknown){
 /// they are invalid.
 /// NB: This includes pointer types, which require a DataLayout to convert
 /// to a concrete value type.
-EVT EVT::getEVT(Type *Ty, bool HandleUnknown){
+EVT EVT::getEVT(Type *Ty, bool HandleUnknown) {
   switch (Ty->getTypeID()) {
   default:
     return MVT::getVT(Ty, HandleUnknown);
@@ -298,7 +318,7 @@ EVT EVT::getEVT(Type *Ty, bool HandleUnknown){
   case Type::ScalableVectorTyID: {
     VectorType *VTy = cast<VectorType>(Ty);
     return getVectorVT(Ty->getContext(),
-                       getEVT(VTy->getElementType(), /*HandleUnknown=*/ false),
+                       getEVT(VTy->getElementType(), /*HandleUnknown=*/false),
                        VTy->getElementCount());
   }
   }
@@ -306,14 +326,22 @@ EVT EVT::getEVT(Type *Ty, bool HandleUnknown){
 
 const fltSemantics &MVT::getFltSemantics() const {
   switch (getScalarType().SimpleTy) {
-  default: llvm_unreachable("Unknown FP format");
-  case MVT::f16:     return APFloat::IEEEhalf();
-  case MVT::bf16:    return APFloat::BFloat();
-  case MVT::f32:     return APFloat::IEEEsingle();
-  case MVT::f64:     return APFloat::IEEEdouble();
-  case MVT::f80:     return APFloat::x87DoubleExtended();
-  case MVT::f128:    return APFloat::IEEEquad();
-  case MVT::ppcf128: return APFloat::PPCDoubleDouble();
+  default:
+    llvm_unreachable("Unknown FP format");
+  case MVT::f16:
+    return APFloat::IEEEhalf();
+  case MVT::bf16:
+    return APFloat::BFloat();
+  case MVT::f32:
+    return APFloat::IEEEsingle();
+  case MVT::f64:
+    return APFloat::IEEEdouble();
+  case MVT::f80:
+    return APFloat::x87DoubleExtended();
+  case MVT::f128:
+    return APFloat::IEEEquad();
+  case MVT::ppcf128:
+    return APFloat::PPCDoubleDouble();
   }
 }
 
diff --git a/llvm/lib/Support/BLAKE3/CMakeLists.txt b/llvm/lib/Support/BLAKE3/CMakeLists.txt
index 99cb78881..d5916bd58 100644
--- a/llvm/lib/Support/BLAKE3/CMakeLists.txt
+++ b/llvm/lib/Support/BLAKE3/CMakeLists.txt
@@ -82,6 +82,6 @@ else()
   disable_blake3_x86_simd()
 endif()
 
-add_library(LLVMSupportBlake3 OBJECT EXCLUDE_FROM_ALL ${LLVM_BLAKE3_FILES})
+add_library(LLVMSupportBlake3 STATIC EXCLUDE_FROM_ALL ${LLVM_BLAKE3_FILES})
 set_target_properties(LLVMSupportBlake3 PROPERTIES FOLDER "LLVM/Libraries")
 llvm_update_compile_flags(LLVMSupportBlake3)
diff --git a/llvm/lib/Target/AArch64/AArch64.h b/llvm/lib/Target/AArch64/AArch64.h
index ffa578d41..0f7c58c7e 100644
--- a/llvm/lib/Target/AArch64/AArch64.h
+++ b/llvm/lib/Target/AArch64/AArch64.h
@@ -74,21 +74,21 @@ FunctionPass *createAArch64StackTaggingPass(bool IsOptNone);
 FunctionPass *createAArch64StackTaggingPreRAPass();
 ModulePass *createAArch64Arm64ECCallLoweringPass();
 
-void initializeAArch64A53Fix835769Pass(PassRegistry&);
-void initializeAArch64A57FPLoadBalancingPass(PassRegistry&);
-void initializeAArch64AdvSIMDScalarPass(PassRegistry&);
-void initializeAArch64PointerAuthPass(PassRegistry&);
-void initializeAArch64BranchTargetsPass(PassRegistry&);
-void initializeAArch64CFIFixupPass(PassRegistry&);
+void initializeAArch64A53Fix835769Pass(PassRegistry &);
+void initializeAArch64A57FPLoadBalancingPass(PassRegistry &);
+void initializeAArch64AdvSIMDScalarPass(PassRegistry &);
+void initializeAArch64PointerAuthPass(PassRegistry &);
+void initializeAArch64BranchTargetsPass(PassRegistry &);
+void initializeAArch64CFIFixupPass(PassRegistry &);
 void initializeAArch64CollectLOHPass(PassRegistry &);
-void initializeAArch64CompressJumpTablesPass(PassRegistry&);
+void initializeAArch64CompressJumpTablesPass(PassRegistry &);
 void initializeAArch64CondBrTuningPass(PassRegistry &);
-void initializeAArch64ConditionOptimizerPass(PassRegistry&);
+void initializeAArch64ConditionOptimizerPass(PassRegistry &);
 void initializeAArch64ConditionalComparesPass(PassRegistry &);
 void initializeAArch64DAGToDAGISelLegacyPass(PassRegistry &);
-void initializeAArch64DeadRegisterDefinitionsPass(PassRegistry&);
+void initializeAArch64DeadRegisterDefinitionsPass(PassRegistry &);
 void initializeAArch64ExpandPseudoPass(PassRegistry &);
-void initializeAArch64LoadStoreOptPass(PassRegistry&);
+void initializeAArch64LoadStoreOptPass(PassRegistry &);
 void initializeAArch64LowerHomogeneousPrologEpilogPass(PassRegistry &);
 void initializeAArch64MIPeepholeOptPass(PassRegistry &);
 void initializeAArch64O0PreLegalizerCombinerPass(PassRegistry &);
@@ -97,17 +97,17 @@ void initializeAArch64PostLegalizerCombinerPass(PassRegistry &);
 void initializeAArch64PostLegalizerLoweringPass(PassRegistry &);
 void initializeAArch64PostSelectOptimizePass(PassRegistry &);
 void initializeAArch64PreLegalizerCombinerPass(PassRegistry &);
-void initializeAArch64PromoteConstantPass(PassRegistry&);
-void initializeAArch64RedundantCopyEliminationPass(PassRegistry&);
+void initializeAArch64PromoteConstantPass(PassRegistry &);
+void initializeAArch64RedundantCopyEliminationPass(PassRegistry &);
 void initializeAArch64SIMDInstrOptPass(PassRegistry &);
 void initializeAArch64SLSHardeningPass(PassRegistry &);
 void initializeAArch64SpeculationHardeningPass(PassRegistry &);
 void initializeAArch64StackTaggingPass(PassRegistry &);
 void initializeAArch64StackTaggingPreRAPass(PassRegistry &);
-void initializeAArch64StorePairSuppressPass(PassRegistry&);
-void initializeFalkorHWPFFixPass(PassRegistry&);
-void initializeFalkorMarkStridedAccessesLegacyPass(PassRegistry&);
-void initializeLDTLSCleanupPass(PassRegistry&);
+void initializeAArch64StorePairSuppressPass(PassRegistry &);
+void initializeFalkorHWPFFixPass(PassRegistry &);
+void initializeFalkorMarkStridedAccessesLegacyPass(PassRegistry &);
+void initializeLDTLSCleanupPass(PassRegistry &);
 void initializeSMEABIPass(PassRegistry &);
 void initializeSMEPeepholeOptPass(PassRegistry &);
 void initializeSVEIntrinsicOptsPass(PassRegistry &);
diff --git a/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp b/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
index c6b4a219d..d3f01d2b3 100644
--- a/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
+++ b/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
@@ -236,8 +236,7 @@ public:
       bool Local = MF.getFunction().hasLocalLinkage();
       COFF::SymbolStorageClass Scl =
           Local ? COFF::IMAGE_SYM_CLASS_STATIC : COFF::IMAGE_SYM_CLASS_EXTERNAL;
-      int Type =
-        COFF::IMAGE_SYM_DTYPE_FUNCTION << COFF::SCT_COMPLEX_TYPE_SHIFT;
+      int Type = COFF::IMAGE_SYM_DTYPE_FUNCTION << COFF::SCT_COMPLEX_TYPE_SHIFT;
 
       OutStreamer->beginCOFFSymbolDef(CurrentFnSym);
       OutStreamer->emitCOFFSymbolStorageClass(Scl);
@@ -405,8 +404,7 @@ void AArch64AsmPrinter::emitFunctionHeaderComment() {
     OutStreamer->getCommentOS() << ' ' << OutlinerString;
 }
 
-void AArch64AsmPrinter::LowerPATCHABLE_FUNCTION_ENTER(const MachineInstr &MI)
-{
+void AArch64AsmPrinter::LowerPATCHABLE_FUNCTION_ENTER(const MachineInstr &MI) {
   const Function &F = MF->getFunction();
   if (F.hasFnAttribute("patchable-function-entry")) {
     unsigned Num;
@@ -1276,10 +1274,12 @@ void AArch64AsmPrinter::PrintDebugValueComment(const MachineInstr *MI,
 
 void AArch64AsmPrinter::emitJumpTableInfo() {
   const MachineJumpTableInfo *MJTI = MF->getJumpTableInfo();
-  if (!MJTI) return;
+  if (!MJTI)
+    return;
 
   const std::vector<MachineJumpTableEntry> &JT = MJTI->getJumpTables();
-  if (JT.empty()) return;
+  if (JT.empty())
+    return;
 
   const TargetLoweringObjectFile &TLOF = getObjFileLowering();
   MCSection *ReadOnlySec = TLOF.getSectionForJumpTable(MF->getFunction(), TM);
@@ -1287,10 +1287,11 @@ void AArch64AsmPrinter::emitJumpTableInfo() {
 
   auto AFI = MF->getInfo<AArch64FunctionInfo>();
   for (unsigned JTI = 0, e = JT.size(); JTI != e; ++JTI) {
-    const std::vector<MachineBasicBlock*> &JTBBs = JT[JTI].MBBs;
+    const std::vector<MachineBasicBlock *> &JTBBs = JT[JTI].MBBs;
 
     // If this jump table was deleted, ignore it.
-    if (JTBBs.empty()) continue;
+    if (JTBBs.empty())
+      continue;
 
     unsigned Size = AFI->getJumpTableEntrySize(JTI);
     emitAlignment(Align(Size));
@@ -1476,16 +1477,22 @@ void AArch64AsmPrinter::LowerJumpTableDest(llvm::MCStreamer &OutStreamer,
   }
 
   auto LabelExpr = MCSymbolRefExpr::create(Label, MF->getContext());
-  EmitToStreamer(OutStreamer, MCInstBuilder(AArch64::ADR)
-                                  .addReg(DestReg)
-                                  .addExpr(LabelExpr));
+  EmitToStreamer(
+      OutStreamer,
+      MCInstBuilder(AArch64::ADR).addReg(DestReg).addExpr(LabelExpr));
 
   // Load the number of instruction-steps to offset from the label.
   unsigned LdrOpcode;
   switch (Size) {
-  case 1: LdrOpcode = AArch64::LDRBBroX; break;
-  case 2: LdrOpcode = AArch64::LDRHHroX; break;
-  case 4: LdrOpcode = AArch64::LDRSWroX; break;
+  case 1:
+    LdrOpcode = AArch64::LDRBBroX;
+    break;
+  case 2:
+    LdrOpcode = AArch64::LDRHHroX;
+    break;
+  case 4:
+    LdrOpcode = AArch64::LDRSWroX;
+    break;
   default:
     llvm_unreachable("Unknown jump table size");
   }
@@ -1839,7 +1846,8 @@ void AArch64AsmPrinter::emitFMov0(const MachineInstr &MI) {
   } else {
     MCInst FMov;
     switch (MI.getOpcode()) {
-    default: llvm_unreachable("Unexpected opcode");
+    default:
+      llvm_unreachable("Unexpected opcode");
     case AArch64::FMOVH0:
       FMov.setOpcode(STI->hasFullFP16() ? AArch64::FMOVWHr : AArch64::FMOVWSr);
       if (!STI->hasFullFP16())
@@ -2602,7 +2610,8 @@ void AArch64AsmPrinter::EmitToStreamer(MCStreamer &S, const MCInst &Inst) {
 }
 
 void AArch64AsmPrinter::emitInstruction(const MachineInstr *MI) {
-  AArch64_MC::verifyInstructionPredicates(MI->getOpcode(), STI->getFeatureBits());
+  AArch64_MC::verifyInstructionPredicates(MI->getOpcode(),
+                                          STI->getFeatureBits());
 
 #ifndef NDEBUG
   InstsEmitted = 0;
@@ -2635,7 +2644,7 @@ void AArch64AsmPrinter::emitInstruction(const MachineInstr *MI) {
   }
 
   AArch64TargetStreamer *TS =
-    static_cast<AArch64TargetStreamer *>(OutStreamer->getTargetStreamer());
+      static_cast<AArch64TargetStreamer *>(OutStreamer->getTargetStreamer());
   // Do any manual lowerings.
   switch (MI->getOpcode()) {
   default:
@@ -2662,33 +2671,33 @@ void AArch64AsmPrinter::emitInstruction(const MachineInstr *MI) {
     }
     break;
   }
-    case AArch64::MOVMCSym: {
-      Register DestReg = MI->getOperand(0).getReg();
-      const MachineOperand &MO_Sym = MI->getOperand(1);
-      MachineOperand Hi_MOSym(MO_Sym), Lo_MOSym(MO_Sym);
-      MCOperand Hi_MCSym, Lo_MCSym;
-
-      Hi_MOSym.setTargetFlags(AArch64II::MO_G1 | AArch64II::MO_S);
-      Lo_MOSym.setTargetFlags(AArch64II::MO_G0 | AArch64II::MO_NC);
-
-      MCInstLowering.lowerOperand(Hi_MOSym, Hi_MCSym);
-      MCInstLowering.lowerOperand(Lo_MOSym, Lo_MCSym);
-
-      MCInst MovZ;
-      MovZ.setOpcode(AArch64::MOVZXi);
-      MovZ.addOperand(MCOperand::createReg(DestReg));
-      MovZ.addOperand(Hi_MCSym);
-      MovZ.addOperand(MCOperand::createImm(16));
-      EmitToStreamer(*OutStreamer, MovZ);
-
-      MCInst MovK;
-      MovK.setOpcode(AArch64::MOVKXi);
-      MovK.addOperand(MCOperand::createReg(DestReg));
-      MovK.addOperand(MCOperand::createReg(DestReg));
-      MovK.addOperand(Lo_MCSym);
-      MovK.addOperand(MCOperand::createImm(0));
-      EmitToStreamer(*OutStreamer, MovK);
-      return;
+  case AArch64::MOVMCSym: {
+    Register DestReg = MI->getOperand(0).getReg();
+    const MachineOperand &MO_Sym = MI->getOperand(1);
+    MachineOperand Hi_MOSym(MO_Sym), Lo_MOSym(MO_Sym);
+    MCOperand Hi_MCSym, Lo_MCSym;
+
+    Hi_MOSym.setTargetFlags(AArch64II::MO_G1 | AArch64II::MO_S);
+    Lo_MOSym.setTargetFlags(AArch64II::MO_G0 | AArch64II::MO_NC);
+
+    MCInstLowering.lowerOperand(Hi_MOSym, Hi_MCSym);
+    MCInstLowering.lowerOperand(Lo_MOSym, Lo_MCSym);
+
+    MCInst MovZ;
+    MovZ.setOpcode(AArch64::MOVZXi);
+    MovZ.addOperand(MCOperand::createReg(DestReg));
+    MovZ.addOperand(Hi_MCSym);
+    MovZ.addOperand(MCOperand::createImm(16));
+    EmitToStreamer(*OutStreamer, MovZ);
+
+    MCInst MovK;
+    MovK.setOpcode(AArch64::MOVKXi);
+    MovK.addOperand(MCOperand::createReg(DestReg));
+    MovK.addOperand(MCOperand::createReg(DestReg));
+    MovK.addOperand(Lo_MCSym);
+    MovK.addOperand(MCOperand::createImm(0));
+    EmitToStreamer(*OutStreamer, MovK);
+    return;
   }
   case AArch64::MOVIv2d_ns:
     // It is generally beneficial to rewrite "fmov s0, wzr" to "movi d0, #0".
@@ -2720,16 +2729,16 @@ void AArch64AsmPrinter::emitInstruction(const MachineInstr *MI) {
     return;
 
   case AArch64::EMITBKEY: {
-      ExceptionHandling ExceptionHandlingType = MAI->getExceptionHandlingType();
-      if (ExceptionHandlingType != ExceptionHandling::DwarfCFI &&
-          ExceptionHandlingType != ExceptionHandling::ARM)
-        return;
-
-      if (getFunctionCFISectionType(*MF) == CFISection::None)
-        return;
+    ExceptionHandling ExceptionHandlingType = MAI->getExceptionHandlingType();
+    if (ExceptionHandlingType != ExceptionHandling::DwarfCFI &&
+        ExceptionHandlingType != ExceptionHandling::ARM)
+      return;
 
-      OutStreamer->emitCFIBKeyFrame();
+    if (getFunctionCFISectionType(*MF) == CFISection::None)
       return;
+
+    OutStreamer->emitCFIBKeyFrame();
+    return;
   }
 
   case AArch64::EMITMTETAGGED: {
@@ -3063,14 +3072,14 @@ void AArch64AsmPrinter::emitInstruction(const MachineInstr *MI) {
       return;
     }
     assert((MI->getOperand(1).getImm() - MI->getOperand(0).getImm() == 1) &&
-            "Non-consecutive registers not allowed for save_regp");
+           "Non-consecutive registers not allowed for save_regp");
     TS->emitARM64WinCFISaveRegP(MI->getOperand(0).getImm(),
                                 MI->getOperand(2).getImm());
     return;
 
   case AArch64::SEH_SaveRegP_X:
     assert((MI->getOperand(1).getImm() - MI->getOperand(0).getImm() == 1) &&
-            "Non-consecutive registers not allowed for save_regp_x");
+           "Non-consecutive registers not allowed for save_regp_x");
     assert(MI->getOperand(2).getImm() < 0 &&
            "Pre increment SEH opcode must have a negative offset");
     TS->emitARM64WinCFISaveRegPX(MI->getOperand(0).getImm(),
@@ -3091,14 +3100,14 @@ void AArch64AsmPrinter::emitInstruction(const MachineInstr *MI) {
 
   case AArch64::SEH_SaveFRegP:
     assert((MI->getOperand(1).getImm() - MI->getOperand(0).getImm() == 1) &&
-            "Non-consecutive registers not allowed for save_regp");
+           "Non-consecutive registers not allowed for save_regp");
     TS->emitARM64WinCFISaveFRegP(MI->getOperand(0).getImm(),
                                  MI->getOperand(2).getImm());
     return;
 
   case AArch64::SEH_SaveFRegP_X:
     assert((MI->getOperand(1).getImm() - MI->getOperand(0).getImm() == 1) &&
-            "Non-consecutive registers not allowed for save_regp_x");
+           "Non-consecutive registers not allowed for save_regp_x");
     assert(MI->getOperand(2).getImm() < 0 &&
            "Pre increment SEH opcode must have a negative offset");
     TS->emitARM64WinCFISaveFRegPX(MI->getOperand(0).getImm(),
diff --git a/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp b/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
index d3abd79b8..a1bf31385 100644
--- a/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
@@ -929,25 +929,25 @@ static MCRegister getRegisterOrZero(MCRegister Reg, bool HasSVE) {
   case AArch64::W##n:                                                          \
   case AArch64::X##n:                                                          \
     return AArch64::X##n
-  CASE(0);
-  CASE(1);
-  CASE(2);
-  CASE(3);
-  CASE(4);
-  CASE(5);
-  CASE(6);
-  CASE(7);
-  CASE(8);
-  CASE(9);
-  CASE(10);
-  CASE(11);
-  CASE(12);
-  CASE(13);
-  CASE(14);
-  CASE(15);
-  CASE(16);
-  CASE(17);
-  CASE(18);
+    CASE(0);
+    CASE(1);
+    CASE(2);
+    CASE(3);
+    CASE(4);
+    CASE(5);
+    CASE(6);
+    CASE(7);
+    CASE(8);
+    CASE(9);
+    CASE(10);
+    CASE(11);
+    CASE(12);
+    CASE(13);
+    CASE(14);
+    CASE(15);
+    CASE(16);
+    CASE(17);
+    CASE(18);
 #undef CASE
 
     // FPRs
@@ -958,38 +958,38 @@ static MCRegister getRegisterOrZero(MCRegister Reg, bool HasSVE) {
   case AArch64::D##n:                                                          \
   case AArch64::Q##n:                                                          \
     return HasSVE ? AArch64::Z##n : AArch64::Q##n
-  CASE(0);
-  CASE(1);
-  CASE(2);
-  CASE(3);
-  CASE(4);
-  CASE(5);
-  CASE(6);
-  CASE(7);
-  CASE(8);
-  CASE(9);
-  CASE(10);
-  CASE(11);
-  CASE(12);
-  CASE(13);
-  CASE(14);
-  CASE(15);
-  CASE(16);
-  CASE(17);
-  CASE(18);
-  CASE(19);
-  CASE(20);
-  CASE(21);
-  CASE(22);
-  CASE(23);
-  CASE(24);
-  CASE(25);
-  CASE(26);
-  CASE(27);
-  CASE(28);
-  CASE(29);
-  CASE(30);
-  CASE(31);
+    CASE(0);
+    CASE(1);
+    CASE(2);
+    CASE(3);
+    CASE(4);
+    CASE(5);
+    CASE(6);
+    CASE(7);
+    CASE(8);
+    CASE(9);
+    CASE(10);
+    CASE(11);
+    CASE(12);
+    CASE(13);
+    CASE(14);
+    CASE(15);
+    CASE(16);
+    CASE(17);
+    CASE(18);
+    CASE(19);
+    CASE(20);
+    CASE(21);
+    CASE(22);
+    CASE(23);
+    CASE(24);
+    CASE(25);
+    CASE(26);
+    CASE(27);
+    CASE(28);
+    CASE(29);
+    CASE(30);
+    CASE(31);
 #undef CASE
   }
 }
@@ -1290,7 +1290,7 @@ static MachineBasicBlock::iterator InsertSEH(MachineBasicBlock::iterator MBBI,
     Imm = -Imm;
     [[fallthrough]];
   case AArch64::STRXpre: {
-    unsigned Reg =  RegInfo->getSEHRegNum(MBBI->getOperand(1).getReg());
+    unsigned Reg = RegInfo->getSEHRegNum(MBBI->getOperand(1).getReg());
     MIB = BuildMI(MF, DL, TII.get(AArch64::SEH_SaveReg_X))
               .addImm(Reg)
               .addImm(Imm)
@@ -1299,8 +1299,8 @@ static MachineBasicBlock::iterator InsertSEH(MachineBasicBlock::iterator MBBI,
   }
   case AArch64::STPDi:
   case AArch64::LDPDi: {
-    unsigned Reg0 =  RegInfo->getSEHRegNum(MBBI->getOperand(0).getReg());
-    unsigned Reg1 =  RegInfo->getSEHRegNum(MBBI->getOperand(1).getReg());
+    unsigned Reg0 = RegInfo->getSEHRegNum(MBBI->getOperand(0).getReg());
+    unsigned Reg1 = RegInfo->getSEHRegNum(MBBI->getOperand(1).getReg());
     MIB = BuildMI(MF, DL, TII.get(AArch64::SEH_SaveFRegP))
               .addImm(Reg0)
               .addImm(Reg1)
@@ -2087,9 +2087,12 @@ void AArch64FrameLowering::emitPrologue(MachineFunction &MF,
       BuildMI(MBB, MBBI, DL, TII->get(AArch64::BL))
           .addExternalSymbol(ChkStk)
           .addReg(AArch64::X15, RegState::Implicit)
-          .addReg(AArch64::X16, RegState::Implicit | RegState::Define | RegState::Dead)
-          .addReg(AArch64::X17, RegState::Implicit | RegState::Define | RegState::Dead)
-          .addReg(AArch64::NZCV, RegState::Implicit | RegState::Define | RegState::Dead)
+          .addReg(AArch64::X16,
+                  RegState::Implicit | RegState::Define | RegState::Dead)
+          .addReg(AArch64::X17,
+                  RegState::Implicit | RegState::Define | RegState::Dead)
+          .addReg(AArch64::NZCV,
+                  RegState::Implicit | RegState::Define | RegState::Dead)
           .setMIFlags(MachineInstr::FrameSetup);
       if (NeedsWinCFI) {
         HasWinCFI = true;
@@ -2112,9 +2115,12 @@ void AArch64FrameLowering::emitPrologue(MachineFunction &MF,
       BuildMI(MBB, MBBI, DL, TII->get(getBLRCallOpcode(MF)))
           .addReg(AArch64::X16, RegState::Kill)
           .addReg(AArch64::X15, RegState::Implicit | RegState::Define)
-          .addReg(AArch64::X16, RegState::Implicit | RegState::Define | RegState::Dead)
-          .addReg(AArch64::X17, RegState::Implicit | RegState::Define | RegState::Dead)
-          .addReg(AArch64::NZCV, RegState::Implicit | RegState::Define | RegState::Dead)
+          .addReg(AArch64::X16,
+                  RegState::Implicit | RegState::Define | RegState::Dead)
+          .addReg(AArch64::X17,
+                  RegState::Implicit | RegState::Define | RegState::Dead)
+          .addReg(AArch64::NZCV,
+                  RegState::Implicit | RegState::Define | RegState::Dead)
           .setMIFlags(MachineInstr::FrameSetup);
       if (NeedsWinCFI) {
         HasWinCFI = true;
@@ -2325,8 +2331,8 @@ void AArch64FrameLowering::emitEpilogue(MachineFunction &MF,
     }
   });
 
-  int64_t NumBytes = IsFunclet ? getWinEHFuncletFrameSize(MF)
-                               : MFI.getStackSize();
+  int64_t NumBytes =
+      IsFunclet ? getWinEHFuncletFrameSize(MF) : MFI.getStackSize();
 
   // All calls are tail calls in GHC calling conv, and functions have no
   // prologue/epilogue.
@@ -2789,7 +2795,7 @@ StackOffset AArch64FrameLowering::resolveFrameOffsetReference(
         // Funclets access the locals contained in the parent's stack frame
         // via the frame pointer, so we have to use the FP in the parent
         // function.
-        (void) Subtarget;
+        (void)Subtarget;
         assert(Subtarget.isCallingConvWin64(MF.getFunction().getCallingConv(),
                                             MF.getFunction().isVarArg()) &&
                "Funclets should only be present on Win64");
@@ -2808,8 +2814,8 @@ StackOffset AArch64FrameLowering::resolveFrameOffsetReference(
       "non-argument/CSR objects cannot be accessed through the frame pointer");
 
   if (isSVE) {
-    StackOffset FPOffset =
-        StackOffset::get(-AFI->getCalleeSaveBaseToFrameRecordOffset(), ObjectOffset);
+    StackOffset FPOffset = StackOffset::get(
+        -AFI->getCalleeSaveBaseToFrameRecordOffset(), ObjectOffset);
     StackOffset SPOffset =
         SVEStackSize +
         StackOffset::get(MFI.getStackSize() - AFI->getCalleeSavedStackSize(),
@@ -3353,7 +3359,7 @@ bool AArch64FrameLowering::spillCalleeSavedRegisters(
 
     if (RPI.isPaired() && RPI.isScalable()) {
       [[maybe_unused]] const AArch64Subtarget &Subtarget =
-                              MF.getSubtarget<AArch64Subtarget>();
+          MF.getSubtarget<AArch64Subtarget>();
       AArch64FunctionInfo *AFI = MF.getInfo<AArch64FunctionInfo>();
       unsigned PnReg = AFI->getPredicateRegForFillSpill();
       assert((PnReg != 0 && enableMultiVectorSpillFill(Subtarget, MF)) &&
@@ -3523,7 +3529,7 @@ bool AArch64FrameLowering::restoreCalleeSavedRegisters(
     AArch64FunctionInfo *AFI = MF.getInfo<AArch64FunctionInfo>();
     if (RPI.isPaired() && RPI.isScalable()) {
       [[maybe_unused]] const AArch64Subtarget &Subtarget =
-                              MF.getSubtarget<AArch64Subtarget>();
+          MF.getSubtarget<AArch64Subtarget>();
       unsigned PnReg = AFI->getPredicateRegForFillSpill();
       assert((PnReg != 0 && enableMultiVectorSpillFill(Subtarget, MF)) &&
              "Expects SVE2.1 or SME2 target and a predicate register");
@@ -4056,8 +4062,8 @@ bool AArch64FrameLowering::enableStackSlotScavenging(
 }
 
 /// returns true if there are any SVE callee saves.
-static bool getSVECalleeSaveSlotRange(const MachineFrameInfo &MFI,
-                                      int &Min, int &Max) {
+static bool getSVECalleeSaveSlotRange(const MachineFrameInfo &MFI, int &Min,
+                                      int &Max) {
   Min = std::numeric_limits<int>::max();
   Max = std::numeric_limits<int>::min();
 
@@ -4163,7 +4169,8 @@ static int64_t determineSVEStackObjectOffsets(MachineFrameInfo &MFI,
 int64_t AArch64FrameLowering::estimateSVEStackObjectOffsets(
     MachineFrameInfo &MFI) const {
   int MinCSFrameIndex, MaxCSFrameIndex;
-  return determineSVEStackObjectOffsets(MFI, MinCSFrameIndex, MaxCSFrameIndex, false);
+  return determineSVEStackObjectOffsets(MFI, MinCSFrameIndex, MaxCSFrameIndex,
+                                        false);
 }
 
 int64_t AArch64FrameLowering::assignSVEStackObjectOffsets(
@@ -4607,10 +4614,9 @@ void TagStoreEdit::emitUnrolled(MachineBasicBlock::iterator InsertI) {
   MachineInstr *LastI = nullptr;
   while (Size) {
     int64_t InstrSize = (Size > 16) ? 32 : 16;
-    unsigned Opcode =
-        InstrSize == 16
-            ? (ZeroData ? AArch64::STZGi : AArch64::STGi)
-            : (ZeroData ? AArch64::STZ2Gi : AArch64::ST2Gi);
+    unsigned Opcode = InstrSize == 16
+                          ? (ZeroData ? AArch64::STZGi : AArch64::STGi)
+                          : (ZeroData ? AArch64::STZ2Gi : AArch64::ST2Gi);
     assert(BaseRegOffsetBytes % 16 == 0);
     MachineInstr *I = BuildMI(*MBB, InsertI, DL, TII->get(Opcode))
                           .addReg(AArch64::SP)
diff --git a/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp b/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
index 0f2b969fb..2ab358b30 100644
--- a/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
+++ b/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
@@ -1064,29 +1064,29 @@ bool AArch64InstrInfo::isFalkorShiftExtFast(const MachineInstr &MI) {
 bool AArch64InstrInfo::isSEHInstruction(const MachineInstr &MI) {
   unsigned Opc = MI.getOpcode();
   switch (Opc) {
-    default:
-      return false;
-    case AArch64::SEH_StackAlloc:
-    case AArch64::SEH_SaveFPLR:
-    case AArch64::SEH_SaveFPLR_X:
-    case AArch64::SEH_SaveReg:
-    case AArch64::SEH_SaveReg_X:
-    case AArch64::SEH_SaveRegP:
-    case AArch64::SEH_SaveRegP_X:
-    case AArch64::SEH_SaveFReg:
-    case AArch64::SEH_SaveFReg_X:
-    case AArch64::SEH_SaveFRegP:
-    case AArch64::SEH_SaveFRegP_X:
-    case AArch64::SEH_SetFP:
-    case AArch64::SEH_AddFP:
-    case AArch64::SEH_Nop:
-    case AArch64::SEH_PrologEnd:
-    case AArch64::SEH_EpilogStart:
-    case AArch64::SEH_EpilogEnd:
-    case AArch64::SEH_PACSignLR:
-    case AArch64::SEH_SaveAnyRegQP:
-    case AArch64::SEH_SaveAnyRegQPX:
-      return true;
+  default:
+    return false;
+  case AArch64::SEH_StackAlloc:
+  case AArch64::SEH_SaveFPLR:
+  case AArch64::SEH_SaveFPLR_X:
+  case AArch64::SEH_SaveReg:
+  case AArch64::SEH_SaveReg_X:
+  case AArch64::SEH_SaveRegP:
+  case AArch64::SEH_SaveRegP_X:
+  case AArch64::SEH_SaveFReg:
+  case AArch64::SEH_SaveFReg_X:
+  case AArch64::SEH_SaveFRegP:
+  case AArch64::SEH_SaveFRegP_X:
+  case AArch64::SEH_SetFP:
+  case AArch64::SEH_AddFP:
+  case AArch64::SEH_Nop:
+  case AArch64::SEH_PrologEnd:
+  case AArch64::SEH_EpilogStart:
+  case AArch64::SEH_EpilogEnd:
+  case AArch64::SEH_PACSignLR:
+  case AArch64::SEH_SaveAnyRegQP:
+  case AArch64::SEH_SaveAnyRegQPX:
+    return true;
   }
 }
 
@@ -1238,8 +1238,8 @@ bool AArch64InstrInfo::analyzeCompare(const MachineInstr &MI, Register &SrcReg,
     SrcReg2 = 0;
     CmpMask = ~0;
     CmpValue = AArch64_AM::decodeLogicalImmediate(
-                   MI.getOperand(2).getImm(),
-                   MI.getOpcode() == AArch64::ANDSWri ? 32 : 64);
+        MI.getOperand(2).getImm(),
+        MI.getOpcode() == AArch64::ANDSWri ? 32 : 64);
     return true;
   }
 
@@ -1988,8 +1988,7 @@ bool AArch64InstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
 
   if (MI.getOpcode() == AArch64::CATCHRET) {
     // Skip to the first instruction before the epilog.
-    const TargetInstrInfo *TII =
-      MBB.getParent()->getSubtarget().getInstrInfo();
+    const TargetInstrInfo *TII = MBB.getParent()->getSubtarget().getInstrInfo();
     MachineBasicBlock *TargetMBB = MI.getOperand(0).getMBB();
     auto MBBI = MachineBasicBlock::iterator(MI);
     MachineBasicBlock::iterator FirstEpilogSEH = std::prev(MBBI);
@@ -2326,31 +2325,56 @@ bool AArch64InstrInfo::hasUnscaledLdStOffset(unsigned Opc) {
 
 std::optional<unsigned> AArch64InstrInfo::getUnscaledLdSt(unsigned Opc) {
   switch (Opc) {
-  default: return {};
-  case AArch64::PRFMui: return AArch64::PRFUMi;
-  case AArch64::LDRXui: return AArch64::LDURXi;
-  case AArch64::LDRWui: return AArch64::LDURWi;
-  case AArch64::LDRBui: return AArch64::LDURBi;
-  case AArch64::LDRHui: return AArch64::LDURHi;
-  case AArch64::LDRSui: return AArch64::LDURSi;
-  case AArch64::LDRDui: return AArch64::LDURDi;
-  case AArch64::LDRQui: return AArch64::LDURQi;
-  case AArch64::LDRBBui: return AArch64::LDURBBi;
-  case AArch64::LDRHHui: return AArch64::LDURHHi;
-  case AArch64::LDRSBXui: return AArch64::LDURSBXi;
-  case AArch64::LDRSBWui: return AArch64::LDURSBWi;
-  case AArch64::LDRSHXui: return AArch64::LDURSHXi;
-  case AArch64::LDRSHWui: return AArch64::LDURSHWi;
-  case AArch64::LDRSWui: return AArch64::LDURSWi;
-  case AArch64::STRXui: return AArch64::STURXi;
-  case AArch64::STRWui: return AArch64::STURWi;
-  case AArch64::STRBui: return AArch64::STURBi;
-  case AArch64::STRHui: return AArch64::STURHi;
-  case AArch64::STRSui: return AArch64::STURSi;
-  case AArch64::STRDui: return AArch64::STURDi;
-  case AArch64::STRQui: return AArch64::STURQi;
-  case AArch64::STRBBui: return AArch64::STURBBi;
-  case AArch64::STRHHui: return AArch64::STURHHi;
+  default:
+    return {};
+  case AArch64::PRFMui:
+    return AArch64::PRFUMi;
+  case AArch64::LDRXui:
+    return AArch64::LDURXi;
+  case AArch64::LDRWui:
+    return AArch64::LDURWi;
+  case AArch64::LDRBui:
+    return AArch64::LDURBi;
+  case AArch64::LDRHui:
+    return AArch64::LDURHi;
+  case AArch64::LDRSui:
+    return AArch64::LDURSi;
+  case AArch64::LDRDui:
+    return AArch64::LDURDi;
+  case AArch64::LDRQui:
+    return AArch64::LDURQi;
+  case AArch64::LDRBBui:
+    return AArch64::LDURBBi;
+  case AArch64::LDRHHui:
+    return AArch64::LDURHHi;
+  case AArch64::LDRSBXui:
+    return AArch64::LDURSBXi;
+  case AArch64::LDRSBWui:
+    return AArch64::LDURSBWi;
+  case AArch64::LDRSHXui:
+    return AArch64::LDURSHXi;
+  case AArch64::LDRSHWui:
+    return AArch64::LDURSHWi;
+  case AArch64::LDRSWui:
+    return AArch64::LDURSWi;
+  case AArch64::STRXui:
+    return AArch64::STURXi;
+  case AArch64::STRWui:
+    return AArch64::STURWi;
+  case AArch64::STRBui:
+    return AArch64::STURBi;
+  case AArch64::STRHui:
+    return AArch64::STURHi;
+  case AArch64::STRSui:
+    return AArch64::STURSi;
+  case AArch64::STRDui:
+    return AArch64::STURDi;
+  case AArch64::STRQui:
+    return AArch64::STURQi;
+  case AArch64::STRBBui:
+    return AArch64::STURBBi;
+  case AArch64::STRHHui:
+    return AArch64::STURHHi;
   }
 }
 
@@ -2822,8 +2846,8 @@ bool AArch64InstrInfo::isCandidateToMergeOrPair(const MachineInstr &MI) const {
 
   // Do not pair any callee-save store/reload instructions in the
   // prologue/epilogue if the CFI information encoded the operations as separate
-  // instructions, as that will cause the size of the actual prologue to mismatch
-  // with the prologue size recorded in the Windows CFI.
+  // instructions, as that will cause the size of the actual prologue to
+  // mismatch with the prologue size recorded in the Windows CFI.
   const MCAsmInfo *MAI = MI.getMF()->getTarget().getMCAsmInfo();
   bool NeedsWinCFI = MAI->usesWindowsCFI() &&
                      MI.getMF()->getFunction().needsUnwindTableEntry();
@@ -4592,8 +4616,7 @@ bool AArch64InstrInfo::isHForm(const MachineInstr &MI) {
     if (Reg.isPhysical())
       return AArch64::FPR16RegClass.contains(Reg);
     const TargetRegisterClass *TRC = ::getRegClass(MI, Reg);
-    return TRC == &AArch64::FPR16RegClass ||
-           TRC == &AArch64::FPR16_loRegClass;
+    return TRC == &AArch64::FPR16RegClass || TRC == &AArch64::FPR16_loRegClass;
   };
   return llvm::any_of(MI.operands(), IsHFPR);
 }
@@ -4966,9 +4989,9 @@ void AArch64InstrInfo::copyPhysReg(MachineBasicBlock &MBB,
     assert(Subtarget.isSVEorStreamingSVEAvailable() &&
            "Unexpected SVE register.");
     BuildMI(MBB, I, DL, get(AArch64::ORR_PPzPP), DestReg)
-      .addReg(SrcReg) // Pg
-      .addReg(SrcReg)
-      .addReg(SrcReg, getKillRegState(KillSrc));
+        .addReg(SrcReg) // Pg
+        .addReg(SrcReg)
+        .addReg(SrcReg, getKillRegState(KillSrc));
     return;
   }
 
@@ -5000,8 +5023,8 @@ void AArch64InstrInfo::copyPhysReg(MachineBasicBlock &MBB,
     assert(Subtarget.isSVEorStreamingSVEAvailable() &&
            "Unexpected SVE register.");
     BuildMI(MBB, I, DL, get(AArch64::ORR_ZZZ), DestReg)
-      .addReg(SrcReg)
-      .addReg(SrcReg, getKillRegState(KillSrc));
+        .addReg(SrcReg)
+        .addReg(SrcReg, getKillRegState(KillSrc));
     return;
   }
 
@@ -5257,9 +5280,9 @@ void AArch64InstrInfo::copyPhysReg(MachineBasicBlock &MBB,
 static void storeRegPairToStackSlot(const TargetRegisterInfo &TRI,
                                     MachineBasicBlock &MBB,
                                     MachineBasicBlock::iterator InsertBefore,
-                                    const MCInstrDesc &MCID,
-                                    Register SrcReg, bool IsKill,
-                                    unsigned SubIdx0, unsigned SubIdx1, int FI,
+                                    const MCInstrDesc &MCID, Register SrcReg,
+                                    bool IsKill, unsigned SubIdx0,
+                                    unsigned SubIdx1, int FI,
                                     MachineMemOperand *MMO) {
   Register SrcReg0 = SrcReg;
   Register SrcReg1 = SrcReg;
@@ -5336,9 +5359,9 @@ void AArch64InstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
     } else if (AArch64::FPR64RegClass.hasSubClassEq(RC)) {
       Opc = AArch64::STRDui;
     } else if (AArch64::WSeqPairsClassRegClass.hasSubClassEq(RC)) {
-      storeRegPairToStackSlot(getRegisterInfo(), MBB, MBBI,
-                              get(AArch64::STPWi), SrcReg, isKill,
-                              AArch64::sube32, AArch64::subo32, FI, MMO);
+      storeRegPairToStackSlot(getRegisterInfo(), MBB, MBBI, get(AArch64::STPWi),
+                              SrcReg, isKill, AArch64::sube32, AArch64::subo32,
+                              FI, MMO);
       return;
     }
     break;
@@ -5350,9 +5373,9 @@ void AArch64InstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
       Opc = AArch64::ST1Twov1d;
       Offset = false;
     } else if (AArch64::XSeqPairsClassRegClass.hasSubClassEq(RC)) {
-      storeRegPairToStackSlot(getRegisterInfo(), MBB, MBBI,
-                              get(AArch64::STPXi), SrcReg, isKill,
-                              AArch64::sube64, AArch64::subo64, FI, MMO);
+      storeRegPairToStackSlot(getRegisterInfo(), MBB, MBBI, get(AArch64::STPXi),
+                              SrcReg, isKill, AArch64::sube64, AArch64::subo64,
+                              FI, MMO);
       return;
     } else if (AArch64::ZPRRegClass.hasSubClassEq(RC)) {
       assert(Subtarget.isSVEorStreamingSVEAvailable() &&
@@ -5433,9 +5456,8 @@ void AArch64InstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
 static void loadRegPairFromStackSlot(const TargetRegisterInfo &TRI,
                                      MachineBasicBlock &MBB,
                                      MachineBasicBlock::iterator InsertBefore,
-                                     const MCInstrDesc &MCID,
-                                     Register DestReg, unsigned SubIdx0,
-                                     unsigned SubIdx1, int FI,
+                                     const MCInstrDesc &MCID, Register DestReg,
+                                     unsigned SubIdx0, unsigned SubIdx1, int FI,
                                      MachineMemOperand *MMO) {
   Register DestReg0 = DestReg;
   Register DestReg1 = DestReg;
@@ -5949,9 +5971,9 @@ void llvm::emitFrameOffset(MachineBasicBlock &MBB,
 
   if (NumDataVectors) {
     emitFrameOffsetAdj(MBB, MBBI, DL, DestReg, SrcReg, NumDataVectors,
-                       UseSVL ? AArch64::ADDSVL_XXI : AArch64::ADDVL_XXI,
-                       TII, Flag, NeedsWinCFI, nullptr, EmitCFAOffset,
-                       CFAOffset, FrameReg);
+                       UseSVL ? AArch64::ADDSVL_XXI : AArch64::ADDVL_XXI, TII,
+                       Flag, NeedsWinCFI, nullptr, EmitCFAOffset, CFAOffset,
+                       FrameReg);
     CFAOffset += StackOffset::getScalable(-NumDataVectors * 16);
     SrcReg = DestReg;
   }
@@ -5959,16 +5981,16 @@ void llvm::emitFrameOffset(MachineBasicBlock &MBB,
   if (NumPredicateVectors) {
     assert(DestReg != AArch64::SP && "Unaligned access to SP");
     emitFrameOffsetAdj(MBB, MBBI, DL, DestReg, SrcReg, NumPredicateVectors,
-                       UseSVL ? AArch64::ADDSPL_XXI : AArch64::ADDPL_XXI,
-                       TII, Flag, NeedsWinCFI, nullptr, EmitCFAOffset,
-                       CFAOffset, FrameReg);
+                       UseSVL ? AArch64::ADDSPL_XXI : AArch64::ADDPL_XXI, TII,
+                       Flag, NeedsWinCFI, nullptr, EmitCFAOffset, CFAOffset,
+                       FrameReg);
   }
 }
 
 MachineInstr *AArch64InstrInfo::foldMemoryOperandImpl(
     MachineFunction &MF, MachineInstr &MI, ArrayRef<unsigned> Ops,
-    MachineBasicBlock::iterator InsertPt, int FrameIndex,
-    LiveIntervals *LIS, VirtRegMap *VRM) const {
+    MachineBasicBlock::iterator InsertPt, int FrameIndex, LiveIntervals *LIS,
+    VirtRegMap *VRM) const {
   // This is a bit of a hack. Consider this instruction:
   //
   //   %0 = COPY %sp; GPR64all:%0
@@ -6187,9 +6209,8 @@ int llvm::isAArch64FrameOffsetLegal(const MachineInstr &MI,
   std::optional<unsigned> UnscaledOp =
       AArch64InstrInfo::getUnscaledLdSt(MI.getOpcode());
   bool useUnscaledOp = UnscaledOp && (Offset % Scale || Offset < 0);
-  if (useUnscaledOp &&
-      !AArch64InstrInfo::getMemOpInfo(*UnscaledOp, ScaleValue, Width, MinOff,
-                                      MaxOff))
+  if (useUnscaledOp && !AArch64InstrInfo::getMemOpInfo(*UnscaledOp, ScaleValue,
+                                                       Width, MinOff, MaxOff))
     llvm_unreachable("unhandled opcode in isAArch64FrameOffsetLegal");
 
   Scale = ScaleValue.getKnownMinValue();
@@ -6704,7 +6725,7 @@ static bool getFMAPatterns(MachineInstr &Root,
     assert(Root.getOperand(1).isReg() && Root.getOperand(2).isReg() &&
            "FADDHrr does not have register operands");
 
-    Found  = Match(AArch64::FMULHrr, 1, MCP::FMULADDH_OP1);
+    Found = Match(AArch64::FMULHrr, 1, MCP::FMULADDH_OP1);
     Found |= Match(AArch64::FMULHrr, 2, MCP::FMULADDH_OP2);
     break;
   case AArch64::FADDSrr:
@@ -6760,7 +6781,7 @@ static bool getFMAPatterns(MachineInstr &Root,
              Match(AArch64::FMULv4f32, 2, MCP::FMLAv4f32_OP2);
     break;
   case AArch64::FSUBHrr:
-    Found  = Match(AArch64::FMULHrr, 1, MCP::FMULSUBH_OP1);
+    Found = Match(AArch64::FMULHrr, 1, MCP::FMULSUBH_OP1);
     Found |= Match(AArch64::FMULHrr, 2, MCP::FMULSUBH_OP2);
     Found |= Match(AArch64::FNMULHrr, 1, MCP::FNMULSUBH_OP1);
     break;
@@ -7384,13 +7405,12 @@ static MachineInstr *genMaddR(MachineFunction &MF, MachineRegisterInfo &MRI,
 /// Do the following transformation
 /// A - (B + C)  ==>   (A - B) - C
 /// A - (B + C)  ==>   (A - C) - B
-static void
-genSubAdd2SubSub(MachineFunction &MF, MachineRegisterInfo &MRI,
-                 const TargetInstrInfo *TII, MachineInstr &Root,
-                 SmallVectorImpl<MachineInstr *> &InsInstrs,
-                 SmallVectorImpl<MachineInstr *> &DelInstrs,
-                 unsigned IdxOpd1,
-                 DenseMap<unsigned, unsigned> &InstrIdxForVirtReg) {
+static void genSubAdd2SubSub(MachineFunction &MF, MachineRegisterInfo &MRI,
+                             const TargetInstrInfo *TII, MachineInstr &Root,
+                             SmallVectorImpl<MachineInstr *> &InsInstrs,
+                             SmallVectorImpl<MachineInstr *> &DelInstrs,
+                             unsigned IdxOpd1,
+                             DenseMap<unsigned, unsigned> &InstrIdxForVirtReg) {
   assert(IdxOpd1 == 1 || IdxOpd1 == 2);
   unsigned IdxOtherOpd = IdxOpd1 == 1 ? 2 : 1;
   MachineInstr *AddMI = MRI.getUniqueVRegDef(Root.getOperand(2).getReg());
@@ -8720,8 +8740,10 @@ outliningCandidatesSigningScopeConsensus(const outliner::Candidate &a,
   const auto &MFIa = a.getMF()->getInfo<AArch64FunctionInfo>();
   const auto &MFIb = b.getMF()->getInfo<AArch64FunctionInfo>();
 
-  return MFIa->shouldSignReturnAddress(false) == MFIb->shouldSignReturnAddress(false) &&
-         MFIa->shouldSignReturnAddress(true) == MFIb->shouldSignReturnAddress(true);
+  return MFIa->shouldSignReturnAddress(false) ==
+             MFIb->shouldSignReturnAddress(false) &&
+         MFIa->shouldSignReturnAddress(true) ==
+             MFIb->shouldSignReturnAddress(true);
 }
 
 static bool
@@ -8943,7 +8965,7 @@ AArch64InstrInfo::getOutliningCandidateInfo(
       // Find the minimum/maximum offset for this instruction and check
       // if fixing it up would be in range.
       int64_t MinOffset,
-          MaxOffset;  // Unscaled offsets for the instruction.
+          MaxOffset; // Unscaled offsets for the instruction.
       // The scale to multiply the offsets by.
       TypeSize Scale(0U, false), DummyWidth(0U, false);
       getMemOpInfo(MI.getOpcode(), Scale, DummyWidth, MinOffset, MaxOffset);
@@ -9427,8 +9449,8 @@ AArch64InstrInfo::getOutliningTypeImpl(const MachineModuleInfo &MMI,
     // as a tail-call. Explicitly list the call instructions we know about so we
     // don't get unexpected results with call pseudo-instructions.
     auto UnknownCallOutlineType = outliner::InstrType::Illegal;
-    if (MI.getOpcode() == AArch64::BLR ||
-        MI.getOpcode() == AArch64::BLRNoIP || MI.getOpcode() == AArch64::BL)
+    if (MI.getOpcode() == AArch64::BLR || MI.getOpcode() == AArch64::BLRNoIP ||
+        MI.getOpcode() == AArch64::BL)
       UnknownCallOutlineType = outliner::InstrType::LegalTerminator;
 
     if (!Callee)
@@ -9625,8 +9647,8 @@ void AArch64InstrInfo::buildOutlinedFrame(
   if (!MBB.isLiveIn(AArch64::LR))
     MBB.addLiveIn(AArch64::LR);
 
-  MachineInstr *ret = BuildMI(MF, DebugLoc(), get(AArch64::RET))
-                          .addReg(AArch64::LR);
+  MachineInstr *ret =
+      BuildMI(MF, DebugLoc(), get(AArch64::RET)).addReg(AArch64::LR);
   MBB.insert(MBB.end(), ret);
 
   signOutlinedFunction(MF, MBB, this, ShouldSignReturnAddr);
@@ -9688,9 +9710,9 @@ MachineBasicBlock::iterator AArch64InstrInfo::insertOutlinedCall(
                .addReg(AArch64::LR)
                .addImm(0);
     Restore = BuildMI(MF, DebugLoc(), get(AArch64::ORRXrs), AArch64::LR)
-                .addReg(AArch64::XZR)
-                .addReg(Reg)
-                .addImm(0);
+                  .addReg(AArch64::XZR)
+                  .addReg(Reg)
+                  .addImm(0);
   } else {
     // We have the default case. Save and restore from SP.
     Save = BuildMI(MF, DebugLoc(), get(AArch64::STRXpre))
@@ -9719,7 +9741,7 @@ MachineBasicBlock::iterator AArch64InstrInfo::insertOutlinedCall(
 }
 
 bool AArch64InstrInfo::shouldOutlineFromFunctionByDefault(
-  MachineFunction &MF) const {
+    MachineFunction &MF) const {
   return MF.getFunction().hasMinSize();
 }
 
@@ -9734,12 +9756,9 @@ void AArch64InstrInfo::buildClearRegister(Register Reg, MachineBasicBlock &MBB,
   if (TRI.isGeneralPurposeRegister(MF, Reg)) {
     BuildMI(MBB, Iter, DL, get(AArch64::MOVZXi), Reg).addImm(0).addImm(0);
   } else if (STI.isSVEorStreamingSVEAvailable()) {
-    BuildMI(MBB, Iter, DL, get(AArch64::DUP_ZI_D), Reg)
-      .addImm(0)
-      .addImm(0);
+    BuildMI(MBB, Iter, DL, get(AArch64::DUP_ZI_D), Reg).addImm(0).addImm(0);
   } else if (STI.isNeonAvailable()) {
-    BuildMI(MBB, Iter, DL, get(AArch64::MOVIv2d_ns), Reg)
-      .addImm(0);
+    BuildMI(MBB, Iter, DL, get(AArch64::MOVIv2d_ns), Reg).addImm(0);
   } else {
     // This is a streaming-compatible function without SVE. We don't have full
     // Neon (just FPRs), so we can at most use the first 64-bit sub-register.
diff --git a/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp b/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
index 07f072446..5d3e48eb8 100644
--- a/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
+++ b/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
@@ -119,9 +119,9 @@ static cl::opt<bool> EnableAtomicTidy(
     cl::init(true));
 
 static cl::opt<bool>
-EnableEarlyIfConversion("aarch64-enable-early-ifcvt", cl::Hidden,
-                        cl::desc("Run early if-conversion"),
-                        cl::init(true));
+    EnableEarlyIfConversion("aarch64-enable-early-ifcvt", cl::Hidden,
+                            cl::desc("Run early if-conversion"),
+                            cl::init(true));
 
 static cl::opt<bool>
     EnableCondOpt("aarch64-enable-condopt",
@@ -480,7 +480,7 @@ AArch64TargetMachine::getSubtargetImpl(const Function &F) const {
   return I.get();
 }
 
-void AArch64leTargetMachine::anchor() { }
+void AArch64leTargetMachine::anchor() {}
 
 AArch64leTargetMachine::AArch64leTargetMachine(
     const Target &T, const Triple &TT, StringRef CPU, StringRef FS,
@@ -488,7 +488,7 @@ AArch64leTargetMachine::AArch64leTargetMachine(
     std::optional<CodeModel::Model> CM, CodeGenOptLevel OL, bool JIT)
     : AArch64TargetMachine(T, TT, CPU, FS, Options, RM, CM, OL, JIT, true) {}
 
-void AArch64beTargetMachine::anchor() { }
+void AArch64beTargetMachine::anchor() {}
 
 AArch64beTargetMachine::AArch64beTargetMachine(
     const Target &T, const Triple &TT, StringRef CPU, StringRef FS,
@@ -539,7 +539,7 @@ public:
     return DAG;
   }
 
-  void addIRPasses()  override;
+  void addIRPasses() override;
   bool addPreISel() override;
   void addCodeGenPrepare() override;
   bool addInstSelector() override;
@@ -596,8 +596,7 @@ void AArch64PassConfig::addIRPasses() {
   addPass(createAtomicExpandLegacyPass());
 
   // Expand any SVE vector library calls that we can't code generate directly.
-  if (EnableSVEIntrinsicOpts &&
-      TM->getOptLevel() != CodeGenOptLevel::None)
+  if (EnableSVEIntrinsicOpts && TM->getOptLevel() != CodeGenOptLevel::None)
     addPass(createSVEIntrinsicOptsPass());
 
   // Cmpxchg instructions are often used with a subsequent comparison to
diff --git a/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp b/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
index 43f07be15..cff459592 100644
--- a/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
+++ b/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp
@@ -6,14 +6,14 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "AArch64InstrInfo.h"
-#include "MCTargetDesc/AArch64AddressingModes.h"
-#include "MCTargetDesc/AArch64InstPrinter.h"
-#include "MCTargetDesc/AArch64MCExpr.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
-#include "MCTargetDesc/AArch64TargetStreamer.h"
-#include "TargetInfo/AArch64TargetInfo.h"
-#include "Utils/AArch64BaseInfo.h"
+#include "../AArch64InstrInfo.h"
+#include "../MCTargetDesc/AArch64AddressingModes.h"
+#include "../MCTargetDesc/AArch64InstPrinter.h"
+#include "../MCTargetDesc/AArch64MCExpr.h"
+#include "../MCTargetDesc/AArch64MCTargetDesc.h"
+#include "../MCTargetDesc/AArch64TargetStreamer.h"
+#include "../TargetInfo/AArch64TargetInfo.h"
+#include "../Utils/AArch64BaseInfo.h"
 #include "llvm/ADT/APFloat.h"
 #include "llvm/ADT/APInt.h"
 #include "llvm/ADT/ArrayRef.h"
@@ -76,11 +76,7 @@ enum class RegKind {
 
 enum class MatrixKind { Array, Tile, Row, Col };
 
-enum RegConstraintEqualityTy {
-  EqualsReg,
-  EqualsSuperReg,
-  EqualsSubReg
-};
+enum RegConstraintEqualityTy { EqualsReg, EqualsSuperReg, EqualsSubReg };
 
 class AArch64AsmParser : public MCTargetAsmParser {
 private:
@@ -303,7 +299,7 @@ public:
 
   AArch64AsmParser(const MCSubtargetInfo &STI, MCAsmParser &Parser,
                    const MCInstrInfo &MII, const MCTargetOptions &Options)
-    : MCTargetAsmParser(Options, STI, MII) {
+      : MCTargetAsmParser(Options, STI, MII) {
     IsILP32 = STI.getTargetTriple().getEnvironment() == Triple::GNUILP32;
     IsWindowsArm64EC = STI.getTargetTriple().isWindowsArm64EC();
     MCAsmParserExtension::Initialize(Parser);
@@ -427,7 +423,7 @@ private:
     unsigned Stride;
     unsigned NumElements;
     unsigned ElementWidth;
-    RegKind  RegisterKind;
+    RegKind RegisterKind;
   };
 
   struct VectorIndexOp {
@@ -648,12 +644,12 @@ public:
   }
 
   APFloat getFPImm() const {
-    assert (Kind == k_FPImm && "Invalid access!");
+    assert(Kind == k_FPImm && "Invalid access!");
     return APFloat(APFloat::IEEEdouble(), APInt(64, FPImm.Val, true));
   }
 
   bool getFPImmIsExact() const {
-    assert (Kind == k_FPImm && "Invalid access!");
+    assert(Kind == k_FPImm && "Invalid access!");
     return FPImm.IsExact;
   }
 
@@ -890,7 +886,7 @@ public:
     MCSymbolRefExpr::VariantKind DarwinRefKind;
     int64_t Addend;
     if (!AArch64AsmParser::classifySymbolRef(Expr, ELFRefKind, DarwinRefKind,
-                                           Addend)) {
+                                             Addend)) {
       // If we don't understand the expression, assume the best and
       // let the fixup and relocation code deal with it.
       return true;
@@ -935,8 +931,7 @@ public:
     return (Val % Scale) == 0 && Val >= 0 && (Val / Scale) < 0x1000;
   }
 
-  template <int N, int M>
-  bool isImmInRange() const {
+  template <int N, int M> bool isImmInRange() const {
     if (!isImm())
       return false;
     const MCConstantExpr *MCE = dyn_cast<MCConstantExpr>(getImm());
@@ -948,8 +943,7 @@ public:
 
   // NOTE: Also used for isLogicalImmNot as anything that can be represented as
   // a logical immediate can always be represented when inverted.
-  template <typename T>
-  bool isLogicalImm() const {
+  template <typename T> bool isLogicalImm() const {
     if (!isImm())
       return false;
     const MCConstantExpr *MCE = dyn_cast<MCConstantExpr>(getImm());
@@ -1010,8 +1004,8 @@ public:
     AArch64MCExpr::VariantKind ELFRefKind;
     MCSymbolRefExpr::VariantKind DarwinRefKind;
     int64_t Addend;
-    if (AArch64AsmParser::classifySymbolRef(Expr, ELFRefKind,
-                                          DarwinRefKind, Addend)) {
+    if (AArch64AsmParser::classifySymbolRef(Expr, ELFRefKind, DarwinRefKind,
+                                            Addend)) {
       return DarwinRefKind == MCSymbolRefExpr::VK_PAGEOFF ||
              DarwinRefKind == MCSymbolRefExpr::VK_TLVPPAGEOFF ||
              (DarwinRefKind == MCSymbolRefExpr::VK_GOTPAGEOFF && Addend == 0) ||
@@ -1054,8 +1048,7 @@ public:
   // range -32768 to +32512.
   // For element-width of 8 bits a range of -128 to 255 is accepted,
   // since a copy of a byte can be either signed/unsigned.
-  template <typename T>
-  DiagnosticPredicate isSVECpyImm() const {
+  template <typename T> DiagnosticPredicate isSVECpyImm() const {
     if (!isShiftedImm() && (!isImm() || !isa<MCConstantExpr>(getImm())))
       return DiagnosticPredicateTy::NoMatch;
 
@@ -1105,8 +1098,7 @@ public:
     return AArch64_AM::isAdvSIMDModImmType10(MCE->getValue());
   }
 
-  template<int N>
-  bool isBranchTarget() const {
+  template <int N> bool isBranchTarget() const {
     if (!isImm())
       return false;
     const MCConstantExpr *MCE = dyn_cast<MCConstantExpr>(getImm());
@@ -1116,7 +1108,8 @@ public:
     if (Val & 0x3)
       return false;
     assert(N > 0 && "Branch target immediate cannot be 0 bits!");
-    return (Val >= -((1<<(N-1)) << 2) && Val <= (((1<<(N-1))-1) << 2));
+    return (Val >= -((1 << (N - 1)) << 2) &&
+            Val <= (((1 << (N - 1)) - 1) << 2));
   }
 
   bool
@@ -1142,11 +1135,11 @@ public:
   }
 
   bool isMovWSymbolG2() const {
-    return isMovWSymbol(
-        {AArch64MCExpr::VK_ABS_G2, AArch64MCExpr::VK_ABS_G2_S,
-         AArch64MCExpr::VK_ABS_G2_NC, AArch64MCExpr::VK_PREL_G2,
-         AArch64MCExpr::VK_PREL_G2_NC, AArch64MCExpr::VK_TPREL_G2,
-         AArch64MCExpr::VK_DTPREL_G2});
+    return isMovWSymbol({AArch64MCExpr::VK_ABS_G2, AArch64MCExpr::VK_ABS_G2_S,
+                         AArch64MCExpr::VK_ABS_G2_NC, AArch64MCExpr::VK_PREL_G2,
+                         AArch64MCExpr::VK_PREL_G2_NC,
+                         AArch64MCExpr::VK_TPREL_G2,
+                         AArch64MCExpr::VK_DTPREL_G2});
   }
 
   bool isMovWSymbolG1() const {
@@ -1167,9 +1160,9 @@ public:
          AArch64MCExpr::VK_DTPREL_G0, AArch64MCExpr::VK_DTPREL_G0_NC});
   }
 
-  template<int RegWidth, int Shift>
-  bool isMOVZMovAlias() const {
-    if (!isImm()) return false;
+  template <int RegWidth, int Shift> bool isMOVZMovAlias() const {
+    if (!isImm())
+      return false;
 
     const MCExpr *E = getImm();
     if (const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(E)) {
@@ -1182,12 +1175,13 @@ public:
     return !Shift && E;
   }
 
-  template<int RegWidth, int Shift>
-  bool isMOVNMovAlias() const {
-    if (!isImm()) return false;
+  template <int RegWidth, int Shift> bool isMOVNMovAlias() const {
+    if (!isImm())
+      return false;
 
     const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(getImm());
-    if (!CE) return false;
+    if (!CE)
+      return false;
     uint64_t Value = CE->getValue();
 
     return AArch64_AM::isMOVNMovAlias(Value, Shift, RegWidth);
@@ -1207,18 +1201,21 @@ public:
   bool isSysReg() const { return Kind == k_SysReg; }
 
   bool isMRSSystemRegister() const {
-    if (!isSysReg()) return false;
+    if (!isSysReg())
+      return false;
 
     return SysReg.MRSReg != -1U;
   }
 
   bool isMSRSystemRegister() const {
-    if (!isSysReg()) return false;
+    if (!isSysReg())
+      return false;
     return SysReg.MSRReg != -1U;
   }
 
   bool isSystemPStateFieldWithImm0_1() const {
-    if (!isSysReg()) return false;
+    if (!isSysReg())
+      return false;
     return AArch64PState::lookupPStateImm0_1ByEncoding(SysReg.PStateField);
   }
 
@@ -1234,9 +1231,7 @@ public:
     return SVCR.PStateField != -1U;
   }
 
-  bool isReg() const override {
-    return Kind == k_Register;
-  }
+  bool isReg() const override { return Kind == k_Register; }
 
   bool isVectorList() const { return Kind == k_VectorList; }
 
@@ -1346,7 +1341,8 @@ public:
     if (Kind != k_Register || Reg.Kind != RegKind::SVEPredicateAsCounter)
       return DiagnosticPredicateTy::NoMatch;
 
-    if (isSVEPredicateAsCounterReg<Class>() && (Reg.ElementWidth == ElementWidth))
+    if (isSVEPredicateAsCounterReg<Class>() &&
+        (Reg.ElementWidth == ElementWidth))
       return DiagnosticPredicateTy::Match;
 
     return DiagnosticPredicateTy::NearMatch;
@@ -1375,8 +1371,9 @@ public:
     // a shift-amount that does not match what is expected, but for which
     // there is also an unscaled addressing mode (e.g. sxtw/uxtw).
     bool MatchShift = getShiftExtendAmount() == Log2_32(ShiftWidth / 8);
-    if (!MatchShift && (ShiftExtendTy == AArch64_AM::UXTW ||
-                        ShiftExtendTy == AArch64_AM::SXTW) &&
+    if (!MatchShift &&
+        (ShiftExtendTy == AArch64_AM::UXTW ||
+         ShiftExtendTy == AArch64_AM::SXTW) &&
         !ShiftWidthAlwaysSame && hasShiftExtendAmount() && ShiftWidth == 8)
       return DiagnosticPredicateTy::NoMatch;
 
@@ -1388,12 +1385,14 @@ public:
 
   bool isGPR32as64() const {
     return Kind == k_Register && Reg.Kind == RegKind::Scalar &&
-      AArch64MCRegisterClasses[AArch64::GPR64RegClassID].contains(Reg.RegNum);
+           AArch64MCRegisterClasses[AArch64::GPR64RegClassID].contains(
+               Reg.RegNum);
   }
 
   bool isGPR64as32() const {
     return Kind == k_Register && Reg.Kind == RegKind::Scalar &&
-      AArch64MCRegisterClasses[AArch64::GPR32RegClassID].contains(Reg.RegNum);
+           AArch64MCRegisterClasses[AArch64::GPR32RegClassID].contains(
+               Reg.RegNum);
   }
 
   bool isGPR64x8() const {
@@ -1418,12 +1417,14 @@ public:
     return isGPR64<AArch64::GPR64RegClassID>() && Reg.RegNum == AArch64::XZR;
   }
 
-  template<int64_t Angle, int64_t Remainder>
+  template <int64_t Angle, int64_t Remainder>
   DiagnosticPredicate isComplexRotation() const {
-    if (!isImm()) return DiagnosticPredicateTy::NoMatch;
+    if (!isImm())
+      return DiagnosticPredicateTy::NoMatch;
 
     const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(getImm());
-    if (!CE) return DiagnosticPredicateTy::NoMatch;
+    if (!CE)
+      return DiagnosticPredicateTy::NoMatch;
     uint64_t Value = CE->getValue();
 
     if (Value % Angle == Remainder && Value <= 270)
@@ -1499,8 +1500,7 @@ public:
     return DiagnosticPredicateTy::NoMatch;
   }
 
-  template <int Min, int Max>
-  DiagnosticPredicate isVectorIndex() const {
+  template <int Min, int Max> DiagnosticPredicate isVectorIndex() const {
     if (Kind != k_VectorIndex)
       return DiagnosticPredicateTy::NoMatch;
     if (VectorIndex.Val >= Min && VectorIndex.Val <= Max)
@@ -1601,7 +1601,7 @@ public:
     return ET == AArch64_AM::LSL && getShiftExtendAmount() <= 7;
   }
 
-  template<int Width> bool isMemXExtend() const {
+  template <int Width> bool isMemXExtend() const {
     if (!isExtend())
       return false;
     AArch64_AM::ShiftExtendType ET = getShiftExtendType();
@@ -1610,7 +1610,7 @@ public:
             getShiftExtendAmount() == 0);
   }
 
-  template<int Width> bool isMemWExtend() const {
+  template <int Width> bool isMemWExtend() const {
     if (!isExtend())
       return false;
     AArch64_AM::ShiftExtendType ET = getShiftExtendType();
@@ -1619,19 +1619,18 @@ public:
             getShiftExtendAmount() == 0);
   }
 
-  template <unsigned width>
-  bool isArithmeticShifter() const {
+  template <unsigned width> bool isArithmeticShifter() const {
     if (!isShifter())
       return false;
 
     // An arithmetic shifter is LSL, LSR, or ASR.
     AArch64_AM::ShiftExtendType ST = getShiftExtendType();
     return (ST == AArch64_AM::LSL || ST == AArch64_AM::LSR ||
-            ST == AArch64_AM::ASR) && getShiftExtendAmount() < width;
+            ST == AArch64_AM::ASR) &&
+           getShiftExtendAmount() < width;
   }
 
-  template <unsigned width>
-  bool isLogicalShifter() const {
+  template <unsigned width> bool isLogicalShifter() const {
     if (!isShifter())
       return false;
 
@@ -1701,8 +1700,7 @@ public:
   // the latter. As such, in addition to checking for being a legal unscaled
   // address, also check that it is not a legal scaled address. This avoids
   // ambiguity in the matcher.
-  template<int Width>
-  bool isSImm9OffsetFB() const {
+  template <int Width> bool isSImm9OffsetFB() const {
     return isSImm<9>() && !isUImm12Offset<Width / 8>();
   }
 
@@ -1710,11 +1708,11 @@ public:
     // Validation was handled during parsing, so we just verify that
     // something didn't go haywire.
     if (!isImm())
-        return false;
+      return false;
 
     if (const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(Imm.Val)) {
       int64_t Val = CE->getValue();
-      int64_t Min = - (4096 * (1LL << (21 - 1)));
+      int64_t Min = -(4096 * (1LL << (21 - 1)));
       int64_t Max = 4096 * ((1LL << (21 - 1)) - 1);
       return (Val % 4096) == 0 && Val >= Min && Val <= Max;
     }
@@ -1726,11 +1724,11 @@ public:
     // Validation was handled during parsing, so we just verify that
     // something didn't go haywire.
     if (!isImm())
-        return false;
+      return false;
 
     if (const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(Imm.Val)) {
       int64_t Val = CE->getValue();
-      int64_t Min = - (1LL << (21 - 1));
+      int64_t Min = -(1LL << (21 - 1));
       int64_t Max = ((1LL << (21 - 1)) - 1);
       return Val >= Min && Val <= Max;
     }
@@ -1790,8 +1788,8 @@ public:
         AArch64MCRegisterClasses[AArch64::GPR64RegClassID].contains(getReg()));
 
     const MCRegisterInfo *RI = Ctx.getRegisterInfo();
-    uint32_t Reg = RI->getRegClass(AArch64::GPR32RegClassID).getRegister(
-        RI->getEncodingValue(getReg()));
+    uint32_t Reg = RI->getRegClass(AArch64::GPR32RegClassID)
+                       .getRegister(RI->getEncodingValue(getReg()));
 
     Inst.addOperand(MCOperand::createReg(Reg));
   }
@@ -1802,8 +1800,8 @@ public:
         AArch64MCRegisterClasses[AArch64::GPR32RegClassID].contains(getReg()));
 
     const MCRegisterInfo *RI = Ctx.getRegisterInfo();
-    uint32_t Reg = RI->getRegClass(AArch64::GPR64RegClassID).getRegister(
-        RI->getEncodingValue(getReg()));
+    uint32_t Reg = RI->getRegClass(AArch64::GPR64RegClassID)
+                       .getRegister(RI->getEncodingValue(getReg()));
 
     Inst.addOperand(MCOperand::createReg(Reg));
   }
@@ -1812,11 +1810,21 @@ public:
   void addFPRasZPRRegOperands(MCInst &Inst, unsigned N) const {
     unsigned Base;
     switch (Width) {
-    case 8:   Base = AArch64::B0; break;
-    case 16:  Base = AArch64::H0; break;
-    case 32:  Base = AArch64::S0; break;
-    case 64:  Base = AArch64::D0; break;
-    case 128: Base = AArch64::Q0; break;
+    case 8:
+      Base = AArch64::B0;
+      break;
+    case 16:
+      Base = AArch64::H0;
+      break;
+    case 32:
+      Base = AArch64::S0;
+      break;
+    case 64:
+      Base = AArch64::D0;
+      break;
+    case 128:
+      Base = AArch64::Q0;
+      break;
     default:
       llvm_unreachable("Unsupported width");
     }
@@ -1876,18 +1884,15 @@ public:
     assert((!IsConsecutive || (getVectorListStride() == 1)) &&
            "Expected consecutive registers");
     static const unsigned FirstRegs[][5] = {
-      /* DReg */ { AArch64::Q0,
-                   AArch64::D0,       AArch64::D0_D1,
-                   AArch64::D0_D1_D2, AArch64::D0_D1_D2_D3 },
-      /* QReg */ { AArch64::Q0,
-                   AArch64::Q0,       AArch64::Q0_Q1,
-                   AArch64::Q0_Q1_Q2, AArch64::Q0_Q1_Q2_Q3 },
-      /* ZReg */ { AArch64::Z0,
-                   AArch64::Z0,       AArch64::Z0_Z1,
-                   AArch64::Z0_Z1_Z2, AArch64::Z0_Z1_Z2_Z3 },
-      /* PReg */ { AArch64::P0,
-                   AArch64::P0,       AArch64::P0_P1 }
-    };
+        /* DReg */ {AArch64::Q0, AArch64::D0, AArch64::D0_D1, AArch64::D0_D1_D2,
+                    AArch64::D0_D1_D2_D3},
+        /* QReg */
+        {AArch64::Q0, AArch64::Q0, AArch64::Q0_Q1, AArch64::Q0_Q1_Q2,
+         AArch64::Q0_Q1_Q2_Q3},
+        /* ZReg */
+        {AArch64::Z0, AArch64::Z0, AArch64::Z0_Z1, AArch64::Z0_Z1_Z2,
+         AArch64::Z0_Z1_Z2_Z3},
+        /* PReg */ {AArch64::P0, AArch64::P0, AArch64::P0_P1}};
 
     assert((RegTy != VecListIdx_ZReg || NumRegs <= 4) &&
            " NumRegs must be <= 4 for ZRegs");
@@ -2007,7 +2012,7 @@ public:
     addImmOperands(Inst, N);
   }
 
-  template<int Scale>
+  template <int Scale>
   void addUImm12OffsetOperands(MCInst &Inst, unsigned N) const {
     assert(N == 1 && "Invalid number of operands!");
     const MCConstantExpr *MCE = dyn_cast<MCConstantExpr>(getImm());
@@ -2234,7 +2239,8 @@ public:
   void addExtendOperands(MCInst &Inst, unsigned N) const {
     assert(N == 1 && "Invalid number of operands!");
     AArch64_AM::ShiftExtendType ET = getShiftExtendType();
-    if (ET == AArch64_AM::LSL) ET = AArch64_AM::UXTW;
+    if (ET == AArch64_AM::LSL)
+      ET = AArch64_AM::UXTW;
     unsigned Imm = AArch64_AM::getArithExtendImm(ET, getShiftExtendAmount());
     Inst.addOperand(MCOperand::createImm(Imm));
   }
@@ -2242,7 +2248,8 @@ public:
   void addExtend64Operands(MCInst &Inst, unsigned N) const {
     assert(N == 1 && "Invalid number of operands!");
     AArch64_AM::ShiftExtendType ET = getShiftExtendType();
-    if (ET == AArch64_AM::LSL) ET = AArch64_AM::UXTX;
+    if (ET == AArch64_AM::LSL)
+      ET = AArch64_AM::UXTX;
     unsigned Imm = AArch64_AM::getArithExtendImm(ET, getShiftExtendAmount());
     Inst.addOperand(MCOperand::createImm(Imm));
   }
@@ -2267,7 +2274,7 @@ public:
     Inst.addOperand(MCOperand::createImm(hasShiftExtendAmount()));
   }
 
-  template<int Shift>
+  template <int Shift>
   void addMOVZMovAliasOperands(MCInst &Inst, unsigned N) const {
     assert(N == 1 && "Invalid number of operands!");
 
@@ -2280,7 +2287,7 @@ public:
     }
   }
 
-  template<int Shift>
+  template <int Shift>
   void addMOVNMovAliasOperands(MCInst &Inst, unsigned N) const {
     assert(N == 1 && "Invalid number of operands!");
 
@@ -2318,8 +2325,7 @@ public:
   CreateReg(unsigned RegNum, RegKind Kind, SMLoc S, SMLoc E, MCContext &Ctx,
             RegConstraintEqualityTy EqTy = RegConstraintEqualityTy::EqualsReg,
             AArch64_AM::ShiftExtendType ExtTy = AArch64_AM::LSL,
-            unsigned ShiftAmount = 0,
-            unsigned HasExplicitAmount = false) {
+            unsigned ShiftAmount = 0, unsigned HasExplicitAmount = false) {
     auto Op = std::make_unique<AArch64Operand>(k_Register, Ctx);
     Op->Reg.RegNum = RegNum;
     Op->Reg.Kind = Kind;
@@ -2333,12 +2339,10 @@ public:
     return Op;
   }
 
-  static std::unique_ptr<AArch64Operand>
-  CreateVectorReg(unsigned RegNum, RegKind Kind, unsigned ElementWidth,
-                  SMLoc S, SMLoc E, MCContext &Ctx,
-                  AArch64_AM::ShiftExtendType ExtTy = AArch64_AM::LSL,
-                  unsigned ShiftAmount = 0,
-                  unsigned HasExplicitAmount = false) {
+  static std::unique_ptr<AArch64Operand> CreateVectorReg(
+      unsigned RegNum, RegKind Kind, unsigned ElementWidth, SMLoc S, SMLoc E,
+      MCContext &Ctx, AArch64_AM::ShiftExtendType ExtTy = AArch64_AM::LSL,
+      unsigned ShiftAmount = 0, unsigned HasExplicitAmount = false) {
     assert((Kind == RegKind::NeonVector || Kind == RegKind::SVEDataVector ||
             Kind == RegKind::SVEPredicateVector ||
             Kind == RegKind::SVEPredicateAsCounter) &&
@@ -2427,7 +2431,7 @@ public:
                                                           SMLoc S, SMLoc E,
                                                           MCContext &Ctx) {
     auto Op = std::make_unique<AArch64Operand>(k_ShiftedImm, Ctx);
-    Op->ShiftedImm .Val = Val;
+    Op->ShiftedImm.Val = Val;
     Op->ShiftedImm.ShiftAmount = ShiftAmount;
     Op->StartLoc = S;
     Op->EndLoc = E;
@@ -2454,8 +2458,8 @@ public:
     return Op;
   }
 
-  static std::unique_ptr<AArch64Operand>
-  CreateFPImm(APFloat Val, bool IsExact, SMLoc S, MCContext &Ctx) {
+  static std::unique_ptr<AArch64Operand> CreateFPImm(APFloat Val, bool IsExact,
+                                                     SMLoc S, MCContext &Ctx) {
     auto Op = std::make_unique<AArch64Operand>(k_FPImm, Ctx);
     Op->FPImm.Val = Val.bitcastToAPInt().getSExtValue();
     Op->FPImm.IsExact = IsExact;
@@ -2465,8 +2469,7 @@ public:
   }
 
   static std::unique_ptr<AArch64Operand> CreateBarrier(unsigned Val,
-                                                       StringRef Str,
-                                                       SMLoc S,
+                                                       StringRef Str, SMLoc S,
                                                        MCContext &Ctx,
                                                        bool HasnXSModifier) {
     auto Op = std::make_unique<AArch64Operand>(k_Barrier, Ctx);
@@ -2479,11 +2482,9 @@ public:
     return Op;
   }
 
-  static std::unique_ptr<AArch64Operand> CreateSysReg(StringRef Str, SMLoc S,
-                                                      uint32_t MRSReg,
-                                                      uint32_t MSRReg,
-                                                      uint32_t PStateField,
-                                                      MCContext &Ctx) {
+  static std::unique_ptr<AArch64Operand>
+  CreateSysReg(StringRef Str, SMLoc S, uint32_t MRSReg, uint32_t MSRReg,
+               uint32_t PStateField, MCContext &Ctx) {
     auto Op = std::make_unique<AArch64Operand>(k_SysReg, Ctx);
     Op->SysReg.Data = Str.data();
     Op->SysReg.Length = Str.size();
@@ -2515,10 +2516,8 @@ public:
     return Op;
   }
 
-  static std::unique_ptr<AArch64Operand> CreatePrefetch(unsigned Val,
-                                                        StringRef Str,
-                                                        SMLoc S,
-                                                        MCContext &Ctx) {
+  static std::unique_ptr<AArch64Operand>
+  CreatePrefetch(unsigned Val, StringRef Str, SMLoc S, MCContext &Ctx) {
     auto Op = std::make_unique<AArch64Operand>(k_Prefetch, Ctx);
     Op->Prefetch.Val = Val;
     Op->Barrier.Data = Str.data();
@@ -2528,10 +2527,8 @@ public:
     return Op;
   }
 
-  static std::unique_ptr<AArch64Operand> CreatePSBHint(unsigned Val,
-                                                       StringRef Str,
-                                                       SMLoc S,
-                                                       MCContext &Ctx) {
+  static std::unique_ptr<AArch64Operand>
+  CreatePSBHint(unsigned Val, StringRef Str, SMLoc S, MCContext &Ctx) {
     auto Op = std::make_unique<AArch64Operand>(k_PSBHint, Ctx);
     Op->PSBHint.Val = Val;
     Op->PSBHint.Data = Str.data();
@@ -2541,10 +2538,8 @@ public:
     return Op;
   }
 
-  static std::unique_ptr<AArch64Operand> CreateBTIHint(unsigned Val,
-                                                       StringRef Str,
-                                                       SMLoc S,
-                                                       MCContext &Ctx) {
+  static std::unique_ptr<AArch64Operand>
+  CreateBTIHint(unsigned Val, StringRef Str, SMLoc S, MCContext &Ctx) {
     auto Op = std::make_unique<AArch64Operand>(k_BTIHint, Ctx);
     Op->BTIHint.Val = Val | 32;
     Op->BTIHint.Data = Str.data();
@@ -3032,7 +3027,7 @@ unsigned AArch64AsmParser::matchRegisterNameAlias(StringRef Name,
   if ((RegNum = matchMatrixRegName(Name)))
     return Kind == RegKind::Matrix ? RegNum : 0;
 
- if (Name.equals_insensitive("zt0"))
+  if (Name.equals_insensitive("zt0"))
     return Kind == RegKind::LookupTable ? unsigned(AArch64::ZT0) : 0;
 
   // The parsed register must be of RegKind Scalar
@@ -3042,11 +3037,11 @@ unsigned AArch64AsmParser::matchRegisterNameAlias(StringRef Name,
   if (!RegNum) {
     // Handle a few common aliases of registers.
     if (auto RegNum = StringSwitch<unsigned>(Name.lower())
-                    .Case("fp", AArch64::FP)
-                    .Case("lr",  AArch64::LR)
-                    .Case("x31", AArch64::XZR)
-                    .Case("w31", AArch64::WZR)
-                    .Default(0))
+                          .Case("fp", AArch64::FP)
+                          .Case("lr", AArch64::LR)
+                          .Case("x31", AArch64::XZR)
+                          .Case("w31", AArch64::WZR)
+                          .Default(0))
       return Kind == RegKind::Scalar ? RegNum : 0;
 
     // Check for aliases registered via .req. Canonicalize to lower case.
@@ -3127,8 +3122,7 @@ ParseStatus AArch64AsmParser::tryParseRPRFMOperand(OperandVector &Operands) {
   unsigned MaxVal = 63;
 
   // Immediate case, with optional leading hash:
-  if (parseOptionalToken(AsmToken::Hash) ||
-      Tok.is(AsmToken::Integer)) {
+  if (parseOptionalToken(AsmToken::Hash) || Tok.is(AsmToken::Integer)) {
     const MCExpr *ImmVal;
     if (getParser().parseExpression(ImmVal))
       return ParseStatus::Failure;
@@ -3187,8 +3181,7 @@ ParseStatus AArch64AsmParser::tryParsePrefetch(OperandVector &Operands) {
 
   // Either an identifier for named values or a 5-bit immediate.
   // Eat optional hash.
-  if (parseOptionalToken(AsmToken::Hash) ||
-      Tok.is(AsmToken::Integer)) {
+  if (parseOptionalToken(AsmToken::Hash) || Tok.is(AsmToken::Integer)) {
     const MCExpr *ImmVal;
     if (getParser().parseExpression(ImmVal))
       return ParseStatus::Failure;
@@ -3214,8 +3207,8 @@ ParseStatus AArch64AsmParser::tryParsePrefetch(OperandVector &Operands) {
   if (!PRFM)
     return TokError("prefetch hint expected");
 
-  Operands.push_back(AArch64Operand::CreatePrefetch(
-      *PRFM, Tok.getString(), S, getContext()));
+  Operands.push_back(
+      AArch64Operand::CreatePrefetch(*PRFM, Tok.getString(), S, getContext()));
   Lex(); // Eat identifier token.
   return ParseStatus::Success;
 }
@@ -3402,8 +3395,7 @@ ParseStatus AArch64AsmParser::tryParseFPImm(OperandVector &Operands) {
       return TokError("encoded floating point value out of range");
 
     APFloat F((double)AArch64_AM::getFPImmFloat(Tok.getIntVal()));
-    Operands.push_back(
-        AArch64Operand::CreateFPImm(F, true, S, getContext()));
+    Operands.push_back(AArch64Operand::CreateFPImm(F, true, S, getContext()));
   } else {
     // Parse FP representation.
     APFloat RealVal(APFloat::IEEEdouble());
@@ -3500,39 +3492,39 @@ AArch64AsmParser::tryParseImmWithOptionalShift(OperandVector &Operands) {
 AArch64CC::CondCode
 AArch64AsmParser::parseCondCodeString(StringRef Cond, std::string &Suggestion) {
   AArch64CC::CondCode CC = StringSwitch<AArch64CC::CondCode>(Cond.lower())
-                    .Case("eq", AArch64CC::EQ)
-                    .Case("ne", AArch64CC::NE)
-                    .Case("cs", AArch64CC::HS)
-                    .Case("hs", AArch64CC::HS)
-                    .Case("cc", AArch64CC::LO)
-                    .Case("lo", AArch64CC::LO)
-                    .Case("mi", AArch64CC::MI)
-                    .Case("pl", AArch64CC::PL)
-                    .Case("vs", AArch64CC::VS)
-                    .Case("vc", AArch64CC::VC)
-                    .Case("hi", AArch64CC::HI)
-                    .Case("ls", AArch64CC::LS)
-                    .Case("ge", AArch64CC::GE)
-                    .Case("lt", AArch64CC::LT)
-                    .Case("gt", AArch64CC::GT)
-                    .Case("le", AArch64CC::LE)
-                    .Case("al", AArch64CC::AL)
-                    .Case("nv", AArch64CC::NV)
-                    .Default(AArch64CC::Invalid);
+                               .Case("eq", AArch64CC::EQ)
+                               .Case("ne", AArch64CC::NE)
+                               .Case("cs", AArch64CC::HS)
+                               .Case("hs", AArch64CC::HS)
+                               .Case("cc", AArch64CC::LO)
+                               .Case("lo", AArch64CC::LO)
+                               .Case("mi", AArch64CC::MI)
+                               .Case("pl", AArch64CC::PL)
+                               .Case("vs", AArch64CC::VS)
+                               .Case("vc", AArch64CC::VC)
+                               .Case("hi", AArch64CC::HI)
+                               .Case("ls", AArch64CC::LS)
+                               .Case("ge", AArch64CC::GE)
+                               .Case("lt", AArch64CC::LT)
+                               .Case("gt", AArch64CC::GT)
+                               .Case("le", AArch64CC::LE)
+                               .Case("al", AArch64CC::AL)
+                               .Case("nv", AArch64CC::NV)
+                               .Default(AArch64CC::Invalid);
 
   if (CC == AArch64CC::Invalid && getSTI().hasFeature(AArch64::FeatureSVE)) {
     CC = StringSwitch<AArch64CC::CondCode>(Cond.lower())
-                    .Case("none",  AArch64CC::EQ)
-                    .Case("any",   AArch64CC::NE)
-                    .Case("nlast", AArch64CC::HS)
-                    .Case("last",  AArch64CC::LO)
-                    .Case("first", AArch64CC::MI)
-                    .Case("nfrst", AArch64CC::PL)
-                    .Case("pmore", AArch64CC::HI)
-                    .Case("plast", AArch64CC::LS)
-                    .Case("tcont", AArch64CC::GE)
-                    .Case("tstop", AArch64CC::LT)
-                    .Default(AArch64CC::Invalid);
+             .Case("none", AArch64CC::EQ)
+             .Case("any", AArch64CC::NE)
+             .Case("nlast", AArch64CC::HS)
+             .Case("last", AArch64CC::LO)
+             .Case("first", AArch64CC::MI)
+             .Case("nfrst", AArch64CC::PL)
+             .Case("pmore", AArch64CC::HI)
+             .Case("plast", AArch64CC::LS)
+             .Case("tcont", AArch64CC::GE)
+             .Case("tstop", AArch64CC::LT)
+             .Default(AArch64CC::Invalid);
 
     if (CC == AArch64CC::Invalid && Cond.lower() == "nfirst")
       Suggestion = "nfrst";
@@ -3560,7 +3552,8 @@ bool AArch64AsmParser::parseCondCode(OperandVector &Operands,
 
   if (invertCondCode) {
     if (CC == AArch64CC::AL || CC == AArch64CC::NV)
-      return TokError("condition codes AL and NV are invalid for this instruction");
+      return TokError(
+          "condition codes AL and NV are invalid for this instruction");
     CC = AArch64CC::getInvertedCondCode(AArch64CC::CondCode(CC));
   }
 
@@ -3876,7 +3869,7 @@ static void setRequiredFeatureString(FeatureBitset FBS, std::string &Str) {
     Str += "ARMv8r";
   else {
     SmallVector<std::string, 2> ExtMatches;
-    for (const auto& Ext : ExtensionMap) {
+    for (const auto &Ext : ExtensionMap) {
       // Use & in case multiple features are enabled
       if ((FBS & Ext.Features) != FeatureBitset())
         ExtMatches.push_back(Ext.Name);
@@ -3885,8 +3878,8 @@ static void setRequiredFeatureString(FeatureBitset FBS, std::string &Str) {
   }
 }
 
-void AArch64AsmParser::createSysAlias(uint16_t Encoding, OperandVector &Operands,
-                                      SMLoc S) {
+void AArch64AsmParser::createSysAlias(uint16_t Encoding,
+                                      OperandVector &Operands, SMLoc S) {
   const uint16_t Op2 = Encoding & 7;
   const uint16_t Cm = (Encoding & 0x78) >> 3;
   const uint16_t Cn = (Encoding & 0x780) >> 7;
@@ -3908,7 +3901,7 @@ void AArch64AsmParser::createSysAlias(uint16_t Encoding, OperandVector &Operands
 /// parseSysAlias - The IC, DC, AT, and TLBI instructions are simple aliases for
 /// the SYS instruction. Parse them specially so that we create a SYS MCInst.
 bool AArch64AsmParser::parseSysAlias(StringRef Name, SMLoc NameLoc,
-                                   OperandVector &Operands) {
+                                     OperandVector &Operands) {
   if (Name.contains('.'))
     return TokError("invalid operand");
 
@@ -3959,7 +3952,8 @@ bool AArch64AsmParser::parseSysAlias(StringRef Name, SMLoc NameLoc,
       return TokError(Str);
     }
     createSysAlias(TLBI->Encoding, Operands, S);
-  } else if (Mnemonic == "cfp" || Mnemonic == "dvp" || Mnemonic == "cpp" || Mnemonic == "cosp") {
+  } else if (Mnemonic == "cfp" || Mnemonic == "dvp" || Mnemonic == "cpp" ||
+             Mnemonic == "cosp") {
 
     if (Op.lower() != "rctx")
       return TokError("invalid operand for prediction restriction instruction");
@@ -4203,9 +4197,8 @@ ParseStatus AArch64AsmParser::tryParseSysReg(OperandVector &Operands) {
       PStateImm = PState1->Encoding;
   }
 
-  Operands.push_back(
-      AArch64Operand::CreateSysReg(Tok.getString(), getLoc(), MRSReg, MSRReg,
-                                   PStateImm, getContext()));
+  Operands.push_back(AArch64Operand::CreateSysReg(
+      Tok.getString(), getLoc(), MRSReg, MSRReg, PStateImm, getContext()));
   Lex(); // Eat identifier
 
   return ParseStatus::Success;
@@ -4246,9 +4239,8 @@ bool AArch64AsmParser::tryParseNeonVectorRegister(OperandVector &Operands) {
     return true;
 
   unsigned ElementWidth = KindRes->second;
-  Operands.push_back(
-      AArch64Operand::CreateVectorReg(Reg, RegKind::NeonVector, ElementWidth,
-                                      S, getLoc(), getContext()));
+  Operands.push_back(AArch64Operand::CreateVectorReg(
+      Reg, RegKind::NeonVector, ElementWidth, S, getLoc(), getContext()));
 
   // If there was an explicit qualifier, that goes on as a literal text
   // operand.
@@ -4341,8 +4333,7 @@ AArch64AsmParser::tryParseSVEPredicateVector(OperandVector &Operands) {
 
   unsigned ElementWidth = KindRes->second;
   Operands.push_back(AArch64Operand::CreateVectorReg(
-      RegNum, RK, ElementWidth, S,
-      getLoc(), getContext()));
+      RegNum, RK, ElementWidth, S, getLoc(), getContext()));
 
   if (getLexer().is(AsmToken::LBrac)) {
     if (RK == RegKind::SVEPredicateAsCounter) {
@@ -4662,8 +4653,7 @@ ParseStatus AArch64AsmParser::tryParseVectorList(OperandVector &Operands,
       return Error(Loc, "invalid number of vectors");
 
     Count += Space;
-  }
-  else {
+  } else {
     bool HasCalculatedStride = false;
     while (parseOptionalToken(AsmToken::Comma)) {
       SMLoc Loc = getLoc();
@@ -4816,7 +4806,7 @@ ParseStatus AArch64AsmParser::tryParseGPROperand(OperandVector &Operands) {
   if (!Res.isSuccess())
     return Res;
 
-  auto Ext = static_cast<AArch64Operand*>(ExtOpnd.back().get());
+  auto Ext = static_cast<AArch64Operand *>(ExtOpnd.back().get());
   Operands.push_back(AArch64Operand::CreateReg(
       RegNum, RegKind::Scalar, StartLoc, Ext->getEndLoc(), getContext(), EqTy,
       Ext->getShiftExtendType(), Ext->getShiftExtendAmount(),
@@ -4907,7 +4897,7 @@ bool AArch64AsmParser::parseKeywordOperand(OperandVector &Operands) {
 /// parseOperand - Parse a arm instruction operand.  For now this parses the
 /// operand regardless of the mnemonic.
 bool AArch64AsmParser::parseOperand(OperandVector &Operands, bool isCondCode,
-                                  bool invertCondCode) {
+                                    bool invertCondCode) {
   MCAsmParser &Parser = getParser();
 
   ParseStatus ResTy =
@@ -5086,7 +5076,7 @@ bool AArch64AsmParser::parseOperand(OperandVector &Operands, bool isCondCode,
         AArch64MCRegisterClasses[AArch64::GPR64allRegClassID].contains(
             Operands[1]->getReg());
 
-    MCContext& Ctx = getContext();
+    MCContext &Ctx = getContext();
     E = SMLoc::getFromPointer(Loc.getPointer() - 1);
     // If the op is an imm and can be fit into a mov, then replace ldr with mov.
     if (isa<MCConstantExpr>(SubExprVal)) {
@@ -5101,8 +5091,8 @@ bool AArch64AsmParser::parseOperand(OperandVector &Operands, bool isCondCode,
         Operands.push_back(AArch64Operand::CreateImm(
             MCConstantExpr::create(Imm, Ctx), S, E, Ctx));
         if (ShiftAmt)
-          Operands.push_back(AArch64Operand::CreateShiftExtend(AArch64_AM::LSL,
-                     ShiftAmt, true, S, E, Ctx));
+          Operands.push_back(AArch64Operand::CreateShiftExtend(
+              AArch64_AM::LSL, ShiftAmt, true, S, E, Ctx));
         return false;
       }
       APInt Simm = APInt(64, Imm << ShiftAmt);
@@ -5111,8 +5101,8 @@ bool AArch64AsmParser::parseOperand(OperandVector &Operands, bool isCondCode,
         return Error(Loc, "Immediate too large for register");
     }
     // If it is a label or an imm that cannot fit in a movz, put it into CP.
-    const MCExpr *CPLoc =
-        getTargetStreamer().addConstantPoolEntry(SubExprVal, IsXReg ? 8 : 4, Loc);
+    const MCExpr *CPLoc = getTargetStreamer().addConstantPoolEntry(
+        SubExprVal, IsXReg ? 8 : 4, Loc);
     Operands.push_back(AArch64Operand::CreateImm(CPLoc, S, E, Ctx));
     return false;
   }
@@ -5180,8 +5170,8 @@ bool AArch64AsmParser::parseRegisterInRange(unsigned &Out, unsigned Base,
 
 bool AArch64AsmParser::areEqualRegs(const MCParsedAsmOperand &Op1,
                                     const MCParsedAsmOperand &Op2) const {
-  auto &AOp1 = static_cast<const AArch64Operand&>(Op1);
-  auto &AOp2 = static_cast<const AArch64Operand&>(Op2);
+  auto &AOp1 = static_cast<const AArch64Operand &>(Op1);
+  auto &AOp2 = static_cast<const AArch64Operand &>(Op2);
 
   if (AOp1.isVectorList() && AOp2.isVectorList())
     return AOp1.getVectorListCount() == AOp2.getVectorListCount() &&
@@ -5318,9 +5308,10 @@ bool AArch64AsmParser::parseInstruction(ParseInstructionInfo &Info,
     unsigned N = 1;
     do {
       // Parse and remember the operand.
-      if (parseOperand(Operands, (N == 4 && condCodeFourthOperand) ||
-                                     (N == 3 && condCodeThirdOperand) ||
-                                     (N == 2 && condCodeSecondOperand),
+      if (parseOperand(Operands,
+                       (N == 4 && condCodeFourthOperand) ||
+                           (N == 3 && condCodeThirdOperand) ||
+                           (N == 2 && condCodeSecondOperand),
                        condCodeSecondOperand || condCodeThirdOperand)) {
         return true;
       }
@@ -5387,27 +5378,27 @@ bool AArch64AsmParser::validateInstruction(MCInst &Inst, SMLoc &IDLoc,
   // Before validating the instruction in isolation we run through the rules
   // applicable when it follows a prefix instruction.
   // NOTE: brk & hlt can be prefixed but require no additional validation.
-  if (Prefix.isActive() &&
-      (Inst.getOpcode() != AArch64::BRK) &&
+  if (Prefix.isActive() && (Inst.getOpcode() != AArch64::BRK) &&
       (Inst.getOpcode() != AArch64::HLT)) {
 
     // Prefixed intructions must have a destructive operand.
     if ((MCID.TSFlags & AArch64::DestructiveInstTypeMask) ==
         AArch64::NotDestructive)
       return Error(IDLoc, "instruction is unpredictable when following a"
-                   " movprfx, suggest replacing movprfx with mov");
+                          " movprfx, suggest replacing movprfx with mov");
 
     // Destination operands must match.
     if (Inst.getOperand(0).getReg() != Prefix.getDstReg())
       return Error(Loc[0], "instruction is unpredictable when following a"
-                   " movprfx writing to a different destination");
+                           " movprfx writing to a different destination");
 
     // Destination operand must not be used in any other location.
     for (unsigned i = 1; i < Inst.getNumOperands(); ++i) {
       if (Inst.getOperand(i).isReg() &&
           (MCID.getOperandConstraint(i, MCOI::TIED_TO) == -1) &&
           isMatchingOrAlias(Prefix.getDstReg(), Inst.getOperand(i).getReg()))
-        return Error(Loc[0], "instruction is unpredictable when following a"
+        return Error(Loc[0],
+                     "instruction is unpredictable when following a"
                      " movprfx and destination also used as non-destructive"
                      " source");
     }
@@ -5427,17 +5418,20 @@ bool AArch64AsmParser::validateInstruction(MCInst &Inst, SMLoc &IDLoc,
       // Instruction must be predicated if the movprfx is predicated.
       if (PgIdx == -1 ||
           (MCID.TSFlags & AArch64::ElementSizeMask) == AArch64::ElementSizeNone)
-        return Error(IDLoc, "instruction is unpredictable when following a"
+        return Error(IDLoc,
+                     "instruction is unpredictable when following a"
                      " predicated movprfx, suggest using unpredicated movprfx");
 
       // Instruction must use same general predicate as the movprfx.
       if (Inst.getOperand(PgIdx).getReg() != Prefix.getPgReg())
-        return Error(IDLoc, "instruction is unpredictable when following a"
+        return Error(IDLoc,
+                     "instruction is unpredictable when following a"
                      " predicated movprfx using a different general predicate");
 
       // Instruction element type must match the movprfx.
       if ((MCID.TSFlags & AArch64::ElementSizeMask) != Prefix.getElementSize())
-        return Error(IDLoc, "instruction is unpredictable when following a"
+        return Error(IDLoc,
+                     "instruction is unpredictable when following a"
                      " predicated movprfx with a different element size");
     }
   }
@@ -5625,9 +5619,8 @@ bool AArch64AsmParser::validateInstruction(MCInst &Inst, SMLoc &IDLoc,
     MCRegister Xt = Inst.getOperand(0).getReg();
     MCRegister Xn = Inst.getOperand(1).getReg();
     if (Xt == Xn)
-      return Error(Loc[0],
-          "unpredictable LDRA instruction, writeback base"
-          " is also a destination");
+      return Error(Loc[0], "unpredictable LDRA instruction, writeback base"
+                           " is also a destination");
     break;
   }
   }
@@ -5897,14 +5890,15 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
   case Match_InvalidCondCode:
     return Error(Loc, "expected AArch64 condition code");
   case Match_AddSubRegExtendSmall:
-    return Error(Loc,
-      "expected '[su]xt[bhw]' with optional integer in range [0, 4]");
+    return Error(
+        Loc, "expected '[su]xt[bhw]' with optional integer in range [0, 4]");
   case Match_AddSubRegExtendLarge:
-    return Error(Loc,
-      "expected 'sxtx' 'uxtx' or 'lsl' with optional integer in range [0, 4]");
+    return Error(Loc, "expected 'sxtx' 'uxtx' or 'lsl' with optional integer "
+                      "in range [0, 4]");
   case Match_AddSubSecondSource:
-    return Error(Loc,
-      "expected compatible register, symbol or integer in range [0, 4095]");
+    return Error(
+        Loc,
+        "expected compatible register, symbol or integer in range [0, 4095]");
   case Match_LogicalSecondSource:
     return Error(Loc, "expected compatible register or logical immediate");
   case Match_InvalidMovImm32Shift:
@@ -5912,11 +5906,11 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
   case Match_InvalidMovImm64Shift:
     return Error(Loc, "expected 'lsl' with optional integer 0, 16, 32 or 48");
   case Match_AddSubRegShift32:
-    return Error(Loc,
-       "expected 'lsl', 'lsr' or 'asr' with optional integer in range [0, 31]");
+    return Error(Loc, "expected 'lsl', 'lsr' or 'asr' with optional integer in "
+                      "range [0, 31]");
   case Match_AddSubRegShift64:
-    return Error(Loc,
-       "expected 'lsl', 'lsr' or 'asr' with optional integer in range [0, 63]");
+    return Error(Loc, "expected 'lsl', 'lsr' or 'asr' with optional integer in "
+                      "range [0, 63]");
   case Match_InvalidFPImm:
     return Error(Loc,
                  "expected compatible register or floating-point constant");
@@ -5971,8 +5965,7 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
   case Match_InvalidMemoryIndexed1UImm6:
     return Error(Loc, "index must be in range [0, 63].");
   case Match_InvalidMemoryWExtend8:
-    return Error(Loc,
-                 "expected 'uxtw' or 'sxtw' with optional shift of #0");
+    return Error(Loc, "expected 'uxtw' or 'sxtw' with optional shift of #0");
   case Match_InvalidMemoryWExtend16:
     return Error(Loc,
                  "expected 'uxtw' or 'sxtw' with optional shift of #0 or #1");
@@ -5986,8 +5979,7 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
     return Error(Loc,
                  "expected 'uxtw' or 'sxtw' with optional shift of #0 or #4");
   case Match_InvalidMemoryXExtend8:
-    return Error(Loc,
-                 "expected 'lsl' or 'sxtx' with optional shift of #0");
+    return Error(Loc, "expected 'lsl' or 'sxtx' with optional shift of #0");
   case Match_InvalidMemoryXExtend16:
     return Error(Loc,
                  "expected 'lsl' or 'sxtx' with optional shift of #0 or #1");
@@ -6127,11 +6119,14 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
   case Match_InvalidGPR64shifted8:
     return Error(Loc, "register must be x0..x30 or xzr, without shift");
   case Match_InvalidGPR64shifted16:
-    return Error(Loc, "register must be x0..x30 or xzr, with required shift 'lsl #1'");
+    return Error(
+        Loc, "register must be x0..x30 or xzr, with required shift 'lsl #1'");
   case Match_InvalidGPR64shifted32:
-    return Error(Loc, "register must be x0..x30 or xzr, with required shift 'lsl #2'");
+    return Error(
+        Loc, "register must be x0..x30 or xzr, with required shift 'lsl #2'");
   case Match_InvalidGPR64shifted64:
-    return Error(Loc, "register must be x0..x30 or xzr, with required shift 'lsl #3'");
+    return Error(
+        Loc, "register must be x0..x30 or xzr, with required shift 'lsl #3'");
   case Match_InvalidGPR64shifted128:
     return Error(
         Loc, "register must be x0..x30 or xzr, with required shift 'lsl #4'");
@@ -6147,44 +6142,60 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
     return Error(Loc, "register must be x0..x30 with required shift 'lsl #4'");
   case Match_InvalidZPR32UXTW8:
   case Match_InvalidZPR32SXTW8:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, (uxtw|sxtw)'");
+    return Error(
+        Loc,
+        "invalid shift/extend specified, expected 'z[0..31].s, (uxtw|sxtw)'");
   case Match_InvalidZPR32UXTW16:
   case Match_InvalidZPR32SXTW16:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, (uxtw|sxtw) #1'");
+    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, "
+                      "(uxtw|sxtw) #1'");
   case Match_InvalidZPR32UXTW32:
   case Match_InvalidZPR32SXTW32:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, (uxtw|sxtw) #2'");
+    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, "
+                      "(uxtw|sxtw) #2'");
   case Match_InvalidZPR32UXTW64:
   case Match_InvalidZPR32SXTW64:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, (uxtw|sxtw) #3'");
+    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, "
+                      "(uxtw|sxtw) #3'");
   case Match_InvalidZPR64UXTW8:
   case Match_InvalidZPR64SXTW8:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, (uxtw|sxtw)'");
+    return Error(
+        Loc,
+        "invalid shift/extend specified, expected 'z[0..31].d, (uxtw|sxtw)'");
   case Match_InvalidZPR64UXTW16:
   case Match_InvalidZPR64SXTW16:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, (lsl|uxtw|sxtw) #1'");
+    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, "
+                      "(lsl|uxtw|sxtw) #1'");
   case Match_InvalidZPR64UXTW32:
   case Match_InvalidZPR64SXTW32:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, (lsl|uxtw|sxtw) #2'");
+    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, "
+                      "(lsl|uxtw|sxtw) #2'");
   case Match_InvalidZPR64UXTW64:
   case Match_InvalidZPR64SXTW64:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, (lsl|uxtw|sxtw) #3'");
+    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, "
+                      "(lsl|uxtw|sxtw) #3'");
   case Match_InvalidZPR32LSL8:
     return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s'");
   case Match_InvalidZPR32LSL16:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, lsl #1'");
+    return Error(
+        Loc, "invalid shift/extend specified, expected 'z[0..31].s, lsl #1'");
   case Match_InvalidZPR32LSL32:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, lsl #2'");
+    return Error(
+        Loc, "invalid shift/extend specified, expected 'z[0..31].s, lsl #2'");
   case Match_InvalidZPR32LSL64:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].s, lsl #3'");
+    return Error(
+        Loc, "invalid shift/extend specified, expected 'z[0..31].s, lsl #3'");
   case Match_InvalidZPR64LSL8:
     return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d'");
   case Match_InvalidZPR64LSL16:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, lsl #1'");
+    return Error(
+        Loc, "invalid shift/extend specified, expected 'z[0..31].d, lsl #1'");
   case Match_InvalidZPR64LSL32:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, lsl #2'");
+    return Error(
+        Loc, "invalid shift/extend specified, expected 'z[0..31].d, lsl #2'");
   case Match_InvalidZPR64LSL64:
-    return Error(Loc, "invalid shift/extend specified, expected 'z[0..31].d, lsl #3'");
+    return Error(
+        Loc, "invalid shift/extend specified, expected 'z[0..31].d, lsl #3'");
   case Match_InvalidZPR0:
     return Error(Loc, "expected register without element width suffix");
   case Match_InvalidZPR8:
@@ -6194,20 +6205,26 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
   case Match_InvalidZPR128:
     return Error(Loc, "invalid element width");
   case Match_InvalidZPR_3b8:
-    return Error(Loc, "Invalid restricted vector register, expected z0.b..z7.b");
+    return Error(Loc,
+                 "Invalid restricted vector register, expected z0.b..z7.b");
   case Match_InvalidZPR_3b16:
-    return Error(Loc, "Invalid restricted vector register, expected z0.h..z7.h");
+    return Error(Loc,
+                 "Invalid restricted vector register, expected z0.h..z7.h");
   case Match_InvalidZPR_3b32:
-    return Error(Loc, "Invalid restricted vector register, expected z0.s..z7.s");
+    return Error(Loc,
+                 "Invalid restricted vector register, expected z0.s..z7.s");
   case Match_InvalidZPR_4b8:
     return Error(Loc,
                  "Invalid restricted vector register, expected z0.b..z15.b");
   case Match_InvalidZPR_4b16:
-    return Error(Loc, "Invalid restricted vector register, expected z0.h..z15.h");
+    return Error(Loc,
+                 "Invalid restricted vector register, expected z0.h..z15.h");
   case Match_InvalidZPR_4b32:
-    return Error(Loc, "Invalid restricted vector register, expected z0.s..z15.s");
+    return Error(Loc,
+                 "Invalid restricted vector register, expected z0.s..z15.s");
   case Match_InvalidZPR_4b64:
-    return Error(Loc, "Invalid restricted vector register, expected z0.d..z15.d");
+    return Error(Loc,
+                 "Invalid restricted vector register, expected z0.d..z15.d");
   case Match_InvalidZPRMul2_Lo8:
     return Error(Loc, "Invalid restricted vector register, expected even "
                       "register in z0.b..z14.b");
@@ -6246,7 +6263,8 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
   case Match_InvalidSVEPredicateDReg:
     return Error(Loc, "invalid predicate register.");
   case Match_InvalidSVEPredicate3bAnyReg:
-    return Error(Loc, "invalid restricted predicate register, expected p0..p7 (without element suffix)");
+    return Error(Loc, "invalid restricted predicate register, expected p0..p7 "
+                      "(without element suffix)");
   case Match_InvalidSVEPNPredicateB_p8to15Reg:
   case Match_InvalidSVEPNPredicateH_p8to15Reg:
   case Match_InvalidSVEPNPredicateS_p8to15Reg:
@@ -6268,9 +6286,10 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
   case Match_InvalidSVEPredicateListMul2x16:
   case Match_InvalidSVEPredicateListMul2x32:
   case Match_InvalidSVEPredicateListMul2x64:
-    return Error(Loc, "Invalid vector list, expected list with 2 consecutive "
-                      "predicate registers, where the first vector is a multiple of 2 "
-                      "and with correct element type");
+    return Error(
+        Loc, "Invalid vector list, expected list with 2 consecutive "
+             "predicate registers, where the first vector is a multiple of 2 "
+             "and with correct element type");
   case Match_InvalidSVEExactFPImmOperandHalfOne:
     return Error(Loc, "Invalid floating point constant, expected 0.5 or 1.0.");
   case Match_InvalidSVEExactFPImmOperandHalfTwo:
@@ -6370,8 +6389,7 @@ bool AArch64AsmParser::showMatchError(SMLoc Loc, unsigned ErrCode,
         "4 registers apart, and the first register in the range [z0, z3] or "
         "[z16, z19] and with correct element type");
   case Match_AddSubLSLImm3ShiftLarge:
-    return Error(Loc,
-      "expected 'lsl' with optional integer in range [0, 7]");
+    return Error(Loc, "expected 'lsl' with optional integer in range [0, 7]");
   default:
     llvm_unreachable("unexpected error code!");
   }
@@ -6428,7 +6446,8 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
 
     if (Op1.isScalarReg() && LSBOp.isImm() && WidthOp.isImm()) {
       const MCConstantExpr *LSBCE = dyn_cast<MCConstantExpr>(LSBOp.getImm());
-      const MCConstantExpr *WidthCE = dyn_cast<MCConstantExpr>(WidthOp.getImm());
+      const MCConstantExpr *WidthCE =
+          dyn_cast<MCConstantExpr>(WidthOp.getImm());
 
       if (LSBCE && WidthCE) {
         uint64_t LSB = LSBCE->getValue();
@@ -6608,7 +6627,8 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
       StringRef Suffix = Op1.isToken() ? Op1.getToken() : Op2.getToken();
       if (Suffix.lower() == ".2d" &&
           cast<MCConstantExpr>(Op3.getImm())->getValue() == 0) {
-        Warning(IDLoc, "instruction movi.2d with immediate #0 may not function"
+        Warning(IDLoc,
+                "instruction movi.2d with immediate #0 may not function"
                 " correctly on this CPU, converting to equivalent movi.16b");
         // Switch the suffix to .16b.
         unsigned Idx = Op1.isToken() ? 1 : 2;
@@ -6627,9 +6647,8 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
     AArch64Operand &Op = static_cast<AArch64Operand &>(*Operands[2]);
     if (Op.isScalarReg()) {
       MCRegister Reg = getXRegFromWReg(Op.getReg());
-      Operands[2] = AArch64Operand::CreateReg(Reg, RegKind::Scalar,
-                                              Op.getStartLoc(), Op.getEndLoc(),
-                                              getContext());
+      Operands[2] = AArch64Operand::CreateReg(
+          Reg, RegKind::Scalar, Op.getStartLoc(), Op.getEndLoc(), getContext());
     }
   }
   // FIXME: Likewise for sxt[bh] with a Xd dst operand
@@ -6643,9 +6662,9 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
       AArch64Operand &Op = static_cast<AArch64Operand &>(*Operands[2]);
       if (Op.isScalarReg()) {
         MCRegister Reg = getXRegFromWReg(Op.getReg());
-        Operands[2] = AArch64Operand::CreateReg(Reg, RegKind::Scalar,
-                                                Op.getStartLoc(),
-                                                Op.getEndLoc(), getContext());
+        Operands[2] =
+            AArch64Operand::CreateReg(Reg, RegKind::Scalar, Op.getStartLoc(),
+                                      Op.getEndLoc(), getContext());
       }
     }
   }
@@ -6660,9 +6679,9 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
       AArch64Operand &Op = static_cast<AArch64Operand &>(*Operands[1]);
       if (Op.isScalarReg()) {
         MCRegister Reg = getWRegFromXReg(Op.getReg());
-        Operands[1] = AArch64Operand::CreateReg(Reg, RegKind::Scalar,
-                                                Op.getStartLoc(),
-                                                Op.getEndLoc(), getContext());
+        Operands[1] =
+            AArch64Operand::CreateReg(Reg, RegKind::Scalar, Op.getStartLoc(),
+                                      Op.getEndLoc(), getContext());
       }
     }
   }
@@ -6671,9 +6690,8 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
   FeatureBitset MissingFeatures;
   // First try to match against the secondary set of tables containing the
   // short-form NEON instructions (e.g. "fadd.2s v0, v1, v2").
-  unsigned MatchResult =
-      MatchInstructionImpl(Operands, Inst, ErrorInfo, MissingFeatures,
-                           MatchingInlineAsm, 1);
+  unsigned MatchResult = MatchInstructionImpl(
+      Operands, Inst, ErrorInfo, MissingFeatures, MatchingInlineAsm, 1);
 
   // If that fails, try against the alternate table containing long-form NEON:
   // "fadd v0.2s, v1.2s, v2.2s"
@@ -6684,9 +6702,8 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
     auto ShortFormNEONMatchResult = MatchResult;
     auto ShortFormNEONMissingFeatures = MissingFeatures;
 
-    MatchResult =
-        MatchInstructionImpl(Operands, Inst, ErrorInfo, MissingFeatures,
-                             MatchingInlineAsm, 0);
+    MatchResult = MatchInstructionImpl(Operands, Inst, ErrorInfo,
+                                       MissingFeatures, MatchingInlineAsm, 0);
 
     // Now, both matches failed, and the long-form match failed on the mnemonic
     // suffix token operand.  The short-form match failure is probably more
@@ -6978,7 +6995,8 @@ bool AArch64AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
   case Match_MSR:
   case Match_MRS: {
     if (ErrorInfo >= Operands.size())
-      return Error(IDLoc, "too few operands for instruction", SMRange(IDLoc, (*Operands.back()).getEndLoc()));
+      return Error(IDLoc, "too few operands for instruction",
+                   SMRange(IDLoc, (*Operands.back()).getEndLoc()));
     // Any time we get here, there's nothing fancy to do. Just get the
     // operand SMLoc and display the diagnostic.
     SMLoc ErrorLoc = ((AArch64Operand &)*Operands[ErrorInfo]).getStartLoc();
@@ -7176,7 +7194,8 @@ bool AArch64AsmParser::parseDirectiveArch(SMLoc L) {
   AArch64::getExtensionFeatures(ArchInfo->DefaultExts, AArch64Features);
 
   MCSubtargetInfo &STI = copySTI();
-  std::vector<std::string> ArchFeatures(AArch64Features.begin(), AArch64Features.end());
+  std::vector<std::string> ArchFeatures(AArch64Features.begin(),
+                                        AArch64Features.end());
   STI.setDefaultFeatures("generic", /*TuneCPU*/ "generic",
                          join(ArchFeatures.begin(), ArchFeatures.end(), ","));
 
@@ -7418,8 +7437,7 @@ bool AArch64AsmParser::parseDirectiveReq(StringRef Name, SMLoc L) {
   if (!ParseRes.isSuccess()) {
     StringRef Kind;
     RegisterKind = RegKind::SVEDataVector;
-    ParseRes =
-        tryParseVectorRegister(RegNum, Kind, RegKind::SVEDataVector);
+    ParseRes = tryParseVectorRegister(RegNum, Kind, RegKind::SVEDataVector);
 
     if (ParseRes.isFailure())
       return true;
@@ -7432,7 +7450,8 @@ bool AArch64AsmParser::parseDirectiveReq(StringRef Name, SMLoc L) {
   if (!ParseRes.isSuccess()) {
     StringRef Kind;
     RegisterKind = RegKind::SVEPredicateVector;
-    ParseRes = tryParseVectorRegister(RegNum, Kind, RegKind::SVEPredicateVector);
+    ParseRes =
+        tryParseVectorRegister(RegNum, Kind, RegKind::SVEPredicateVector);
 
     if (ParseRes.isFailure())
       return true;
@@ -7449,7 +7468,7 @@ bool AArch64AsmParser::parseDirectiveReq(StringRef Name, SMLoc L) {
   if (parseEOL())
     return true;
 
-  auto pair = std::make_pair(RegisterKind, (unsigned) RegNum);
+  auto pair = std::make_pair(RegisterKind, (unsigned)RegNum);
   if (RegisterReqs.insert(std::make_pair(Name, pair)).first->second != pair)
     Warning(L, "ignoring redefinition of register alias '" + Name + "'");
 
@@ -8197,11 +8216,9 @@ bool AArch64AsmParser::parseAuthExpr(const MCExpr *&Res, SMLoc &EndLoc) {
   return false;
 }
 
-bool
-AArch64AsmParser::classifySymbolRef(const MCExpr *Expr,
-                                    AArch64MCExpr::VariantKind &ELFRefKind,
-                                    MCSymbolRefExpr::VariantKind &DarwinRefKind,
-                                    int64_t &Addend) {
+bool AArch64AsmParser::classifySymbolRef(
+    const MCExpr *Expr, AArch64MCExpr::VariantKind &ELFRefKind,
+    MCSymbolRefExpr::VariantKind &DarwinRefKind, int64_t &Addend) {
   ELFRefKind = AArch64MCExpr::VK_INVALID;
   DarwinRefKind = MCSymbolRefExpr::VK_None;
   Addend = 0;
@@ -8376,15 +8393,17 @@ ParseStatus AArch64AsmParser::tryParseGPRSeqPair(OperandVector &Operands) {
 
   MCRegister Pair;
   if (isXReg) {
-    Pair = RI->getMatchingSuperReg(FirstReg, AArch64::sube64,
-           &AArch64MCRegisterClasses[AArch64::XSeqPairsClassRegClassID]);
+    Pair = RI->getMatchingSuperReg(
+        FirstReg, AArch64::sube64,
+        &AArch64MCRegisterClasses[AArch64::XSeqPairsClassRegClassID]);
   } else {
-    Pair = RI->getMatchingSuperReg(FirstReg, AArch64::sube32,
-           &AArch64MCRegisterClasses[AArch64::WSeqPairsClassRegClassID]);
+    Pair = RI->getMatchingSuperReg(
+        FirstReg, AArch64::sube32,
+        &AArch64MCRegisterClasses[AArch64::WSeqPairsClassRegClassID]);
   }
 
   Operands.push_back(AArch64Operand::CreateReg(Pair, RegKind::Scalar, S,
-      getLoc(), getContext()));
+                                               getLoc(), getContext()));
 
   return ParseStatus::Success;
 }
diff --git a/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp b/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
index 8b1c16d31..9bcd1860f 100644
--- a/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
+++ b/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp
@@ -10,11 +10,11 @@
 //===----------------------------------------------------------------------===//
 
 #include "AArch64Disassembler.h"
+#include "../MCTargetDesc/AArch64AddressingModes.h"
+#include "../MCTargetDesc/AArch64MCTargetDesc.h"
+#include "../TargetInfo/AArch64TargetInfo.h"
+#include "../Utils/AArch64BaseInfo.h"
 #include "AArch64ExternalSymbolizer.h"
-#include "MCTargetDesc/AArch64AddressingModes.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
-#include "TargetInfo/AArch64TargetInfo.h"
-#include "Utils/AArch64BaseInfo.h"
 #include "llvm/MC/MCDecoderOps.h"
 #include "llvm/MC/MCDisassembler/MCRelocationInfo.h"
 #include "llvm/MC/MCInst.h"
diff --git a/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp b/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp
index 09d706f0a..48be0eefa 100644
--- a/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp
+++ b/llvm/lib/Target/AArch64/Disassembler/AArch64ExternalSymbolizer.cpp
@@ -7,7 +7,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "AArch64ExternalSymbolizer.h"
-#include "Utils/AArch64BaseInfo.h"
+#include "../Utils/AArch64BaseInfo.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCExpr.h"
 #include "llvm/MC/MCInst.h"
@@ -87,22 +87,21 @@ bool AArch64ExternalSymbolizer::tryAddingSymbolicOperand(
       }
       if (ReferenceType == LLVMDisassembler_ReferenceType_Out_SymbolStub)
         CommentStream << "symbol stub for: " << ReferenceName;
-      else if (ReferenceType ==
-               LLVMDisassembler_ReferenceType_Out_Objc_Message)
+      else if (ReferenceType == LLVMDisassembler_ReferenceType_Out_Objc_Message)
         CommentStream << "Objc message: " << ReferenceName;
     } else if (MI.getOpcode() == AArch64::ADRP) {
-        ReferenceType = LLVMDisassembler_ReferenceType_In_ARM64_ADRP;
-        // otool expects the fully encoded ADRP instruction to be passed in as
-        // the value here, so reconstruct it:
-        const MCRegisterInfo &MCRI = *Ctx.getRegisterInfo();
-        uint32_t EncodedInst = 0x90000000;
-        EncodedInst |= (Value & 0x3) << 29; // immlo
-        EncodedInst |= ((Value >> 2) & 0x7FFFF) << 5; // immhi
-        EncodedInst |= MCRI.getEncodingValue(MI.getOperand(0).getReg()); // reg
-        SymbolLookUp(DisInfo, EncodedInst, &ReferenceType, Address,
-                     &ReferenceName);
-        CommentStream << format("0x%llx", (0xfffffffffffff000LL & Address) +
-                                              Value * 0x1000);
+      ReferenceType = LLVMDisassembler_ReferenceType_In_ARM64_ADRP;
+      // otool expects the fully encoded ADRP instruction to be passed in as
+      // the value here, so reconstruct it:
+      const MCRegisterInfo &MCRI = *Ctx.getRegisterInfo();
+      uint32_t EncodedInst = 0x90000000;
+      EncodedInst |= (Value & 0x3) << 29;                              // immlo
+      EncodedInst |= ((Value >> 2) & 0x7FFFF) << 5;                    // immhi
+      EncodedInst |= MCRI.getEncodingValue(MI.getOperand(0).getReg()); // reg
+      SymbolLookUp(DisInfo, EncodedInst, &ReferenceType, Address,
+                   &ReferenceName);
+      CommentStream << format("0x%llx", (0xfffffffffffff000LL & Address) +
+                                            Value * 0x1000);
     } else if (MI.getOpcode() == AArch64::ADDXri ||
                MI.getOpcode() == AArch64::LDRXui ||
                MI.getOpcode() == AArch64::LDRXl ||
@@ -118,16 +117,16 @@ bool AArch64ExternalSymbolizer::tryAddingSymbolicOperand(
       } else if (MI.getOpcode() == AArch64::ADR) {
         ReferenceType = LLVMDisassembler_ReferenceType_In_ARM64_ADR;
         SymbolLookUp(DisInfo, Address + Value, &ReferenceType, Address,
-                            &ReferenceName);
+                     &ReferenceName);
       } else {
         const MCRegisterInfo &MCRI = *Ctx.getRegisterInfo();
         // otool expects the fully encoded ADD/LDR instruction to be passed in
         // as the value here, so reconstruct it:
         unsigned EncodedInst =
-          MI.getOpcode() == AArch64::ADDXri ? 0x91000000: 0xF9400000;
+            MI.getOpcode() == AArch64::ADDXri ? 0x91000000 : 0xF9400000;
         EncodedInst |= Value << 10; // imm12 [+ shift:2 for ADD]
-        EncodedInst |=
-          MCRI.getEncodingValue(MI.getOperand(1).getReg()) << 5; // Rn
+        EncodedInst |= MCRI.getEncodingValue(MI.getOperand(1).getReg())
+                       << 5;                                             // Rn
         EncodedInst |= MCRI.getEncodingValue(MI.getOperand(0).getReg()); // Rd
 
         SymbolLookUp(DisInfo, EncodedInst, &ReferenceType, Address,
@@ -141,10 +140,9 @@ bool AArch64ExternalSymbolizer::tryAddingSymbolicOperand(
         CommentStream.write_escaped(ReferenceName);
         CommentStream << "\"";
       } else if (ReferenceType ==
-               LLVMDisassembler_ReferenceType_Out_Objc_CFString_Ref)
+                 LLVMDisassembler_ReferenceType_Out_Objc_CFString_Ref)
         CommentStream << "Objc cfstring ref: @\"" << ReferenceName << "\"";
-      else if (ReferenceType ==
-               LLVMDisassembler_ReferenceType_Out_Objc_Message)
+      else if (ReferenceType == LLVMDisassembler_ReferenceType_Out_Objc_Message)
         CommentStream << "Objc message: " << ReferenceName;
       else if (ReferenceType ==
                LLVMDisassembler_ReferenceType_Out_Objc_Message_Ref)
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp b/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
index e4719b26c..6c1818efd 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64CallLowering.cpp
@@ -13,12 +13,12 @@
 //===----------------------------------------------------------------------===//
 
 #include "AArch64CallLowering.h"
+#include "../AArch64ISelLowering.h"
+#include "../AArch64MachineFunctionInfo.h"
+#include "../AArch64RegisterInfo.h"
+#include "../AArch64Subtarget.h"
+#include "../Utils/AArch64SMEAttributes.h"
 #include "AArch64GlobalISelUtils.h"
-#include "AArch64ISelLowering.h"
-#include "AArch64MachineFunctionInfo.h"
-#include "AArch64RegisterInfo.h"
-#include "AArch64Subtarget.h"
-#include "Utils/AArch64SMEAttributes.h"
 #include "llvm/ADT/ArrayRef.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/Analysis/ObjCARCUtil.h"
@@ -56,7 +56,7 @@ using namespace AArch64GISelUtils;
 extern cl::opt<bool> EnableSVEGISel;
 
 AArch64CallLowering::AArch64CallLowering(const AArch64TargetLowering &TLI)
-  : CallLowering(&TLI) {}
+    : CallLowering(&TLI) {}
 
 static void applyStackPassedSmallTypeDAGHack(EVT OrigVT, MVT &ValVT,
                                              MVT &LocVT) {
@@ -535,7 +535,8 @@ bool AArch64CallLowering::fallBackToDAGISel(const MachineFunction &MF) const {
     return true;
   const auto &ST = MF.getSubtarget<AArch64Subtarget>();
   if (!ST.hasNEON() || !ST.hasFPARMv8()) {
-    LLVM_DEBUG(dbgs() << "Falling back to SDAG because we don't support no-NEON\n");
+    LLVM_DEBUG(
+        dbgs() << "Falling back to SDAG because we don't support no-NEON\n");
     return true;
   }
 
@@ -592,8 +593,8 @@ void AArch64CallLowering::saveVarArgRegisters(
           CCValAssign::getReg(i + MF.getFunction().getNumOperands(), MVT::i64,
                               GPRArgRegs[i], MVT::i64, CCValAssign::Full));
       auto MPO = IsWin64CC ? MachinePointerInfo::getFixedStack(
-                               MF, GPRIdx, (i - FirstVariadicGPR) * 8)
-                         : MachinePointerInfo::getStack(MF, i * 8);
+                                 MF, GPRIdx, (i - FirstVariadicGPR) * 8)
+                           : MachinePointerInfo::getStack(MF, i * 8);
       MIRBuilder.buildStore(Val, FIN, MPO, inferAlignFromPtrInfo(MF, MPO));
 
       FIN = MIRBuilder.buildPtrAdd(MRI.createGenericVirtualRegister(p0),
@@ -703,7 +704,8 @@ bool AArch64CallLowering::lowerFormalArguments(
     MIRBuilder.setInstr(*MBB.begin());
 
   const AArch64TargetLowering &TLI = *getTLI<AArch64TargetLowering>();
-  CCAssignFn *AssignFn = TLI.CCAssignFnForCall(F.getCallingConv(), IsWin64 && F.isVarArg());
+  CCAssignFn *AssignFn =
+      TLI.CCAssignFnForCall(F.getCallingConv(), IsWin64 && F.isVarArg());
 
   AArch64IncomingValueAssigner Assigner(AssignFn, AssignFn);
   FormalArgHandler Handler(MIRBuilder, MRI);
@@ -728,7 +730,8 @@ bool AArch64CallLowering::lowerFormalArguments(
   AArch64FunctionInfo *FuncInfo = MF.getInfo<AArch64FunctionInfo>();
   uint64_t StackSize = Assigner.StackSize;
   if (F.isVarArg()) {
-    if ((!Subtarget.isTargetDarwin() && !Subtarget.isWindowsArm64EC()) || IsWin64) {
+    if ((!Subtarget.isTargetDarwin() && !Subtarget.isWindowsArm64EC()) ||
+        IsWin64) {
       // The AAPCS variadic function ABI is identical to the non-variadic
       // one. As a result there may be more arguments in registers and we should
       // save them for future reference.
@@ -919,8 +922,7 @@ bool AArch64CallLowering::areCalleeOutgoingArgsTailCallable(
 
 bool AArch64CallLowering::isEligibleForTailCallOptimization(
     MachineIRBuilder &MIRBuilder, CallLoweringInfo &Info,
-    SmallVectorImpl<ArgInfo> &InArgs,
-    SmallVectorImpl<ArgInfo> &OutArgs) const {
+    SmallVectorImpl<ArgInfo> &InArgs, SmallVectorImpl<ArgInfo> &OutArgs) const {
 
   // Must pass all target-independent checks in order to tail call optimize.
   if (!Info.IsTailCall)
@@ -1014,8 +1016,7 @@ bool AArch64CallLowering::isEligibleForTailCallOptimization(
   if (!areCalleeOutgoingArgsTailCallable(Info, MF, OutArgs))
     return false;
 
-  LLVM_DEBUG(
-      dbgs() << "... Call is eligible for tail call optimization.\n");
+  LLVM_DEBUG(dbgs() << "... Call is eligible for tail call optimization.\n");
   return true;
 }
 
@@ -1438,7 +1439,7 @@ bool AArch64CallLowering::lowerCall(MachineIRBuilder &MIRBuilder,
   // Finally we can copy the returned value back into its virtual-register. In
   // symmetry with the arguments, the physical register must be an
   // implicit-define of the call instruction.
-  if (Info.CanLowerReturn  && !Info.OrigRet.Ty->isVoidTy()) {
+  if (Info.CanLowerReturn && !Info.OrigRet.Ty->isVoidTy()) {
     CCAssignFn *RetAssignFn = TLI.CCAssignFnForReturn(Info.CallConv);
     CallReturnHandler Handler(MIRBuilder, MRI, MIB);
     bool UsingReturnedArg =
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.h b/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.h
index 9ef833f0f..56a73ea82 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.h
+++ b/llvm/lib/Target/AArch64/GISel/AArch64GlobalISelUtils.h
@@ -11,8 +11,8 @@
 
 #ifndef LLVM_LIB_TARGET_AARCH64_GISEL_AARCH64GLOBALISELUTILS_H
 #define LLVM_LIB_TARGET_AARCH64_GISEL_AARCH64GLOBALISELUTILS_H
-#include "MCTargetDesc/AArch64AddressingModes.h"
-#include "Utils/AArch64BaseInfo.h"
+#include "../MCTargetDesc/AArch64AddressingModes.h"
+#include "../Utils/AArch64BaseInfo.h"
 #include "llvm/CodeGen/GlobalISel/MachineIRBuilder.h"
 #include "llvm/CodeGen/GlobalISel/Utils.h"
 #include "llvm/CodeGen/Register.h"
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp b/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
index 07f036443..c26be4963 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp
@@ -11,15 +11,15 @@
 /// \todo This should be generated by TableGen.
 //===----------------------------------------------------------------------===//
 
+#include "../AArch64InstrInfo.h"
+#include "../AArch64MachineFunctionInfo.h"
+#include "../AArch64RegisterInfo.h"
+#include "../AArch64Subtarget.h"
+#include "../AArch64TargetMachine.h"
+#include "../MCTargetDesc/AArch64AddressingModes.h"
+#include "../MCTargetDesc/AArch64MCTargetDesc.h"
 #include "AArch64GlobalISelUtils.h"
-#include "AArch64InstrInfo.h"
-#include "AArch64MachineFunctionInfo.h"
 #include "AArch64RegisterBankInfo.h"
-#include "AArch64RegisterInfo.h"
-#include "AArch64Subtarget.h"
-#include "AArch64TargetMachine.h"
-#include "MCTargetDesc/AArch64AddressingModes.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
 #include "llvm/BinaryFormat/Dwarf.h"
 #include "llvm/CodeGen/GlobalISel/GIMatchTableExecutorImpl.h"
 #include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
@@ -57,7 +57,7 @@ using namespace AArch64GISelUtils;
 namespace llvm {
 class BlockFrequencyInfo;
 class ProfileSummaryInfo;
-}
+} // namespace llvm
 
 namespace {
 
@@ -65,7 +65,6 @@ namespace {
 #include "AArch64GenGlobalISel.inc"
 #undef GET_GLOBALISEL_PREDICATE_BITSET
 
-
 class AArch64InstructionSelector : public InstructionSelector {
 public:
   AArch64InstructionSelector(const AArch64TargetMachine &TM,
@@ -427,10 +426,11 @@ private:
   /// Returns a \p ComplexRendererFns which contains a base, offset, and whether
   /// or not a shift + extend should be folded into an addressing mode. Returns
   /// None when this is not profitable or possible.
-  ComplexRendererFns
-  selectExtendedSHL(MachineOperand &Root, MachineOperand &Base,
-                    MachineOperand &Offset, unsigned SizeInBytes,
-                    bool WantsExt) const;
+  ComplexRendererFns selectExtendedSHL(MachineOperand &Root,
+                                       MachineOperand &Base,
+                                       MachineOperand &Offset,
+                                       unsigned SizeInBytes,
+                                       bool WantsExt) const;
   ComplexRendererFns selectAddrModeRegisterOffset(MachineOperand &Root) const;
   ComplexRendererFns selectAddrModeXRO(MachineOperand &Root,
                                        unsigned SizeInBytes) const;
@@ -1818,8 +1818,9 @@ bool AArch64InstructionSelector::selectCompareBranchFedByICmp(
   return true;
 }
 
-bool AArch64InstructionSelector::selectCompareBranch(
-    MachineInstr &I, MachineFunction &MF, MachineRegisterInfo &MRI) {
+bool AArch64InstructionSelector::selectCompareBranch(MachineInstr &I,
+                                                     MachineFunction &MF,
+                                                     MachineRegisterInfo &MRI) {
   Register CondReg = I.getOperand(0).getReg();
   MachineInstr *CCMI = MRI.getVRegDef(CondReg);
   // Try to select the G_BRCOND using whatever is feeding the condition if
@@ -2138,8 +2139,9 @@ bool AArch64InstructionSelector::selectVaStartDarwin(
   return true;
 }
 
-void AArch64InstructionSelector::materializeLargeCMVal(
-    MachineInstr &I, const Value *V, unsigned OpFlags) {
+void AArch64InstructionSelector::materializeLargeCMVal(MachineInstr &I,
+                                                       const Value *V,
+                                                       unsigned OpFlags) {
   MachineBasicBlock &MBB = *I.getParent();
   MachineFunction &MF = *MBB.getParent();
   MachineRegisterInfo &MRI = MF.getRegInfo();
@@ -2169,8 +2171,8 @@ void AArch64InstructionSelector::materializeLargeCMVal(
     constrainSelectedInstRegOperands(*MovI, TII, TRI, RBI);
     return DstReg;
   };
-  Register DstReg = BuildMovK(MovZ.getReg(0),
-                              AArch64II::MO_G1 | AArch64II::MO_NC, 16, 0);
+  Register DstReg =
+      BuildMovK(MovZ.getReg(0), AArch64II::MO_G1 | AArch64II::MO_NC, 16, 0);
   DstReg = BuildMovK(DstReg, AArch64II::MO_G2 | AArch64II::MO_NC, 32, 0);
   BuildMovK(DstReg, AArch64II::MO_G3, 48, I.getOperand(0).getReg());
 }
@@ -2271,8 +2273,8 @@ bool AArch64InstructionSelector::preISelLower(MachineInstr &I) {
 /// because the selector works bottom up, uses before defs. By the time we
 /// end up trying to select a G_PTR_ADD, we should have already attempted to
 /// fold this into addressing modes and were therefore unsuccessful.
-bool AArch64InstructionSelector::convertPtrAddToAdd(
-    MachineInstr &I, MachineRegisterInfo &MRI) {
+bool AArch64InstructionSelector::convertPtrAddToAdd(MachineInstr &I,
+                                                    MachineRegisterInfo &MRI) {
   assert(I.getOpcode() == TargetOpcode::G_PTR_ADD && "Expected G_PTR_ADD");
   Register DstReg = I.getOperand(0).getReg();
   Register AddOp1Reg = I.getOperand(1).getReg();
@@ -2579,7 +2581,7 @@ bool AArch64InstructionSelector::select(MachineInstr &I) {
   if (!I.isPreISelOpcode() || Opcode == TargetOpcode::G_PHI) {
     // Certain non-generic instructions also need some special handling.
 
-    if (Opcode ==  TargetOpcode::LOAD_STACK_GUARD)
+    if (Opcode == TargetOpcode::LOAD_STACK_GUARD)
       return constrainSelectedInstRegOperands(I, TII, TRI, RBI);
 
     if (Opcode == TargetOpcode::PHI || Opcode == TargetOpcode::G_PHI) {
@@ -2587,7 +2589,7 @@ bool AArch64InstructionSelector::select(MachineInstr &I) {
       const LLT DefTy = MRI.getType(DefReg);
 
       const RegClassOrRegBank &RegClassOrBank =
-        MRI.getRegClassOrRegBank(DefReg);
+          MRI.getRegClassOrRegBank(DefReg);
 
       const TargetRegisterClass *DefRC =
           dyn_cast<const TargetRegisterClass *>(RegClassOrBank);
@@ -2618,7 +2620,6 @@ bool AArch64InstructionSelector::select(MachineInstr &I) {
     return true;
   }
 
-
   if (I.getNumOperands() != I.getNumExplicitOperands()) {
     LLVM_DEBUG(
         dbgs() << "Generic instruction has unexpected implicit operands\n");
@@ -3074,8 +3075,8 @@ bool AArch64InstructionSelector::select(MachineInstr &I) {
       RBI.constrainGenericRegister(Copy, *RC, MRI);
       LdSt.getOperand(0).setReg(Copy);
     } else if (isa<GLoad>(LdSt) && ValTy.getSizeInBits() > MemSizeInBits) {
-      // If this is an any-extending load from the FPR bank, split it into a regular
-      // load + extend.
+      // If this is an any-extending load from the FPR bank, split it into a
+      // regular load + extend.
       if (RB.getID() == AArch64::FPRRegBankID) {
         unsigned SubReg;
         LLT MemTy = LdSt.getMMO().getMemoryType();
@@ -3109,8 +3110,7 @@ bool AArch64InstructionSelector::select(MachineInstr &I) {
       if (NewOpc == I.getOpcode())
         return nullptr;
       // Check if we can fold anything into the addressing mode.
-      auto AddrModeFns =
-          selectAddrModeIndexed(I.getOperand(1), MemSizeInBytes);
+      auto AddrModeFns = selectAddrModeIndexed(I.getOperand(1), MemSizeInBytes);
       if (!AddrModeFns) {
         // Can't fold anything. Use the original instruction.
         I.setDesc(TII.get(NewOpc));
@@ -3484,14 +3484,14 @@ bool AArch64InstructionSelector::select(MachineInstr &I) {
       }
 
       ExtI = MIB.buildInstr(IsSigned ? AArch64::SBFMXri : AArch64::UBFMXri,
-                             {DefReg}, {SrcReg})
-                  .addImm(0)
-                  .addImm(SrcSize - 1);
+                            {DefReg}, {SrcReg})
+                 .addImm(0)
+                 .addImm(SrcSize - 1);
     } else if (DstSize <= 32) {
       ExtI = MIB.buildInstr(IsSigned ? AArch64::SBFMWri : AArch64::UBFMWri,
-                             {DefReg}, {SrcReg})
-                  .addImm(0)
-                  .addImm(SrcSize - 1);
+                            {DefReg}, {SrcReg})
+                 .addImm(0)
+                 .addImm(SrcSize - 1);
     } else {
       return false;
     }
@@ -3802,7 +3802,7 @@ bool AArch64InstructionSelector::selectJumpTable(MachineInstr &I,
   unsigned JTI = I.getOperand(1).getIndex();
   // We generate a MOVaddrJT which will get expanded to an ADRP + ADD later.
   auto MovMI =
-    MIB.buildInstr(AArch64::MOVaddrJT, {DstReg}, {})
+      MIB.buildInstr(AArch64::MOVaddrJT, {DstReg}, {})
           .addJumpTableIndex(JTI, AArch64II::MO_PAGE)
           .addJumpTableIndex(JTI, AArch64II::MO_NC | AArch64II::MO_PAGEOFF);
   I.eraseFromParent();
@@ -3907,8 +3907,8 @@ AArch64InstructionSelector::emitNarrowVector(Register DstReg, Register SrcReg,
   return Copy;
 }
 
-bool AArch64InstructionSelector::selectMergeValues(
-    MachineInstr &I, MachineRegisterInfo &MRI) {
+bool AArch64InstructionSelector::selectMergeValues(MachineInstr &I,
+                                                   MachineRegisterInfo &MRI) {
   assert(I.getOpcode() == TargetOpcode::G_MERGE_VALUES && "unexpected opcode");
   const LLT DstTy = MRI.getType(I.getOperand(0).getReg());
   const LLT SrcTy = MRI.getType(I.getOperand(1).getReg());
@@ -3957,11 +3957,11 @@ bool AArch64InstructionSelector::selectMergeValues(
   Register SubToRegDef2 = MRI.createVirtualRegister(DstRC);
   // Need to anyext the second scalar before we can use bfm
   MachineInstr &SubRegMI2 = *BuildMI(*I.getParent(), I, I.getDebugLoc(),
-                                    TII.get(TargetOpcode::SUBREG_TO_REG))
-                                .addDef(SubToRegDef2)
-                                .addImm(0)
-                                .addUse(I.getOperand(2).getReg())
-                                .addImm(AArch64::sub_32);
+                                     TII.get(TargetOpcode::SUBREG_TO_REG))
+                                 .addDef(SubToRegDef2)
+                                 .addImm(0)
+                                 .addUse(I.getOperand(2).getReg())
+                                 .addImm(AArch64::sub_32);
   MachineInstr &BFM =
       *BuildMI(*I.getParent(), I, I.getDebugLoc(), TII.get(AArch64::BFMXri))
            .addDef(I.getOperand(0).getReg())
@@ -4065,8 +4065,8 @@ MachineInstr *AArch64InstructionSelector::emitExtractVectorElt(
   return LaneCopyMI;
 }
 
-bool AArch64InstructionSelector::selectExtractElt(
-    MachineInstr &I, MachineRegisterInfo &MRI) {
+bool AArch64InstructionSelector::selectExtractElt(MachineInstr &I,
+                                                  MachineRegisterInfo &MRI) {
   assert(I.getOpcode() == TargetOpcode::G_EXTRACT_VECTOR_ELT &&
          "unexpected opcode!");
   Register DstReg = I.getOperand(0).getReg();
@@ -4093,10 +4093,9 @@ bool AArch64InstructionSelector::selectExtractElt(
     return false;
   unsigned LaneIdx = VRegAndVal->Value.getSExtValue();
 
-
   const RegisterBank &DstRB = *RBI.getRegBank(DstReg, MRI, TRI);
-  MachineInstr *Extract = emitExtractVectorElt(DstReg, DstRB, NarrowTy, SrcReg,
-                                               LaneIdx, MIB);
+  MachineInstr *Extract =
+      emitExtractVectorElt(DstReg, DstRB, NarrowTy, SrcReg, LaneIdx, MIB);
   if (!Extract)
     return false;
 
@@ -4252,8 +4251,8 @@ bool AArch64InstructionSelector::selectUnmergeValues(MachineInstr &I,
   return true;
 }
 
-bool AArch64InstructionSelector::selectConcatVectors(
-    MachineInstr &I, MachineRegisterInfo &MRI)  {
+bool AArch64InstructionSelector::selectConcatVectors(MachineInstr &I,
+                                                     MachineRegisterInfo &MRI) {
   assert(I.getOpcode() == TargetOpcode::G_CONCAT_VECTORS &&
          "Unexpected opcode");
   Register Dst = I.getOperand(0).getReg();
@@ -4893,8 +4892,7 @@ MachineInstr *AArch64InstructionSelector::emitConditionalComparison(
   }
   AArch64CC::CondCode InvOutCC = AArch64CC::getInvertedCondCode(OutCC);
   unsigned NZCV = AArch64CC::getNZCVToSatisfyCondCode(InvOutCC);
-  auto CCmp =
-      MIB.buildInstr(CCmpOpc, {}, {LHS});
+  auto CCmp = MIB.buildInstr(CCmpOpc, {}, {LHS});
   if (CCmpOpc == AArch64::CCMPWi || CCmpOpc == AArch64::CCMPXi)
     CCmp.addImm(C->Value.getZExtValue());
   else if (CCmpOpc == AArch64::CCMNWi || CCmpOpc == AArch64::CCMNXi)
@@ -5037,7 +5035,8 @@ bool AArch64InstructionSelector::tryOptSelectConjunction(GSelect &SelI,
   if (!ConjMI)
     return false;
 
-  emitSelect(SelI.getReg(0), SelI.getTrueReg(), SelI.getFalseReg(), AArch64CC, MIB);
+  emitSelect(SelI.getReg(0), SelI.getTrueReg(), SelI.getFalseReg(), AArch64CC,
+             MIB);
   SelI.eraseFromParent();
   return true;
 }
@@ -5177,15 +5176,14 @@ MachineInstr *AArch64InstructionSelector::tryFoldIntegerCompare(
     if (!ValAndVReg || ValAndVReg->Value != 0)
       return nullptr;
 
-    return emitTST(LHSDef->getOperand(1),
-                   LHSDef->getOperand(2), MIRBuilder);
+    return emitTST(LHSDef->getOperand(1), LHSDef->getOperand(2), MIRBuilder);
   }
 
   return nullptr;
 }
 
-bool AArch64InstructionSelector::selectShuffleVector(
-    MachineInstr &I, MachineRegisterInfo &MRI) {
+bool AArch64InstructionSelector::selectShuffleVector(MachineInstr &I,
+                                                     MachineRegisterInfo &MRI) {
   const LLT DstTy = MRI.getType(I.getOperand(0).getReg());
   Register Src1Reg = I.getOperand(1).getReg();
   const LLT Src1Ty = MRI.getType(Src1Reg);
@@ -7194,9 +7192,11 @@ static bool isSignExtendShiftType(AArch64_AM::ShiftExtendType Type) {
 }
 
 InstructionSelector::ComplexRendererFns
-AArch64InstructionSelector::selectExtendedSHL(
-    MachineOperand &Root, MachineOperand &Base, MachineOperand &Offset,
-    unsigned SizeInBytes, bool WantsExt) const {
+AArch64InstructionSelector::selectExtendedSHL(MachineOperand &Root,
+                                              MachineOperand &Base,
+                                              MachineOperand &Offset,
+                                              unsigned SizeInBytes,
+                                              bool WantsExt) const {
   assert(Base.isReg() && "Expected base to be a register operand");
   assert(Offset.isReg() && "Expected offset to be a register operand");
 
@@ -7413,8 +7413,7 @@ AArch64InstructionSelector::selectAddrModeXRO(MachineOperand &Root,
 
     // Skip immediates that can be selected in the load/store addresing
     // mode.
-    if (ImmOff % SizeInBytes == 0 && ImmOff >= 0 &&
-        ImmOff < (0x1000 << Scale))
+    if (ImmOff % SizeInBytes == 0 && ImmOff >= 0 && ImmOff < (0x1000 << Scale))
       return std::nullopt;
 
     // Helper lambda to decide whether or not it is preferable to emit an add.
@@ -7559,9 +7558,8 @@ AArch64InstructionSelector::selectAddrModeUnscaled(MachineOperand &Root,
 }
 
 InstructionSelector::ComplexRendererFns
-AArch64InstructionSelector::tryFoldAddLowIntoImm(MachineInstr &RootDef,
-                                                 unsigned Size,
-                                                 MachineRegisterInfo &MRI) const {
+AArch64InstructionSelector::tryFoldAddLowIntoImm(
+    MachineInstr &RootDef, unsigned Size, MachineRegisterInfo &MRI) const {
   if (RootDef.getOpcode() != AArch64::G_ADD_LOW)
     return std::nullopt;
   MachineInstr &Adrp = *MRI.getVRegDef(RootDef.getOperand(1).getReg());
@@ -7900,8 +7898,9 @@ void AArch64InstructionSelector::renderTruncImm(MachineInstrBuilder &MIB,
   MIB.addImm(*CstVal);
 }
 
-void AArch64InstructionSelector::renderLogicalImm32(
-  MachineInstrBuilder &MIB, const MachineInstr &I, int OpIdx) const {
+void AArch64InstructionSelector::renderLogicalImm32(MachineInstrBuilder &MIB,
+                                                    const MachineInstr &I,
+                                                    int OpIdx) const {
   assert(I.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
          "Expected G_CONSTANT");
   uint64_t CstVal = I.getOperand(1).getCImm()->getZExtValue();
@@ -7909,8 +7908,9 @@ void AArch64InstructionSelector::renderLogicalImm32(
   MIB.addImm(Enc);
 }
 
-void AArch64InstructionSelector::renderLogicalImm64(
-  MachineInstrBuilder &MIB, const MachineInstr &I, int OpIdx) const {
+void AArch64InstructionSelector::renderLogicalImm64(MachineInstrBuilder &MIB,
+                                                    const MachineInstr &I,
+                                                    int OpIdx) const {
   assert(I.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
          "Expected G_CONSTANT");
   uint64_t CstVal = I.getOperand(1).getCImm()->getZExtValue();
@@ -7993,7 +7993,6 @@ bool AArch64InstructionSelector::isDef32(const MachineInstr &MI) const {
   }
 }
 
-
 // Perform fixups on the given PHI instruction's operands to force them all
 // to be the same as the destination regbank.
 static void fixupPHIOpBanks(MachineInstr &MI, MachineRegisterInfo &MRI,
@@ -8094,4 +8093,4 @@ createAArch64InstructionSelector(const AArch64TargetMachine &TM,
                                  const AArch64RegisterBankInfo &RBI) {
   return new AArch64InstructionSelector(TM, Subtarget, RBI);
 }
-}
+} // namespace llvm
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp b/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
index 93461e39f..99adeef68 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp
@@ -12,7 +12,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "AArch64LegalizerInfo.h"
-#include "AArch64Subtarget.h"
+#include "../AArch64Subtarget.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
 #include "llvm/CodeGen/GlobalISel/LegalizerHelper.h"
@@ -313,8 +313,8 @@ AArch64LegalizerInfo::AArch64LegalizerInfo(const AArch64Subtarget &ST)
       .libcallFor({{s32, s32}, {s64, s32}, {s128, s32}});
 
   getActionDefinitionsBuilder(G_INSERT)
-      .legalIf(all(typeInSet(0, {s32, s64, p0}),
-                   typeInSet(1, {s8, s16, s32}), smallerThan(1, 0)))
+      .legalIf(all(typeInSet(0, {s32, s64, p0}), typeInSet(1, {s8, s16, s32}),
+                   smallerThan(1, 0)))
       .widenScalarToNextPow2(0)
       .clampScalar(0, s32, s64)
       .widenScalarToNextPow2(1)
@@ -333,31 +333,32 @@ AArch64LegalizerInfo::AArch64LegalizerInfo(const AArch64Subtarget &ST)
       .maxScalarIf(typeInSet(1, {s64, p0}), 0, s32)
       .maxScalarIf(typeInSet(1, {s128}), 0, s64);
 
-
   for (unsigned Op : {G_SEXTLOAD, G_ZEXTLOAD}) {
-    auto &Actions =  getActionDefinitionsBuilder(Op);
+    auto &Actions = getActionDefinitionsBuilder(Op);
 
     if (Op == G_SEXTLOAD)
-      Actions.lowerIf(atomicOrderingAtLeastOrStrongerThan(0, AtomicOrdering::Unordered));
+      Actions.lowerIf(
+          atomicOrderingAtLeastOrStrongerThan(0, AtomicOrdering::Unordered));
 
     // Atomics have zero extending behavior.
     Actions
-      .legalForTypesWithMemDesc({{s32, p0, s8, 8},
-                                 {s32, p0, s16, 8},
-                                 {s32, p0, s32, 8},
-                                 {s64, p0, s8, 2},
-                                 {s64, p0, s16, 2},
-                                 {s64, p0, s32, 4},
-                                 {s64, p0, s64, 8},
-                                 {p0, p0, s64, 8},
-                                 {v2s32, p0, s64, 8}})
-      .widenScalarToNextPow2(0)
-      .clampScalar(0, s32, s64)
-      // TODO: We could support sum-of-pow2's but the lowering code doesn't know
-      //       how to do that yet.
-      .unsupportedIfMemSizeNotPow2()
-      // Lower anything left over into G_*EXT and G_LOAD
-      .lower();
+        .legalForTypesWithMemDesc({{s32, p0, s8, 8},
+                                   {s32, p0, s16, 8},
+                                   {s32, p0, s32, 8},
+                                   {s64, p0, s8, 2},
+                                   {s64, p0, s16, 2},
+                                   {s64, p0, s32, 4},
+                                   {s64, p0, s64, 8},
+                                   {p0, p0, s64, 8},
+                                   {v2s32, p0, s64, 8}})
+        .widenScalarToNextPow2(0)
+        .clampScalar(0, s32, s64)
+        // TODO: We could support sum-of-pow2's but the lowering code doesn't
+        // know
+        //       how to do that yet.
+        .unsupportedIfMemSizeNotPow2()
+        // Lower anything left over into G_*EXT and G_LOAD
+        .lower();
   }
 
   auto IsPtrVecPred = [=](const LegalityQuery &Query) {
@@ -822,9 +823,8 @@ AArch64LegalizerInfo::AArch64LegalizerInfo(const AArch64Subtarget &ST)
                    {s128, s64}});
 
   // Control-flow
-  getActionDefinitionsBuilder(G_BRCOND)
-    .legalFor({s32})
-    .clampScalar(0, s32, s32);
+  getActionDefinitionsBuilder(G_BRCOND).legalFor({s32}).clampScalar(0, s32,
+                                                                    s32);
   getActionDefinitionsBuilder(G_BRINDIRECT).legalFor({p0});
 
   getActionDefinitionsBuilder(G_SELECT)
@@ -893,8 +893,7 @@ AArch64LegalizerInfo::AArch64LegalizerInfo(const AArch64Subtarget &ST)
       .widenScalarToNextPow2(0, /*Min*/ 8);
 
   getActionDefinitionsBuilder(G_ATOMIC_CMPXCHG_WITH_SUCCESS)
-      .lowerIf(
-          all(typeInSet(0, {s8, s16, s32, s64, s128}), typeIs(2, p0)));
+      .lowerIf(all(typeInSet(0, {s8, s16, s32, s64, s128}), typeIs(2, p0)));
 
   bool UseOutlineAtomics = ST.outlineAtomics() && !ST.hasLSE();
 
@@ -1552,7 +1551,7 @@ bool AArch64LegalizerInfo::legalizeSmallCMGlobalValue(
   // Don't modify an intrinsic call.
   if (GlobalOp.isSymbol())
     return true;
-  const auto* GV = GlobalOp.getGlobal();
+  const auto *GV = GlobalOp.getGlobal();
   if (GV->isThreadLocal())
     return true; // Don't want to modify TLS vars.
 
@@ -1613,10 +1612,10 @@ bool AArch64LegalizerInfo::legalizeIntrinsic(LegalizerHelper &Helper,
   switch (IntrinsicID) {
   case Intrinsic::vacopy: {
     unsigned PtrSize = ST->isTargetILP32() ? 4 : 8;
-    unsigned VaListSize =
-      (ST->isTargetDarwin() || ST->isTargetWindows())
-          ? PtrSize
-          : ST->isTargetILP32() ? 20 : 32;
+    unsigned VaListSize = (ST->isTargetDarwin() || ST->isTargetWindows())
+                              ? PtrSize
+                          : ST->isTargetILP32() ? 20
+                                                : 32;
 
     MachineFunction &MF = *MI.getMF();
     auto Val = MF.getRegInfo().createGenericVirtualRegister(
@@ -2028,7 +2027,8 @@ bool AArch64LegalizerInfo::legalizeCTPOP(MachineInstr &MI,
   // v8s16,v4s32,v2s64 -> v16i8
   LLT VTy = Size == 128 ? LLT::fixed_vector(16, 8) : LLT::fixed_vector(8, 8);
   if (Ty.isScalar()) {
-    assert((Size == 32 || Size == 64 || Size == 128) && "Expected only 32, 64, or 128 bit scalars!");
+    assert((Size == 32 || Size == 64 || Size == 128) &&
+           "Expected only 32, 64, or 128 bit scalars!");
     if (Size == 32) {
       Val = MIRBuilder.buildZExt(LLT::scalar(64), Val).getReg(0);
     }
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp b/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp
index d76918b91..ebedfe273 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp
@@ -11,8 +11,8 @@
 //
 //===----------------------------------------------------------------------===//
 
+#include "../AArch64TargetMachine.h"
 #include "AArch64GlobalISelUtils.h"
-#include "AArch64TargetMachine.h"
 #include "llvm/CodeGen/GlobalISel/Combiner.h"
 #include "llvm/CodeGen/GlobalISel/CombinerHelper.h"
 #include "llvm/CodeGen/GlobalISel/CombinerInfo.h"
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp b/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
index cf6b2ce9c..fa491f479 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp
@@ -19,7 +19,7 @@
 ///
 //===----------------------------------------------------------------------===//
 
-#include "AArch64TargetMachine.h"
+#include "../AArch64TargetMachine.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/CodeGen/GlobalISel/CSEInfo.h"
 #include "llvm/CodeGen/GlobalISel/CSEMIRBuilder.h"
@@ -498,7 +498,6 @@ private:
   bool IsOptNone;
   AArch64PostLegalizerCombinerImplRuleConfig RuleConfig;
 
-
   struct StoreInfo {
     GStore *St = nullptr;
     // The G_PTR_ADD that's used by the store. We keep this to cache the
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp b/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
index 6bba70d45..e7dddf556 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerLowering.cpp
@@ -19,13 +19,13 @@
 ///
 //===----------------------------------------------------------------------===//
 
-#include "AArch64ExpandImm.h"
+#include "../AArch64ExpandImm.h"
+#include "../AArch64PerfectShuffle.h"
+#include "../AArch64Subtarget.h"
+#include "../MCTargetDesc/AArch64MCTargetDesc.h"
+#include "../Utils/AArch64BaseInfo.h"
 #include "AArch64GlobalISelUtils.h"
-#include "AArch64PerfectShuffle.h"
-#include "AArch64Subtarget.h"
-#include "GISel/AArch64LegalizerInfo.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
-#include "Utils/AArch64BaseInfo.h"
+#include "AArch64LegalizerInfo.h"
 #include "llvm/CodeGen/GlobalISel/Combiner.h"
 #include "llvm/CodeGen/GlobalISel/CombinerHelper.h"
 #include "llvm/CodeGen/GlobalISel/CombinerInfo.h"
@@ -72,7 +72,7 @@ struct ShuffleVectorPseudo {
   SmallVector<SrcOp, 2> SrcOps; ///< Source registers.
   ShuffleVectorPseudo(unsigned Opc, Register Dst,
                       std::initializer_list<SrcOp> SrcOps)
-      : Opc(Opc), Dst(Dst), SrcOps(SrcOps){};
+      : Opc(Opc), Dst(Dst), SrcOps(SrcOps) {};
   ShuffleVectorPseudo() = default;
 };
 
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp b/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
index e9aed6059..f9a10f12c 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp
@@ -11,9 +11,9 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "AArch64.h"
-#include "AArch64TargetMachine.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
+#include "../AArch64.h"
+#include "../AArch64TargetMachine.h"
+#include "../MCTargetDesc/AArch64MCTargetDesc.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/CodeGen/GlobalISel/Utils.h"
 #include "llvm/CodeGen/MachineBasicBlock.h"
@@ -136,7 +136,6 @@ bool AArch64PostSelectOptimize::foldSimpleCrossClassCopies(MachineInstr &MI) {
   if (SrcRC == DstRC)
     return false;
 
-
   if (SrcRC->hasSubClass(DstRC)) {
     // This is the case where the source class is a superclass of the dest, so
     // if the copy is the only user of the source, we can just constrain the
@@ -314,11 +313,9 @@ bool AArch64PostSelectOptimize::runOnMachineFunction(MachineFunction &MF) {
 
 char AArch64PostSelectOptimize::ID = 0;
 INITIALIZE_PASS_BEGIN(AArch64PostSelectOptimize, DEBUG_TYPE,
-                      "Optimize AArch64 selected instructions",
-                      false, false)
+                      "Optimize AArch64 selected instructions", false, false)
 INITIALIZE_PASS_END(AArch64PostSelectOptimize, DEBUG_TYPE,
-                    "Optimize AArch64 selected instructions", false,
-                    false)
+                    "Optimize AArch64 selected instructions", false, false)
 
 namespace llvm {
 FunctionPass *createAArch64PostSelectOptimize() {
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp b/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
index bbf188392..6030ce06c 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp
@@ -11,8 +11,8 @@
 //
 //===----------------------------------------------------------------------===//
 
+#include "../AArch64TargetMachine.h"
 #include "AArch64GlobalISelUtils.h"
-#include "AArch64TargetMachine.h"
 #include "llvm/CodeGen/GlobalISel/CSEInfo.h"
 #include "llvm/CodeGen/GlobalISel/Combiner.h"
 #include "llvm/CodeGen/GlobalISel/CombinerHelper.h"
@@ -182,8 +182,7 @@ bool matchFoldGlobalOffset(MachineInstr &MI, MachineRegisterInfo &MRI,
     return false;
 
   Type *T = GV->getValueType();
-  if (!T->isSized() ||
-      NewOffset > GV->getDataLayout().getTypeAllocSize(T))
+  if (!T->isSized() || NewOffset > GV->getDataLayout().getTypeAllocSize(T))
     return false;
   MatchInfo = std::make_pair(NewOffset, MinOffset);
   return true;
diff --git a/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp b/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
index d9c558819..6eeff9be3 100644
--- a/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
+++ b/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp
@@ -12,8 +12,8 @@
 //===----------------------------------------------------------------------===//
 
 #include "AArch64RegisterBankInfo.h"
-#include "AArch64RegisterInfo.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
+#include "../AArch64RegisterInfo.h"
+#include "../MCTargetDesc/AArch64MCTargetDesc.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
@@ -38,7 +38,7 @@
 #include "AArch64GenRegisterBank.inc"
 
 // This file will be TableGen'ed at some point.
-#include "AArch64GenRegisterBankInfo.def"
+#include "../AArch64GenRegisterBankInfo.def"
 
 using namespace llvm;
 static const unsigned CustomMappingID = 1;
@@ -197,12 +197,12 @@ AArch64RegisterBankInfo::AArch64RegisterBankInfo(
     (void)Map;                                                                 \
     assert(Map[0].BreakDown ==                                                 \
                &AArch64GenRegisterBankInfo::PartMappings[PartialMapDstIdx] &&  \
-           Map[0].NumBreakDowns == 1 && "FPR" #DstSize                         \
-                                        " Dst is incorrectly initialized");    \
+           Map[0].NumBreakDowns == 1 &&                                        \
+           "FPR" #DstSize " Dst is incorrectly initialized");                  \
     assert(Map[1].BreakDown ==                                                 \
                &AArch64GenRegisterBankInfo::PartMappings[PartialMapSrcIdx] &&  \
-           Map[1].NumBreakDowns == 1 && "FPR" #SrcSize                         \
-                                        " Src is incorrectly initialized");    \
+           Map[1].NumBreakDowns == 1 &&                                        \
+           "FPR" #SrcSize " Src is incorrectly initialized");                  \
                                                                                \
   } while (false)
 
@@ -967,7 +967,7 @@ AArch64RegisterBankInfo::getInstrMapping(const MachineInstr &MI) const {
     if (OpRegBankIdx[0] != PMI_FirstGPR)
       break;
 
-    LLT SrcTy = MRI.getType(MI.getOperand(MI.getNumOperands()-1).getReg());
+    LLT SrcTy = MRI.getType(MI.getOperand(MI.getNumOperands() - 1).getReg());
     // UNMERGE into scalars from a vector should always use FPR.
     // Likewise if any of the uses are FP instructions.
     if (SrcTy.isVector() || SrcTy == LLT::scalar(128) ||
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp
index 337b81d68..8d064491c 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64AsmBackend.cpp
@@ -6,10 +6,10 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/AArch64FixupKinds.h"
-#include "MCTargetDesc/AArch64MCExpr.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
-#include "Utils/AArch64BaseInfo.h"
+#include "../Utils/AArch64BaseInfo.h"
+#include "AArch64FixupKinds.h"
+#include "AArch64MCExpr.h"
+#include "AArch64MCTargetDesc.h"
 #include "llvm/BinaryFormat/MachO.h"
 #include "llvm/MC/MCAsmBackend.h"
 #include "llvm/MC/MCAssembler.h"
@@ -32,6 +32,7 @@ namespace {
 class AArch64AsmBackend : public MCAsmBackend {
   static const unsigned PCRelFlagVal =
       MCFixupKindInfo::FKF_IsAlignedDownTo32Bits | MCFixupKindInfo::FKF_IsPCRel;
+
 protected:
   Triple TheTriple;
 
@@ -63,7 +64,7 @@ public:
         {"fixup_aarch64_ldst_imm12_scale16", 10, 12, 0},
         {"fixup_aarch64_ldr_pcrel_imm19", 5, 19, PCRelFlagVal},
         {"fixup_aarch64_movw", 5, 16, 0},
-        {"fixup_aarch64_pcrel_branch9", 5, 9,  PCRelFlagVal},
+        {"fixup_aarch64_pcrel_branch9", 5, 9, PCRelFlagVal},
         {"fixup_aarch64_pcrel_branch14", 5, 14, PCRelFlagVal},
         {"fixup_aarch64_pcrel_branch16", 5, 16, PCRelFlagVal},
         {"fixup_aarch64_pcrel_branch19", 5, 19, PCRelFlagVal},
@@ -291,8 +292,7 @@ static uint64_t adjustFixupValue(const MCFixup &Fixup, const MCValue &Target,
 
     if (RefKind & AArch64MCExpr::VK_NC) {
       Value &= 0xFFFF;
-    }
-    else if (AArch64MCExpr::getSymbolLoc(RefKind) == AArch64MCExpr::VK_SABS) {
+    } else if (AArch64MCExpr::getSymbolLoc(RefKind) == AArch64MCExpr::VK_SABS) {
       if (SignedValue > 0xFFFF || SignedValue < -0xFFFF)
         Ctx.reportError(Fixup.getLoc(), "fixup value out of range");
 
@@ -300,8 +300,7 @@ static uint64_t adjustFixupValue(const MCFixup &Fixup, const MCValue &Target,
       if (SignedValue < 0)
         SignedValue = ~SignedValue;
       Value = static_cast<uint64_t>(SignedValue);
-    }
-    else if (Value > 0xFFFF) {
+    } else if (Value > 0xFFFF) {
       Ctx.reportError(Fixup.getLoc(), "fixup value out of range");
     }
     return Value;
@@ -365,7 +364,7 @@ AArch64AsmBackend::getFixupKind(StringRef Name) const {
     return std::nullopt;
 
   unsigned Type = llvm::StringSwitch<unsigned>(Name)
-#define ELF_RELOC(X, Y)  .Case(#X, Y)
+#define ELF_RELOC(X, Y) .Case(#X, Y)
 #include "llvm/BinaryFormat/ELFRelocs/AArch64.def"
 #undef ELF_RELOC
                       .Case("BFD_RELOC_NONE", ELF::R_AARCH64_NONE)
@@ -380,7 +379,8 @@ AArch64AsmBackend::getFixupKind(StringRef Name) const {
 
 /// getFixupKindContainereSizeInBytes - The number of bytes of the
 /// container involved in big endian or 0 if the item is little endian
-unsigned AArch64AsmBackend::getFixupKindContainereSizeInBytes(unsigned Kind) const {
+unsigned
+AArch64AsmBackend::getFixupKindContainereSizeInBytes(unsigned Kind) const {
   if (Endian == llvm::endianness::little)
     return 0;
 
@@ -455,7 +455,8 @@ void AArch64AsmBackend::applyFixup(const MCAssembler &Asm, const MCFixup &Fixup,
   assert(Offset + NumBytes <= Data.size() && "Invalid fixup offset!");
 
   // Used to point to big endian bytes.
-  unsigned FulleSizeInBytes = getFixupKindContainereSizeInBytes(Fixup.getKind());
+  unsigned FulleSizeInBytes =
+      getFixupKindContainereSizeInBytes(Fixup.getKind());
 
   // For each byte of the fragment that the fixup touches, mask in the
   // bits from the fixup value.
@@ -581,7 +582,7 @@ enum CompactUnwindEncodings {
   UNWIND_ARM64_FRAME_D14_D15_PAIR = 0x00000800
 };
 
-} // end CU namespace
+} // namespace CU
 
 // FIXME: This should be in a separate file.
 class DarwinAArch64AsmBackend : public AArch64AsmBackend {
@@ -784,7 +785,7 @@ public:
   }
 };
 
-}
+} // namespace
 
 namespace {
 class COFFAArch64AsmBackend : public AArch64AsmBackend {
@@ -797,7 +798,7 @@ public:
     return createAArch64WinCOFFObjectWriter(TheTriple);
   }
 };
-}
+} // namespace
 
 MCAsmBackend *llvm::createAArch64leAsmBackend(const Target &T,
                                               const MCSubtargetInfo &STI,
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFObjectWriter.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFObjectWriter.cpp
index 947ec4012..aaa4aa5eb 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFObjectWriter.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64ELFObjectWriter.cpp
@@ -11,9 +11,9 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/AArch64FixupKinds.h"
-#include "MCTargetDesc/AArch64MCExpr.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
+#include "AArch64FixupKinds.h"
+#include "AArch64MCExpr.h"
+#include "AArch64MCTargetDesc.h"
 #include "llvm/BinaryFormat/ELF.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCELFObjectWriter.h"
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
index 875b50554..96749e0de 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.cpp
@@ -11,8 +11,8 @@
 //===----------------------------------------------------------------------===//
 
 #include "AArch64InstPrinter.h"
-#include "MCTargetDesc/AArch64AddressingModes.h"
-#include "Utils/AArch64BaseInfo.h"
+#include "../Utils/AArch64BaseInfo.h"
+#include "AArch64AddressingModes.h"
 #include "llvm/ADT/StringExtras.h"
 #include "llvm/ADT/StringRef.h"
 #include "llvm/MC/MCAsmInfo.h"
@@ -430,346 +430,346 @@ struct LdStNInstrDesc {
 };
 
 static const LdStNInstrDesc LdStNInstInfo[] = {
-  { AArch64::LD1i8,             "ld1",  ".b",     1, true,  0  },
-  { AArch64::LD1i16,            "ld1",  ".h",     1, true,  0  },
-  { AArch64::LD1i32,            "ld1",  ".s",     1, true,  0  },
-  { AArch64::LD1i64,            "ld1",  ".d",     1, true,  0  },
-  { AArch64::LD1i8_POST,        "ld1",  ".b",     2, true,  1  },
-  { AArch64::LD1i16_POST,       "ld1",  ".h",     2, true,  2  },
-  { AArch64::LD1i32_POST,       "ld1",  ".s",     2, true,  4  },
-  { AArch64::LD1i64_POST,       "ld1",  ".d",     2, true,  8  },
-  { AArch64::LD1Rv16b,          "ld1r", ".16b",   0, false, 0  },
-  { AArch64::LD1Rv8h,           "ld1r", ".8h",    0, false, 0  },
-  { AArch64::LD1Rv4s,           "ld1r", ".4s",    0, false, 0  },
-  { AArch64::LD1Rv2d,           "ld1r", ".2d",    0, false, 0  },
-  { AArch64::LD1Rv8b,           "ld1r", ".8b",    0, false, 0  },
-  { AArch64::LD1Rv4h,           "ld1r", ".4h",    0, false, 0  },
-  { AArch64::LD1Rv2s,           "ld1r", ".2s",    0, false, 0  },
-  { AArch64::LD1Rv1d,           "ld1r", ".1d",    0, false, 0  },
-  { AArch64::LD1Rv16b_POST,     "ld1r", ".16b",   1, false, 1  },
-  { AArch64::LD1Rv8h_POST,      "ld1r", ".8h",    1, false, 2  },
-  { AArch64::LD1Rv4s_POST,      "ld1r", ".4s",    1, false, 4  },
-  { AArch64::LD1Rv2d_POST,      "ld1r", ".2d",    1, false, 8  },
-  { AArch64::LD1Rv8b_POST,      "ld1r", ".8b",    1, false, 1  },
-  { AArch64::LD1Rv4h_POST,      "ld1r", ".4h",    1, false, 2  },
-  { AArch64::LD1Rv2s_POST,      "ld1r", ".2s",    1, false, 4  },
-  { AArch64::LD1Rv1d_POST,      "ld1r", ".1d",    1, false, 8  },
-  { AArch64::LD1Onev16b,        "ld1",  ".16b",   0, false, 0  },
-  { AArch64::LD1Onev8h,         "ld1",  ".8h",    0, false, 0  },
-  { AArch64::LD1Onev4s,         "ld1",  ".4s",    0, false, 0  },
-  { AArch64::LD1Onev2d,         "ld1",  ".2d",    0, false, 0  },
-  { AArch64::LD1Onev8b,         "ld1",  ".8b",    0, false, 0  },
-  { AArch64::LD1Onev4h,         "ld1",  ".4h",    0, false, 0  },
-  { AArch64::LD1Onev2s,         "ld1",  ".2s",    0, false, 0  },
-  { AArch64::LD1Onev1d,         "ld1",  ".1d",    0, false, 0  },
-  { AArch64::LD1Onev16b_POST,   "ld1",  ".16b",   1, false, 16 },
-  { AArch64::LD1Onev8h_POST,    "ld1",  ".8h",    1, false, 16 },
-  { AArch64::LD1Onev4s_POST,    "ld1",  ".4s",    1, false, 16 },
-  { AArch64::LD1Onev2d_POST,    "ld1",  ".2d",    1, false, 16 },
-  { AArch64::LD1Onev8b_POST,    "ld1",  ".8b",    1, false, 8  },
-  { AArch64::LD1Onev4h_POST,    "ld1",  ".4h",    1, false, 8  },
-  { AArch64::LD1Onev2s_POST,    "ld1",  ".2s",    1, false, 8  },
-  { AArch64::LD1Onev1d_POST,    "ld1",  ".1d",    1, false, 8  },
-  { AArch64::LD1Twov16b,        "ld1",  ".16b",   0, false, 0  },
-  { AArch64::LD1Twov8h,         "ld1",  ".8h",    0, false, 0  },
-  { AArch64::LD1Twov4s,         "ld1",  ".4s",    0, false, 0  },
-  { AArch64::LD1Twov2d,         "ld1",  ".2d",    0, false, 0  },
-  { AArch64::LD1Twov8b,         "ld1",  ".8b",    0, false, 0  },
-  { AArch64::LD1Twov4h,         "ld1",  ".4h",    0, false, 0  },
-  { AArch64::LD1Twov2s,         "ld1",  ".2s",    0, false, 0  },
-  { AArch64::LD1Twov1d,         "ld1",  ".1d",    0, false, 0  },
-  { AArch64::LD1Twov16b_POST,   "ld1",  ".16b",   1, false, 32 },
-  { AArch64::LD1Twov8h_POST,    "ld1",  ".8h",    1, false, 32 },
-  { AArch64::LD1Twov4s_POST,    "ld1",  ".4s",    1, false, 32 },
-  { AArch64::LD1Twov2d_POST,    "ld1",  ".2d",    1, false, 32 },
-  { AArch64::LD1Twov8b_POST,    "ld1",  ".8b",    1, false, 16 },
-  { AArch64::LD1Twov4h_POST,    "ld1",  ".4h",    1, false, 16 },
-  { AArch64::LD1Twov2s_POST,    "ld1",  ".2s",    1, false, 16 },
-  { AArch64::LD1Twov1d_POST,    "ld1",  ".1d",    1, false, 16 },
-  { AArch64::LD1Threev16b,      "ld1",  ".16b",   0, false, 0  },
-  { AArch64::LD1Threev8h,       "ld1",  ".8h",    0, false, 0  },
-  { AArch64::LD1Threev4s,       "ld1",  ".4s",    0, false, 0  },
-  { AArch64::LD1Threev2d,       "ld1",  ".2d",    0, false, 0  },
-  { AArch64::LD1Threev8b,       "ld1",  ".8b",    0, false, 0  },
-  { AArch64::LD1Threev4h,       "ld1",  ".4h",    0, false, 0  },
-  { AArch64::LD1Threev2s,       "ld1",  ".2s",    0, false, 0  },
-  { AArch64::LD1Threev1d,       "ld1",  ".1d",    0, false, 0  },
-  { AArch64::LD1Threev16b_POST, "ld1",  ".16b",   1, false, 48 },
-  { AArch64::LD1Threev8h_POST,  "ld1",  ".8h",    1, false, 48 },
-  { AArch64::LD1Threev4s_POST,  "ld1",  ".4s",    1, false, 48 },
-  { AArch64::LD1Threev2d_POST,  "ld1",  ".2d",    1, false, 48 },
-  { AArch64::LD1Threev8b_POST,  "ld1",  ".8b",    1, false, 24 },
-  { AArch64::LD1Threev4h_POST,  "ld1",  ".4h",    1, false, 24 },
-  { AArch64::LD1Threev2s_POST,  "ld1",  ".2s",    1, false, 24 },
-  { AArch64::LD1Threev1d_POST,  "ld1",  ".1d",    1, false, 24 },
-  { AArch64::LD1Fourv16b,       "ld1",  ".16b",   0, false, 0  },
-  { AArch64::LD1Fourv8h,        "ld1",  ".8h",    0, false, 0  },
-  { AArch64::LD1Fourv4s,        "ld1",  ".4s",    0, false, 0  },
-  { AArch64::LD1Fourv2d,        "ld1",  ".2d",    0, false, 0  },
-  { AArch64::LD1Fourv8b,        "ld1",  ".8b",    0, false, 0  },
-  { AArch64::LD1Fourv4h,        "ld1",  ".4h",    0, false, 0  },
-  { AArch64::LD1Fourv2s,        "ld1",  ".2s",    0, false, 0  },
-  { AArch64::LD1Fourv1d,        "ld1",  ".1d",    0, false, 0  },
-  { AArch64::LD1Fourv16b_POST,  "ld1",  ".16b",   1, false, 64 },
-  { AArch64::LD1Fourv8h_POST,   "ld1",  ".8h",    1, false, 64 },
-  { AArch64::LD1Fourv4s_POST,   "ld1",  ".4s",    1, false, 64 },
-  { AArch64::LD1Fourv2d_POST,   "ld1",  ".2d",    1, false, 64 },
-  { AArch64::LD1Fourv8b_POST,   "ld1",  ".8b",    1, false, 32 },
-  { AArch64::LD1Fourv4h_POST,   "ld1",  ".4h",    1, false, 32 },
-  { AArch64::LD1Fourv2s_POST,   "ld1",  ".2s",    1, false, 32 },
-  { AArch64::LD1Fourv1d_POST,   "ld1",  ".1d",    1, false, 32 },
-  { AArch64::LD2i8,             "ld2",  ".b",     1, true,  0  },
-  { AArch64::LD2i16,            "ld2",  ".h",     1, true,  0  },
-  { AArch64::LD2i32,            "ld2",  ".s",     1, true,  0  },
-  { AArch64::LD2i64,            "ld2",  ".d",     1, true,  0  },
-  { AArch64::LD2i8_POST,        "ld2",  ".b",     2, true,  2  },
-  { AArch64::LD2i16_POST,       "ld2",  ".h",     2, true,  4  },
-  { AArch64::LD2i32_POST,       "ld2",  ".s",     2, true,  8  },
-  { AArch64::LD2i64_POST,       "ld2",  ".d",     2, true,  16  },
-  { AArch64::LD2Rv16b,          "ld2r", ".16b",   0, false, 0  },
-  { AArch64::LD2Rv8h,           "ld2r", ".8h",    0, false, 0  },
-  { AArch64::LD2Rv4s,           "ld2r", ".4s",    0, false, 0  },
-  { AArch64::LD2Rv2d,           "ld2r", ".2d",    0, false, 0  },
-  { AArch64::LD2Rv8b,           "ld2r", ".8b",    0, false, 0  },
-  { AArch64::LD2Rv4h,           "ld2r", ".4h",    0, false, 0  },
-  { AArch64::LD2Rv2s,           "ld2r", ".2s",    0, false, 0  },
-  { AArch64::LD2Rv1d,           "ld2r", ".1d",    0, false, 0  },
-  { AArch64::LD2Rv16b_POST,     "ld2r", ".16b",   1, false, 2  },
-  { AArch64::LD2Rv8h_POST,      "ld2r", ".8h",    1, false, 4  },
-  { AArch64::LD2Rv4s_POST,      "ld2r", ".4s",    1, false, 8  },
-  { AArch64::LD2Rv2d_POST,      "ld2r", ".2d",    1, false, 16 },
-  { AArch64::LD2Rv8b_POST,      "ld2r", ".8b",    1, false, 2  },
-  { AArch64::LD2Rv4h_POST,      "ld2r", ".4h",    1, false, 4  },
-  { AArch64::LD2Rv2s_POST,      "ld2r", ".2s",    1, false, 8  },
-  { AArch64::LD2Rv1d_POST,      "ld2r", ".1d",    1, false, 16 },
-  { AArch64::LD2Twov16b,        "ld2",  ".16b",   0, false, 0  },
-  { AArch64::LD2Twov8h,         "ld2",  ".8h",    0, false, 0  },
-  { AArch64::LD2Twov4s,         "ld2",  ".4s",    0, false, 0  },
-  { AArch64::LD2Twov2d,         "ld2",  ".2d",    0, false, 0  },
-  { AArch64::LD2Twov8b,         "ld2",  ".8b",    0, false, 0  },
-  { AArch64::LD2Twov4h,         "ld2",  ".4h",    0, false, 0  },
-  { AArch64::LD2Twov2s,         "ld2",  ".2s",    0, false, 0  },
-  { AArch64::LD2Twov16b_POST,   "ld2",  ".16b",   1, false, 32 },
-  { AArch64::LD2Twov8h_POST,    "ld2",  ".8h",    1, false, 32 },
-  { AArch64::LD2Twov4s_POST,    "ld2",  ".4s",    1, false, 32 },
-  { AArch64::LD2Twov2d_POST,    "ld2",  ".2d",    1, false, 32 },
-  { AArch64::LD2Twov8b_POST,    "ld2",  ".8b",    1, false, 16 },
-  { AArch64::LD2Twov4h_POST,    "ld2",  ".4h",    1, false, 16 },
-  { AArch64::LD2Twov2s_POST,    "ld2",  ".2s",    1, false, 16 },
-  { AArch64::LD3i8,             "ld3",  ".b",     1, true,  0  },
-  { AArch64::LD3i16,            "ld3",  ".h",     1, true,  0  },
-  { AArch64::LD3i32,            "ld3",  ".s",     1, true,  0  },
-  { AArch64::LD3i64,            "ld3",  ".d",     1, true,  0  },
-  { AArch64::LD3i8_POST,        "ld3",  ".b",     2, true,  3  },
-  { AArch64::LD3i16_POST,       "ld3",  ".h",     2, true,  6  },
-  { AArch64::LD3i32_POST,       "ld3",  ".s",     2, true,  12 },
-  { AArch64::LD3i64_POST,       "ld3",  ".d",     2, true,  24 },
-  { AArch64::LD3Rv16b,          "ld3r", ".16b",   0, false, 0  },
-  { AArch64::LD3Rv8h,           "ld3r", ".8h",    0, false, 0  },
-  { AArch64::LD3Rv4s,           "ld3r", ".4s",    0, false, 0  },
-  { AArch64::LD3Rv2d,           "ld3r", ".2d",    0, false, 0  },
-  { AArch64::LD3Rv8b,           "ld3r", ".8b",    0, false, 0  },
-  { AArch64::LD3Rv4h,           "ld3r", ".4h",    0, false, 0  },
-  { AArch64::LD3Rv2s,           "ld3r", ".2s",    0, false, 0  },
-  { AArch64::LD3Rv1d,           "ld3r", ".1d",    0, false, 0  },
-  { AArch64::LD3Rv16b_POST,     "ld3r", ".16b",   1, false, 3  },
-  { AArch64::LD3Rv8h_POST,      "ld3r", ".8h",    1, false, 6  },
-  { AArch64::LD3Rv4s_POST,      "ld3r", ".4s",    1, false, 12 },
-  { AArch64::LD3Rv2d_POST,      "ld3r", ".2d",    1, false, 24 },
-  { AArch64::LD3Rv8b_POST,      "ld3r", ".8b",    1, false, 3  },
-  { AArch64::LD3Rv4h_POST,      "ld3r", ".4h",    1, false, 6  },
-  { AArch64::LD3Rv2s_POST,      "ld3r", ".2s",    1, false, 12 },
-  { AArch64::LD3Rv1d_POST,      "ld3r", ".1d",    1, false, 24 },
-  { AArch64::LD3Threev16b,      "ld3",  ".16b",   0, false, 0  },
-  { AArch64::LD3Threev8h,       "ld3",  ".8h",    0, false, 0  },
-  { AArch64::LD3Threev4s,       "ld3",  ".4s",    0, false, 0  },
-  { AArch64::LD3Threev2d,       "ld3",  ".2d",    0, false, 0  },
-  { AArch64::LD3Threev8b,       "ld3",  ".8b",    0, false, 0  },
-  { AArch64::LD3Threev4h,       "ld3",  ".4h",    0, false, 0  },
-  { AArch64::LD3Threev2s,       "ld3",  ".2s",    0, false, 0  },
-  { AArch64::LD3Threev16b_POST, "ld3",  ".16b",   1, false, 48 },
-  { AArch64::LD3Threev8h_POST,  "ld3",  ".8h",    1, false, 48 },
-  { AArch64::LD3Threev4s_POST,  "ld3",  ".4s",    1, false, 48 },
-  { AArch64::LD3Threev2d_POST,  "ld3",  ".2d",    1, false, 48 },
-  { AArch64::LD3Threev8b_POST,  "ld3",  ".8b",    1, false, 24 },
-  { AArch64::LD3Threev4h_POST,  "ld3",  ".4h",    1, false, 24 },
-  { AArch64::LD3Threev2s_POST,  "ld3",  ".2s",    1, false, 24 },
-  { AArch64::LD4i8,             "ld4",  ".b",     1, true,  0  },
-  { AArch64::LD4i16,            "ld4",  ".h",     1, true,  0  },
-  { AArch64::LD4i32,            "ld4",  ".s",     1, true,  0  },
-  { AArch64::LD4i64,            "ld4",  ".d",     1, true,  0  },
-  { AArch64::LD4i8_POST,        "ld4",  ".b",     2, true,  4  },
-  { AArch64::LD4i16_POST,       "ld4",  ".h",     2, true,  8  },
-  { AArch64::LD4i32_POST,       "ld4",  ".s",     2, true,  16 },
-  { AArch64::LD4i64_POST,       "ld4",  ".d",     2, true,  32 },
-  { AArch64::LD4Rv16b,          "ld4r", ".16b",   0, false, 0  },
-  { AArch64::LD4Rv8h,           "ld4r", ".8h",    0, false, 0  },
-  { AArch64::LD4Rv4s,           "ld4r", ".4s",    0, false, 0  },
-  { AArch64::LD4Rv2d,           "ld4r", ".2d",    0, false, 0  },
-  { AArch64::LD4Rv8b,           "ld4r", ".8b",    0, false, 0  },
-  { AArch64::LD4Rv4h,           "ld4r", ".4h",    0, false, 0  },
-  { AArch64::LD4Rv2s,           "ld4r", ".2s",    0, false, 0  },
-  { AArch64::LD4Rv1d,           "ld4r", ".1d",    0, false, 0  },
-  { AArch64::LD4Rv16b_POST,     "ld4r", ".16b",   1, false, 4  },
-  { AArch64::LD4Rv8h_POST,      "ld4r", ".8h",    1, false, 8  },
-  { AArch64::LD4Rv4s_POST,      "ld4r", ".4s",    1, false, 16 },
-  { AArch64::LD4Rv2d_POST,      "ld4r", ".2d",    1, false, 32 },
-  { AArch64::LD4Rv8b_POST,      "ld4r", ".8b",    1, false, 4  },
-  { AArch64::LD4Rv4h_POST,      "ld4r", ".4h",    1, false, 8  },
-  { AArch64::LD4Rv2s_POST,      "ld4r", ".2s",    1, false, 16 },
-  { AArch64::LD4Rv1d_POST,      "ld4r", ".1d",    1, false, 32 },
-  { AArch64::LD4Fourv16b,       "ld4",  ".16b",   0, false, 0  },
-  { AArch64::LD4Fourv8h,        "ld4",  ".8h",    0, false, 0  },
-  { AArch64::LD4Fourv4s,        "ld4",  ".4s",    0, false, 0  },
-  { AArch64::LD4Fourv2d,        "ld4",  ".2d",    0, false, 0  },
-  { AArch64::LD4Fourv8b,        "ld4",  ".8b",    0, false, 0  },
-  { AArch64::LD4Fourv4h,        "ld4",  ".4h",    0, false, 0  },
-  { AArch64::LD4Fourv2s,        "ld4",  ".2s",    0, false, 0  },
-  { AArch64::LD4Fourv16b_POST,  "ld4",  ".16b",   1, false, 64 },
-  { AArch64::LD4Fourv8h_POST,   "ld4",  ".8h",    1, false, 64 },
-  { AArch64::LD4Fourv4s_POST,   "ld4",  ".4s",    1, false, 64 },
-  { AArch64::LD4Fourv2d_POST,   "ld4",  ".2d",    1, false, 64 },
-  { AArch64::LD4Fourv8b_POST,   "ld4",  ".8b",    1, false, 32 },
-  { AArch64::LD4Fourv4h_POST,   "ld4",  ".4h",    1, false, 32 },
-  { AArch64::LD4Fourv2s_POST,   "ld4",  ".2s",    1, false, 32 },
-  { AArch64::ST1i8,             "st1",  ".b",     0, true,  0  },
-  { AArch64::ST1i16,            "st1",  ".h",     0, true,  0  },
-  { AArch64::ST1i32,            "st1",  ".s",     0, true,  0  },
-  { AArch64::ST1i64,            "st1",  ".d",     0, true,  0  },
-  { AArch64::ST1i8_POST,        "st1",  ".b",     1, true,  1  },
-  { AArch64::ST1i16_POST,       "st1",  ".h",     1, true,  2  },
-  { AArch64::ST1i32_POST,       "st1",  ".s",     1, true,  4  },
-  { AArch64::ST1i64_POST,       "st1",  ".d",     1, true,  8  },
-  { AArch64::ST1Onev16b,        "st1",  ".16b",   0, false, 0  },
-  { AArch64::ST1Onev8h,         "st1",  ".8h",    0, false, 0  },
-  { AArch64::ST1Onev4s,         "st1",  ".4s",    0, false, 0  },
-  { AArch64::ST1Onev2d,         "st1",  ".2d",    0, false, 0  },
-  { AArch64::ST1Onev8b,         "st1",  ".8b",    0, false, 0  },
-  { AArch64::ST1Onev4h,         "st1",  ".4h",    0, false, 0  },
-  { AArch64::ST1Onev2s,         "st1",  ".2s",    0, false, 0  },
-  { AArch64::ST1Onev1d,         "st1",  ".1d",    0, false, 0  },
-  { AArch64::ST1Onev16b_POST,   "st1",  ".16b",   1, false, 16 },
-  { AArch64::ST1Onev8h_POST,    "st1",  ".8h",    1, false, 16 },
-  { AArch64::ST1Onev4s_POST,    "st1",  ".4s",    1, false, 16 },
-  { AArch64::ST1Onev2d_POST,    "st1",  ".2d",    1, false, 16 },
-  { AArch64::ST1Onev8b_POST,    "st1",  ".8b",    1, false, 8  },
-  { AArch64::ST1Onev4h_POST,    "st1",  ".4h",    1, false, 8  },
-  { AArch64::ST1Onev2s_POST,    "st1",  ".2s",    1, false, 8  },
-  { AArch64::ST1Onev1d_POST,    "st1",  ".1d",    1, false, 8  },
-  { AArch64::ST1Twov16b,        "st1",  ".16b",   0, false, 0  },
-  { AArch64::ST1Twov8h,         "st1",  ".8h",    0, false, 0  },
-  { AArch64::ST1Twov4s,         "st1",  ".4s",    0, false, 0  },
-  { AArch64::ST1Twov2d,         "st1",  ".2d",    0, false, 0  },
-  { AArch64::ST1Twov8b,         "st1",  ".8b",    0, false, 0  },
-  { AArch64::ST1Twov4h,         "st1",  ".4h",    0, false, 0  },
-  { AArch64::ST1Twov2s,         "st1",  ".2s",    0, false, 0  },
-  { AArch64::ST1Twov1d,         "st1",  ".1d",    0, false, 0  },
-  { AArch64::ST1Twov16b_POST,   "st1",  ".16b",   1, false, 32 },
-  { AArch64::ST1Twov8h_POST,    "st1",  ".8h",    1, false, 32 },
-  { AArch64::ST1Twov4s_POST,    "st1",  ".4s",    1, false, 32 },
-  { AArch64::ST1Twov2d_POST,    "st1",  ".2d",    1, false, 32 },
-  { AArch64::ST1Twov8b_POST,    "st1",  ".8b",    1, false, 16 },
-  { AArch64::ST1Twov4h_POST,    "st1",  ".4h",    1, false, 16 },
-  { AArch64::ST1Twov2s_POST,    "st1",  ".2s",    1, false, 16 },
-  { AArch64::ST1Twov1d_POST,    "st1",  ".1d",    1, false, 16 },
-  { AArch64::ST1Threev16b,      "st1",  ".16b",   0, false, 0  },
-  { AArch64::ST1Threev8h,       "st1",  ".8h",    0, false, 0  },
-  { AArch64::ST1Threev4s,       "st1",  ".4s",    0, false, 0  },
-  { AArch64::ST1Threev2d,       "st1",  ".2d",    0, false, 0  },
-  { AArch64::ST1Threev8b,       "st1",  ".8b",    0, false, 0  },
-  { AArch64::ST1Threev4h,       "st1",  ".4h",    0, false, 0  },
-  { AArch64::ST1Threev2s,       "st1",  ".2s",    0, false, 0  },
-  { AArch64::ST1Threev1d,       "st1",  ".1d",    0, false, 0  },
-  { AArch64::ST1Threev16b_POST, "st1",  ".16b",   1, false, 48 },
-  { AArch64::ST1Threev8h_POST,  "st1",  ".8h",    1, false, 48 },
-  { AArch64::ST1Threev4s_POST,  "st1",  ".4s",    1, false, 48 },
-  { AArch64::ST1Threev2d_POST,  "st1",  ".2d",    1, false, 48 },
-  { AArch64::ST1Threev8b_POST,  "st1",  ".8b",    1, false, 24 },
-  { AArch64::ST1Threev4h_POST,  "st1",  ".4h",    1, false, 24 },
-  { AArch64::ST1Threev2s_POST,  "st1",  ".2s",    1, false, 24 },
-  { AArch64::ST1Threev1d_POST,  "st1",  ".1d",    1, false, 24 },
-  { AArch64::ST1Fourv16b,       "st1",  ".16b",   0, false, 0  },
-  { AArch64::ST1Fourv8h,        "st1",  ".8h",    0, false, 0  },
-  { AArch64::ST1Fourv4s,        "st1",  ".4s",    0, false, 0  },
-  { AArch64::ST1Fourv2d,        "st1",  ".2d",    0, false, 0  },
-  { AArch64::ST1Fourv8b,        "st1",  ".8b",    0, false, 0  },
-  { AArch64::ST1Fourv4h,        "st1",  ".4h",    0, false, 0  },
-  { AArch64::ST1Fourv2s,        "st1",  ".2s",    0, false, 0  },
-  { AArch64::ST1Fourv1d,        "st1",  ".1d",    0, false, 0  },
-  { AArch64::ST1Fourv16b_POST,  "st1",  ".16b",   1, false, 64 },
-  { AArch64::ST1Fourv8h_POST,   "st1",  ".8h",    1, false, 64 },
-  { AArch64::ST1Fourv4s_POST,   "st1",  ".4s",    1, false, 64 },
-  { AArch64::ST1Fourv2d_POST,   "st1",  ".2d",    1, false, 64 },
-  { AArch64::ST1Fourv8b_POST,   "st1",  ".8b",    1, false, 32 },
-  { AArch64::ST1Fourv4h_POST,   "st1",  ".4h",    1, false, 32 },
-  { AArch64::ST1Fourv2s_POST,   "st1",  ".2s",    1, false, 32 },
-  { AArch64::ST1Fourv1d_POST,   "st1",  ".1d",    1, false, 32 },
-  { AArch64::ST2i8,             "st2",  ".b",     0, true,  0  },
-  { AArch64::ST2i16,            "st2",  ".h",     0, true,  0  },
-  { AArch64::ST2i32,            "st2",  ".s",     0, true,  0  },
-  { AArch64::ST2i64,            "st2",  ".d",     0, true,  0  },
-  { AArch64::ST2i8_POST,        "st2",  ".b",     1, true,  2  },
-  { AArch64::ST2i16_POST,       "st2",  ".h",     1, true,  4  },
-  { AArch64::ST2i32_POST,       "st2",  ".s",     1, true,  8  },
-  { AArch64::ST2i64_POST,       "st2",  ".d",     1, true,  16 },
-  { AArch64::ST2Twov16b,        "st2",  ".16b",   0, false, 0  },
-  { AArch64::ST2Twov8h,         "st2",  ".8h",    0, false, 0  },
-  { AArch64::ST2Twov4s,         "st2",  ".4s",    0, false, 0  },
-  { AArch64::ST2Twov2d,         "st2",  ".2d",    0, false, 0  },
-  { AArch64::ST2Twov8b,         "st2",  ".8b",    0, false, 0  },
-  { AArch64::ST2Twov4h,         "st2",  ".4h",    0, false, 0  },
-  { AArch64::ST2Twov2s,         "st2",  ".2s",    0, false, 0  },
-  { AArch64::ST2Twov16b_POST,   "st2",  ".16b",   1, false, 32 },
-  { AArch64::ST2Twov8h_POST,    "st2",  ".8h",    1, false, 32 },
-  { AArch64::ST2Twov4s_POST,    "st2",  ".4s",    1, false, 32 },
-  { AArch64::ST2Twov2d_POST,    "st2",  ".2d",    1, false, 32 },
-  { AArch64::ST2Twov8b_POST,    "st2",  ".8b",    1, false, 16 },
-  { AArch64::ST2Twov4h_POST,    "st2",  ".4h",    1, false, 16 },
-  { AArch64::ST2Twov2s_POST,    "st2",  ".2s",    1, false, 16 },
-  { AArch64::ST3i8,             "st3",  ".b",     0, true,  0  },
-  { AArch64::ST3i16,            "st3",  ".h",     0, true,  0  },
-  { AArch64::ST3i32,            "st3",  ".s",     0, true,  0  },
-  { AArch64::ST3i64,            "st3",  ".d",     0, true,  0  },
-  { AArch64::ST3i8_POST,        "st3",  ".b",     1, true,  3  },
-  { AArch64::ST3i16_POST,       "st3",  ".h",     1, true,  6  },
-  { AArch64::ST3i32_POST,       "st3",  ".s",     1, true,  12 },
-  { AArch64::ST3i64_POST,       "st3",  ".d",     1, true,  24 },
-  { AArch64::ST3Threev16b,      "st3",  ".16b",   0, false, 0  },
-  { AArch64::ST3Threev8h,       "st3",  ".8h",    0, false, 0  },
-  { AArch64::ST3Threev4s,       "st3",  ".4s",    0, false, 0  },
-  { AArch64::ST3Threev2d,       "st3",  ".2d",    0, false, 0  },
-  { AArch64::ST3Threev8b,       "st3",  ".8b",    0, false, 0  },
-  { AArch64::ST3Threev4h,       "st3",  ".4h",    0, false, 0  },
-  { AArch64::ST3Threev2s,       "st3",  ".2s",    0, false, 0  },
-  { AArch64::ST3Threev16b_POST, "st3",  ".16b",   1, false, 48 },
-  { AArch64::ST3Threev8h_POST,  "st3",  ".8h",    1, false, 48 },
-  { AArch64::ST3Threev4s_POST,  "st3",  ".4s",    1, false, 48 },
-  { AArch64::ST3Threev2d_POST,  "st3",  ".2d",    1, false, 48 },
-  { AArch64::ST3Threev8b_POST,  "st3",  ".8b",    1, false, 24 },
-  { AArch64::ST3Threev4h_POST,  "st3",  ".4h",    1, false, 24 },
-  { AArch64::ST3Threev2s_POST,  "st3",  ".2s",    1, false, 24 },
-  { AArch64::ST4i8,             "st4",  ".b",     0, true,  0  },
-  { AArch64::ST4i16,            "st4",  ".h",     0, true,  0  },
-  { AArch64::ST4i32,            "st4",  ".s",     0, true,  0  },
-  { AArch64::ST4i64,            "st4",  ".d",     0, true,  0  },
-  { AArch64::ST4i8_POST,        "st4",  ".b",     1, true,  4  },
-  { AArch64::ST4i16_POST,       "st4",  ".h",     1, true,  8  },
-  { AArch64::ST4i32_POST,       "st4",  ".s",     1, true,  16 },
-  { AArch64::ST4i64_POST,       "st4",  ".d",     1, true,  32 },
-  { AArch64::ST4Fourv16b,       "st4",  ".16b",   0, false, 0  },
-  { AArch64::ST4Fourv8h,        "st4",  ".8h",    0, false, 0  },
-  { AArch64::ST4Fourv4s,        "st4",  ".4s",    0, false, 0  },
-  { AArch64::ST4Fourv2d,        "st4",  ".2d",    0, false, 0  },
-  { AArch64::ST4Fourv8b,        "st4",  ".8b",    0, false, 0  },
-  { AArch64::ST4Fourv4h,        "st4",  ".4h",    0, false, 0  },
-  { AArch64::ST4Fourv2s,        "st4",  ".2s",    0, false, 0  },
-  { AArch64::ST4Fourv16b_POST,  "st4",  ".16b",   1, false, 64 },
-  { AArch64::ST4Fourv8h_POST,   "st4",  ".8h",    1, false, 64 },
-  { AArch64::ST4Fourv4s_POST,   "st4",  ".4s",    1, false, 64 },
-  { AArch64::ST4Fourv2d_POST,   "st4",  ".2d",    1, false, 64 },
-  { AArch64::ST4Fourv8b_POST,   "st4",  ".8b",    1, false, 32 },
-  { AArch64::ST4Fourv4h_POST,   "st4",  ".4h",    1, false, 32 },
-  { AArch64::ST4Fourv2s_POST,   "st4",  ".2s",    1, false, 32 },
+    {AArch64::LD1i8, "ld1", ".b", 1, true, 0},
+    {AArch64::LD1i16, "ld1", ".h", 1, true, 0},
+    {AArch64::LD1i32, "ld1", ".s", 1, true, 0},
+    {AArch64::LD1i64, "ld1", ".d", 1, true, 0},
+    {AArch64::LD1i8_POST, "ld1", ".b", 2, true, 1},
+    {AArch64::LD1i16_POST, "ld1", ".h", 2, true, 2},
+    {AArch64::LD1i32_POST, "ld1", ".s", 2, true, 4},
+    {AArch64::LD1i64_POST, "ld1", ".d", 2, true, 8},
+    {AArch64::LD1Rv16b, "ld1r", ".16b", 0, false, 0},
+    {AArch64::LD1Rv8h, "ld1r", ".8h", 0, false, 0},
+    {AArch64::LD1Rv4s, "ld1r", ".4s", 0, false, 0},
+    {AArch64::LD1Rv2d, "ld1r", ".2d", 0, false, 0},
+    {AArch64::LD1Rv8b, "ld1r", ".8b", 0, false, 0},
+    {AArch64::LD1Rv4h, "ld1r", ".4h", 0, false, 0},
+    {AArch64::LD1Rv2s, "ld1r", ".2s", 0, false, 0},
+    {AArch64::LD1Rv1d, "ld1r", ".1d", 0, false, 0},
+    {AArch64::LD1Rv16b_POST, "ld1r", ".16b", 1, false, 1},
+    {AArch64::LD1Rv8h_POST, "ld1r", ".8h", 1, false, 2},
+    {AArch64::LD1Rv4s_POST, "ld1r", ".4s", 1, false, 4},
+    {AArch64::LD1Rv2d_POST, "ld1r", ".2d", 1, false, 8},
+    {AArch64::LD1Rv8b_POST, "ld1r", ".8b", 1, false, 1},
+    {AArch64::LD1Rv4h_POST, "ld1r", ".4h", 1, false, 2},
+    {AArch64::LD1Rv2s_POST, "ld1r", ".2s", 1, false, 4},
+    {AArch64::LD1Rv1d_POST, "ld1r", ".1d", 1, false, 8},
+    {AArch64::LD1Onev16b, "ld1", ".16b", 0, false, 0},
+    {AArch64::LD1Onev8h, "ld1", ".8h", 0, false, 0},
+    {AArch64::LD1Onev4s, "ld1", ".4s", 0, false, 0},
+    {AArch64::LD1Onev2d, "ld1", ".2d", 0, false, 0},
+    {AArch64::LD1Onev8b, "ld1", ".8b", 0, false, 0},
+    {AArch64::LD1Onev4h, "ld1", ".4h", 0, false, 0},
+    {AArch64::LD1Onev2s, "ld1", ".2s", 0, false, 0},
+    {AArch64::LD1Onev1d, "ld1", ".1d", 0, false, 0},
+    {AArch64::LD1Onev16b_POST, "ld1", ".16b", 1, false, 16},
+    {AArch64::LD1Onev8h_POST, "ld1", ".8h", 1, false, 16},
+    {AArch64::LD1Onev4s_POST, "ld1", ".4s", 1, false, 16},
+    {AArch64::LD1Onev2d_POST, "ld1", ".2d", 1, false, 16},
+    {AArch64::LD1Onev8b_POST, "ld1", ".8b", 1, false, 8},
+    {AArch64::LD1Onev4h_POST, "ld1", ".4h", 1, false, 8},
+    {AArch64::LD1Onev2s_POST, "ld1", ".2s", 1, false, 8},
+    {AArch64::LD1Onev1d_POST, "ld1", ".1d", 1, false, 8},
+    {AArch64::LD1Twov16b, "ld1", ".16b", 0, false, 0},
+    {AArch64::LD1Twov8h, "ld1", ".8h", 0, false, 0},
+    {AArch64::LD1Twov4s, "ld1", ".4s", 0, false, 0},
+    {AArch64::LD1Twov2d, "ld1", ".2d", 0, false, 0},
+    {AArch64::LD1Twov8b, "ld1", ".8b", 0, false, 0},
+    {AArch64::LD1Twov4h, "ld1", ".4h", 0, false, 0},
+    {AArch64::LD1Twov2s, "ld1", ".2s", 0, false, 0},
+    {AArch64::LD1Twov1d, "ld1", ".1d", 0, false, 0},
+    {AArch64::LD1Twov16b_POST, "ld1", ".16b", 1, false, 32},
+    {AArch64::LD1Twov8h_POST, "ld1", ".8h", 1, false, 32},
+    {AArch64::LD1Twov4s_POST, "ld1", ".4s", 1, false, 32},
+    {AArch64::LD1Twov2d_POST, "ld1", ".2d", 1, false, 32},
+    {AArch64::LD1Twov8b_POST, "ld1", ".8b", 1, false, 16},
+    {AArch64::LD1Twov4h_POST, "ld1", ".4h", 1, false, 16},
+    {AArch64::LD1Twov2s_POST, "ld1", ".2s", 1, false, 16},
+    {AArch64::LD1Twov1d_POST, "ld1", ".1d", 1, false, 16},
+    {AArch64::LD1Threev16b, "ld1", ".16b", 0, false, 0},
+    {AArch64::LD1Threev8h, "ld1", ".8h", 0, false, 0},
+    {AArch64::LD1Threev4s, "ld1", ".4s", 0, false, 0},
+    {AArch64::LD1Threev2d, "ld1", ".2d", 0, false, 0},
+    {AArch64::LD1Threev8b, "ld1", ".8b", 0, false, 0},
+    {AArch64::LD1Threev4h, "ld1", ".4h", 0, false, 0},
+    {AArch64::LD1Threev2s, "ld1", ".2s", 0, false, 0},
+    {AArch64::LD1Threev1d, "ld1", ".1d", 0, false, 0},
+    {AArch64::LD1Threev16b_POST, "ld1", ".16b", 1, false, 48},
+    {AArch64::LD1Threev8h_POST, "ld1", ".8h", 1, false, 48},
+    {AArch64::LD1Threev4s_POST, "ld1", ".4s", 1, false, 48},
+    {AArch64::LD1Threev2d_POST, "ld1", ".2d", 1, false, 48},
+    {AArch64::LD1Threev8b_POST, "ld1", ".8b", 1, false, 24},
+    {AArch64::LD1Threev4h_POST, "ld1", ".4h", 1, false, 24},
+    {AArch64::LD1Threev2s_POST, "ld1", ".2s", 1, false, 24},
+    {AArch64::LD1Threev1d_POST, "ld1", ".1d", 1, false, 24},
+    {AArch64::LD1Fourv16b, "ld1", ".16b", 0, false, 0},
+    {AArch64::LD1Fourv8h, "ld1", ".8h", 0, false, 0},
+    {AArch64::LD1Fourv4s, "ld1", ".4s", 0, false, 0},
+    {AArch64::LD1Fourv2d, "ld1", ".2d", 0, false, 0},
+    {AArch64::LD1Fourv8b, "ld1", ".8b", 0, false, 0},
+    {AArch64::LD1Fourv4h, "ld1", ".4h", 0, false, 0},
+    {AArch64::LD1Fourv2s, "ld1", ".2s", 0, false, 0},
+    {AArch64::LD1Fourv1d, "ld1", ".1d", 0, false, 0},
+    {AArch64::LD1Fourv16b_POST, "ld1", ".16b", 1, false, 64},
+    {AArch64::LD1Fourv8h_POST, "ld1", ".8h", 1, false, 64},
+    {AArch64::LD1Fourv4s_POST, "ld1", ".4s", 1, false, 64},
+    {AArch64::LD1Fourv2d_POST, "ld1", ".2d", 1, false, 64},
+    {AArch64::LD1Fourv8b_POST, "ld1", ".8b", 1, false, 32},
+    {AArch64::LD1Fourv4h_POST, "ld1", ".4h", 1, false, 32},
+    {AArch64::LD1Fourv2s_POST, "ld1", ".2s", 1, false, 32},
+    {AArch64::LD1Fourv1d_POST, "ld1", ".1d", 1, false, 32},
+    {AArch64::LD2i8, "ld2", ".b", 1, true, 0},
+    {AArch64::LD2i16, "ld2", ".h", 1, true, 0},
+    {AArch64::LD2i32, "ld2", ".s", 1, true, 0},
+    {AArch64::LD2i64, "ld2", ".d", 1, true, 0},
+    {AArch64::LD2i8_POST, "ld2", ".b", 2, true, 2},
+    {AArch64::LD2i16_POST, "ld2", ".h", 2, true, 4},
+    {AArch64::LD2i32_POST, "ld2", ".s", 2, true, 8},
+    {AArch64::LD2i64_POST, "ld2", ".d", 2, true, 16},
+    {AArch64::LD2Rv16b, "ld2r", ".16b", 0, false, 0},
+    {AArch64::LD2Rv8h, "ld2r", ".8h", 0, false, 0},
+    {AArch64::LD2Rv4s, "ld2r", ".4s", 0, false, 0},
+    {AArch64::LD2Rv2d, "ld2r", ".2d", 0, false, 0},
+    {AArch64::LD2Rv8b, "ld2r", ".8b", 0, false, 0},
+    {AArch64::LD2Rv4h, "ld2r", ".4h", 0, false, 0},
+    {AArch64::LD2Rv2s, "ld2r", ".2s", 0, false, 0},
+    {AArch64::LD2Rv1d, "ld2r", ".1d", 0, false, 0},
+    {AArch64::LD2Rv16b_POST, "ld2r", ".16b", 1, false, 2},
+    {AArch64::LD2Rv8h_POST, "ld2r", ".8h", 1, false, 4},
+    {AArch64::LD2Rv4s_POST, "ld2r", ".4s", 1, false, 8},
+    {AArch64::LD2Rv2d_POST, "ld2r", ".2d", 1, false, 16},
+    {AArch64::LD2Rv8b_POST, "ld2r", ".8b", 1, false, 2},
+    {AArch64::LD2Rv4h_POST, "ld2r", ".4h", 1, false, 4},
+    {AArch64::LD2Rv2s_POST, "ld2r", ".2s", 1, false, 8},
+    {AArch64::LD2Rv1d_POST, "ld2r", ".1d", 1, false, 16},
+    {AArch64::LD2Twov16b, "ld2", ".16b", 0, false, 0},
+    {AArch64::LD2Twov8h, "ld2", ".8h", 0, false, 0},
+    {AArch64::LD2Twov4s, "ld2", ".4s", 0, false, 0},
+    {AArch64::LD2Twov2d, "ld2", ".2d", 0, false, 0},
+    {AArch64::LD2Twov8b, "ld2", ".8b", 0, false, 0},
+    {AArch64::LD2Twov4h, "ld2", ".4h", 0, false, 0},
+    {AArch64::LD2Twov2s, "ld2", ".2s", 0, false, 0},
+    {AArch64::LD2Twov16b_POST, "ld2", ".16b", 1, false, 32},
+    {AArch64::LD2Twov8h_POST, "ld2", ".8h", 1, false, 32},
+    {AArch64::LD2Twov4s_POST, "ld2", ".4s", 1, false, 32},
+    {AArch64::LD2Twov2d_POST, "ld2", ".2d", 1, false, 32},
+    {AArch64::LD2Twov8b_POST, "ld2", ".8b", 1, false, 16},
+    {AArch64::LD2Twov4h_POST, "ld2", ".4h", 1, false, 16},
+    {AArch64::LD2Twov2s_POST, "ld2", ".2s", 1, false, 16},
+    {AArch64::LD3i8, "ld3", ".b", 1, true, 0},
+    {AArch64::LD3i16, "ld3", ".h", 1, true, 0},
+    {AArch64::LD3i32, "ld3", ".s", 1, true, 0},
+    {AArch64::LD3i64, "ld3", ".d", 1, true, 0},
+    {AArch64::LD3i8_POST, "ld3", ".b", 2, true, 3},
+    {AArch64::LD3i16_POST, "ld3", ".h", 2, true, 6},
+    {AArch64::LD3i32_POST, "ld3", ".s", 2, true, 12},
+    {AArch64::LD3i64_POST, "ld3", ".d", 2, true, 24},
+    {AArch64::LD3Rv16b, "ld3r", ".16b", 0, false, 0},
+    {AArch64::LD3Rv8h, "ld3r", ".8h", 0, false, 0},
+    {AArch64::LD3Rv4s, "ld3r", ".4s", 0, false, 0},
+    {AArch64::LD3Rv2d, "ld3r", ".2d", 0, false, 0},
+    {AArch64::LD3Rv8b, "ld3r", ".8b", 0, false, 0},
+    {AArch64::LD3Rv4h, "ld3r", ".4h", 0, false, 0},
+    {AArch64::LD3Rv2s, "ld3r", ".2s", 0, false, 0},
+    {AArch64::LD3Rv1d, "ld3r", ".1d", 0, false, 0},
+    {AArch64::LD3Rv16b_POST, "ld3r", ".16b", 1, false, 3},
+    {AArch64::LD3Rv8h_POST, "ld3r", ".8h", 1, false, 6},
+    {AArch64::LD3Rv4s_POST, "ld3r", ".4s", 1, false, 12},
+    {AArch64::LD3Rv2d_POST, "ld3r", ".2d", 1, false, 24},
+    {AArch64::LD3Rv8b_POST, "ld3r", ".8b", 1, false, 3},
+    {AArch64::LD3Rv4h_POST, "ld3r", ".4h", 1, false, 6},
+    {AArch64::LD3Rv2s_POST, "ld3r", ".2s", 1, false, 12},
+    {AArch64::LD3Rv1d_POST, "ld3r", ".1d", 1, false, 24},
+    {AArch64::LD3Threev16b, "ld3", ".16b", 0, false, 0},
+    {AArch64::LD3Threev8h, "ld3", ".8h", 0, false, 0},
+    {AArch64::LD3Threev4s, "ld3", ".4s", 0, false, 0},
+    {AArch64::LD3Threev2d, "ld3", ".2d", 0, false, 0},
+    {AArch64::LD3Threev8b, "ld3", ".8b", 0, false, 0},
+    {AArch64::LD3Threev4h, "ld3", ".4h", 0, false, 0},
+    {AArch64::LD3Threev2s, "ld3", ".2s", 0, false, 0},
+    {AArch64::LD3Threev16b_POST, "ld3", ".16b", 1, false, 48},
+    {AArch64::LD3Threev8h_POST, "ld3", ".8h", 1, false, 48},
+    {AArch64::LD3Threev4s_POST, "ld3", ".4s", 1, false, 48},
+    {AArch64::LD3Threev2d_POST, "ld3", ".2d", 1, false, 48},
+    {AArch64::LD3Threev8b_POST, "ld3", ".8b", 1, false, 24},
+    {AArch64::LD3Threev4h_POST, "ld3", ".4h", 1, false, 24},
+    {AArch64::LD3Threev2s_POST, "ld3", ".2s", 1, false, 24},
+    {AArch64::LD4i8, "ld4", ".b", 1, true, 0},
+    {AArch64::LD4i16, "ld4", ".h", 1, true, 0},
+    {AArch64::LD4i32, "ld4", ".s", 1, true, 0},
+    {AArch64::LD4i64, "ld4", ".d", 1, true, 0},
+    {AArch64::LD4i8_POST, "ld4", ".b", 2, true, 4},
+    {AArch64::LD4i16_POST, "ld4", ".h", 2, true, 8},
+    {AArch64::LD4i32_POST, "ld4", ".s", 2, true, 16},
+    {AArch64::LD4i64_POST, "ld4", ".d", 2, true, 32},
+    {AArch64::LD4Rv16b, "ld4r", ".16b", 0, false, 0},
+    {AArch64::LD4Rv8h, "ld4r", ".8h", 0, false, 0},
+    {AArch64::LD4Rv4s, "ld4r", ".4s", 0, false, 0},
+    {AArch64::LD4Rv2d, "ld4r", ".2d", 0, false, 0},
+    {AArch64::LD4Rv8b, "ld4r", ".8b", 0, false, 0},
+    {AArch64::LD4Rv4h, "ld4r", ".4h", 0, false, 0},
+    {AArch64::LD4Rv2s, "ld4r", ".2s", 0, false, 0},
+    {AArch64::LD4Rv1d, "ld4r", ".1d", 0, false, 0},
+    {AArch64::LD4Rv16b_POST, "ld4r", ".16b", 1, false, 4},
+    {AArch64::LD4Rv8h_POST, "ld4r", ".8h", 1, false, 8},
+    {AArch64::LD4Rv4s_POST, "ld4r", ".4s", 1, false, 16},
+    {AArch64::LD4Rv2d_POST, "ld4r", ".2d", 1, false, 32},
+    {AArch64::LD4Rv8b_POST, "ld4r", ".8b", 1, false, 4},
+    {AArch64::LD4Rv4h_POST, "ld4r", ".4h", 1, false, 8},
+    {AArch64::LD4Rv2s_POST, "ld4r", ".2s", 1, false, 16},
+    {AArch64::LD4Rv1d_POST, "ld4r", ".1d", 1, false, 32},
+    {AArch64::LD4Fourv16b, "ld4", ".16b", 0, false, 0},
+    {AArch64::LD4Fourv8h, "ld4", ".8h", 0, false, 0},
+    {AArch64::LD4Fourv4s, "ld4", ".4s", 0, false, 0},
+    {AArch64::LD4Fourv2d, "ld4", ".2d", 0, false, 0},
+    {AArch64::LD4Fourv8b, "ld4", ".8b", 0, false, 0},
+    {AArch64::LD4Fourv4h, "ld4", ".4h", 0, false, 0},
+    {AArch64::LD4Fourv2s, "ld4", ".2s", 0, false, 0},
+    {AArch64::LD4Fourv16b_POST, "ld4", ".16b", 1, false, 64},
+    {AArch64::LD4Fourv8h_POST, "ld4", ".8h", 1, false, 64},
+    {AArch64::LD4Fourv4s_POST, "ld4", ".4s", 1, false, 64},
+    {AArch64::LD4Fourv2d_POST, "ld4", ".2d", 1, false, 64},
+    {AArch64::LD4Fourv8b_POST, "ld4", ".8b", 1, false, 32},
+    {AArch64::LD4Fourv4h_POST, "ld4", ".4h", 1, false, 32},
+    {AArch64::LD4Fourv2s_POST, "ld4", ".2s", 1, false, 32},
+    {AArch64::ST1i8, "st1", ".b", 0, true, 0},
+    {AArch64::ST1i16, "st1", ".h", 0, true, 0},
+    {AArch64::ST1i32, "st1", ".s", 0, true, 0},
+    {AArch64::ST1i64, "st1", ".d", 0, true, 0},
+    {AArch64::ST1i8_POST, "st1", ".b", 1, true, 1},
+    {AArch64::ST1i16_POST, "st1", ".h", 1, true, 2},
+    {AArch64::ST1i32_POST, "st1", ".s", 1, true, 4},
+    {AArch64::ST1i64_POST, "st1", ".d", 1, true, 8},
+    {AArch64::ST1Onev16b, "st1", ".16b", 0, false, 0},
+    {AArch64::ST1Onev8h, "st1", ".8h", 0, false, 0},
+    {AArch64::ST1Onev4s, "st1", ".4s", 0, false, 0},
+    {AArch64::ST1Onev2d, "st1", ".2d", 0, false, 0},
+    {AArch64::ST1Onev8b, "st1", ".8b", 0, false, 0},
+    {AArch64::ST1Onev4h, "st1", ".4h", 0, false, 0},
+    {AArch64::ST1Onev2s, "st1", ".2s", 0, false, 0},
+    {AArch64::ST1Onev1d, "st1", ".1d", 0, false, 0},
+    {AArch64::ST1Onev16b_POST, "st1", ".16b", 1, false, 16},
+    {AArch64::ST1Onev8h_POST, "st1", ".8h", 1, false, 16},
+    {AArch64::ST1Onev4s_POST, "st1", ".4s", 1, false, 16},
+    {AArch64::ST1Onev2d_POST, "st1", ".2d", 1, false, 16},
+    {AArch64::ST1Onev8b_POST, "st1", ".8b", 1, false, 8},
+    {AArch64::ST1Onev4h_POST, "st1", ".4h", 1, false, 8},
+    {AArch64::ST1Onev2s_POST, "st1", ".2s", 1, false, 8},
+    {AArch64::ST1Onev1d_POST, "st1", ".1d", 1, false, 8},
+    {AArch64::ST1Twov16b, "st1", ".16b", 0, false, 0},
+    {AArch64::ST1Twov8h, "st1", ".8h", 0, false, 0},
+    {AArch64::ST1Twov4s, "st1", ".4s", 0, false, 0},
+    {AArch64::ST1Twov2d, "st1", ".2d", 0, false, 0},
+    {AArch64::ST1Twov8b, "st1", ".8b", 0, false, 0},
+    {AArch64::ST1Twov4h, "st1", ".4h", 0, false, 0},
+    {AArch64::ST1Twov2s, "st1", ".2s", 0, false, 0},
+    {AArch64::ST1Twov1d, "st1", ".1d", 0, false, 0},
+    {AArch64::ST1Twov16b_POST, "st1", ".16b", 1, false, 32},
+    {AArch64::ST1Twov8h_POST, "st1", ".8h", 1, false, 32},
+    {AArch64::ST1Twov4s_POST, "st1", ".4s", 1, false, 32},
+    {AArch64::ST1Twov2d_POST, "st1", ".2d", 1, false, 32},
+    {AArch64::ST1Twov8b_POST, "st1", ".8b", 1, false, 16},
+    {AArch64::ST1Twov4h_POST, "st1", ".4h", 1, false, 16},
+    {AArch64::ST1Twov2s_POST, "st1", ".2s", 1, false, 16},
+    {AArch64::ST1Twov1d_POST, "st1", ".1d", 1, false, 16},
+    {AArch64::ST1Threev16b, "st1", ".16b", 0, false, 0},
+    {AArch64::ST1Threev8h, "st1", ".8h", 0, false, 0},
+    {AArch64::ST1Threev4s, "st1", ".4s", 0, false, 0},
+    {AArch64::ST1Threev2d, "st1", ".2d", 0, false, 0},
+    {AArch64::ST1Threev8b, "st1", ".8b", 0, false, 0},
+    {AArch64::ST1Threev4h, "st1", ".4h", 0, false, 0},
+    {AArch64::ST1Threev2s, "st1", ".2s", 0, false, 0},
+    {AArch64::ST1Threev1d, "st1", ".1d", 0, false, 0},
+    {AArch64::ST1Threev16b_POST, "st1", ".16b", 1, false, 48},
+    {AArch64::ST1Threev8h_POST, "st1", ".8h", 1, false, 48},
+    {AArch64::ST1Threev4s_POST, "st1", ".4s", 1, false, 48},
+    {AArch64::ST1Threev2d_POST, "st1", ".2d", 1, false, 48},
+    {AArch64::ST1Threev8b_POST, "st1", ".8b", 1, false, 24},
+    {AArch64::ST1Threev4h_POST, "st1", ".4h", 1, false, 24},
+    {AArch64::ST1Threev2s_POST, "st1", ".2s", 1, false, 24},
+    {AArch64::ST1Threev1d_POST, "st1", ".1d", 1, false, 24},
+    {AArch64::ST1Fourv16b, "st1", ".16b", 0, false, 0},
+    {AArch64::ST1Fourv8h, "st1", ".8h", 0, false, 0},
+    {AArch64::ST1Fourv4s, "st1", ".4s", 0, false, 0},
+    {AArch64::ST1Fourv2d, "st1", ".2d", 0, false, 0},
+    {AArch64::ST1Fourv8b, "st1", ".8b", 0, false, 0},
+    {AArch64::ST1Fourv4h, "st1", ".4h", 0, false, 0},
+    {AArch64::ST1Fourv2s, "st1", ".2s", 0, false, 0},
+    {AArch64::ST1Fourv1d, "st1", ".1d", 0, false, 0},
+    {AArch64::ST1Fourv16b_POST, "st1", ".16b", 1, false, 64},
+    {AArch64::ST1Fourv8h_POST, "st1", ".8h", 1, false, 64},
+    {AArch64::ST1Fourv4s_POST, "st1", ".4s", 1, false, 64},
+    {AArch64::ST1Fourv2d_POST, "st1", ".2d", 1, false, 64},
+    {AArch64::ST1Fourv8b_POST, "st1", ".8b", 1, false, 32},
+    {AArch64::ST1Fourv4h_POST, "st1", ".4h", 1, false, 32},
+    {AArch64::ST1Fourv2s_POST, "st1", ".2s", 1, false, 32},
+    {AArch64::ST1Fourv1d_POST, "st1", ".1d", 1, false, 32},
+    {AArch64::ST2i8, "st2", ".b", 0, true, 0},
+    {AArch64::ST2i16, "st2", ".h", 0, true, 0},
+    {AArch64::ST2i32, "st2", ".s", 0, true, 0},
+    {AArch64::ST2i64, "st2", ".d", 0, true, 0},
+    {AArch64::ST2i8_POST, "st2", ".b", 1, true, 2},
+    {AArch64::ST2i16_POST, "st2", ".h", 1, true, 4},
+    {AArch64::ST2i32_POST, "st2", ".s", 1, true, 8},
+    {AArch64::ST2i64_POST, "st2", ".d", 1, true, 16},
+    {AArch64::ST2Twov16b, "st2", ".16b", 0, false, 0},
+    {AArch64::ST2Twov8h, "st2", ".8h", 0, false, 0},
+    {AArch64::ST2Twov4s, "st2", ".4s", 0, false, 0},
+    {AArch64::ST2Twov2d, "st2", ".2d", 0, false, 0},
+    {AArch64::ST2Twov8b, "st2", ".8b", 0, false, 0},
+    {AArch64::ST2Twov4h, "st2", ".4h", 0, false, 0},
+    {AArch64::ST2Twov2s, "st2", ".2s", 0, false, 0},
+    {AArch64::ST2Twov16b_POST, "st2", ".16b", 1, false, 32},
+    {AArch64::ST2Twov8h_POST, "st2", ".8h", 1, false, 32},
+    {AArch64::ST2Twov4s_POST, "st2", ".4s", 1, false, 32},
+    {AArch64::ST2Twov2d_POST, "st2", ".2d", 1, false, 32},
+    {AArch64::ST2Twov8b_POST, "st2", ".8b", 1, false, 16},
+    {AArch64::ST2Twov4h_POST, "st2", ".4h", 1, false, 16},
+    {AArch64::ST2Twov2s_POST, "st2", ".2s", 1, false, 16},
+    {AArch64::ST3i8, "st3", ".b", 0, true, 0},
+    {AArch64::ST3i16, "st3", ".h", 0, true, 0},
+    {AArch64::ST3i32, "st3", ".s", 0, true, 0},
+    {AArch64::ST3i64, "st3", ".d", 0, true, 0},
+    {AArch64::ST3i8_POST, "st3", ".b", 1, true, 3},
+    {AArch64::ST3i16_POST, "st3", ".h", 1, true, 6},
+    {AArch64::ST3i32_POST, "st3", ".s", 1, true, 12},
+    {AArch64::ST3i64_POST, "st3", ".d", 1, true, 24},
+    {AArch64::ST3Threev16b, "st3", ".16b", 0, false, 0},
+    {AArch64::ST3Threev8h, "st3", ".8h", 0, false, 0},
+    {AArch64::ST3Threev4s, "st3", ".4s", 0, false, 0},
+    {AArch64::ST3Threev2d, "st3", ".2d", 0, false, 0},
+    {AArch64::ST3Threev8b, "st3", ".8b", 0, false, 0},
+    {AArch64::ST3Threev4h, "st3", ".4h", 0, false, 0},
+    {AArch64::ST3Threev2s, "st3", ".2s", 0, false, 0},
+    {AArch64::ST3Threev16b_POST, "st3", ".16b", 1, false, 48},
+    {AArch64::ST3Threev8h_POST, "st3", ".8h", 1, false, 48},
+    {AArch64::ST3Threev4s_POST, "st3", ".4s", 1, false, 48},
+    {AArch64::ST3Threev2d_POST, "st3", ".2d", 1, false, 48},
+    {AArch64::ST3Threev8b_POST, "st3", ".8b", 1, false, 24},
+    {AArch64::ST3Threev4h_POST, "st3", ".4h", 1, false, 24},
+    {AArch64::ST3Threev2s_POST, "st3", ".2s", 1, false, 24},
+    {AArch64::ST4i8, "st4", ".b", 0, true, 0},
+    {AArch64::ST4i16, "st4", ".h", 0, true, 0},
+    {AArch64::ST4i32, "st4", ".s", 0, true, 0},
+    {AArch64::ST4i64, "st4", ".d", 0, true, 0},
+    {AArch64::ST4i8_POST, "st4", ".b", 1, true, 4},
+    {AArch64::ST4i16_POST, "st4", ".h", 1, true, 8},
+    {AArch64::ST4i32_POST, "st4", ".s", 1, true, 16},
+    {AArch64::ST4i64_POST, "st4", ".d", 1, true, 32},
+    {AArch64::ST4Fourv16b, "st4", ".16b", 0, false, 0},
+    {AArch64::ST4Fourv8h, "st4", ".8h", 0, false, 0},
+    {AArch64::ST4Fourv4s, "st4", ".4s", 0, false, 0},
+    {AArch64::ST4Fourv2d, "st4", ".2d", 0, false, 0},
+    {AArch64::ST4Fourv8b, "st4", ".8b", 0, false, 0},
+    {AArch64::ST4Fourv4h, "st4", ".4h", 0, false, 0},
+    {AArch64::ST4Fourv2s, "st4", ".2s", 0, false, 0},
+    {AArch64::ST4Fourv16b_POST, "st4", ".16b", 1, false, 64},
+    {AArch64::ST4Fourv8h_POST, "st4", ".8h", 1, false, 64},
+    {AArch64::ST4Fourv4s_POST, "st4", ".4s", 1, false, 64},
+    {AArch64::ST4Fourv2d_POST, "st4", ".2d", 1, false, 64},
+    {AArch64::ST4Fourv8b_POST, "st4", ".8b", 1, false, 32},
+    {AArch64::ST4Fourv4h_POST, "st4", ".4h", 1, false, 32},
+    {AArch64::ST4Fourv2s_POST, "st4", ".2s", 1, false, 32},
 };
 
 static const LdStNInstrDesc *getLdStNInstrDesc(unsigned Opcode) {
@@ -922,17 +922,21 @@ bool AArch64InstPrinter::printSysAlias(const MCInst *MI,
 
   if (CnVal == 7) {
     switch (CmVal) {
-    default: return false;
+    default:
+      return false;
     // Maybe IC, maybe Prediction Restriction
     case 1:
       switch (Op1Val) {
-      default: return false;
-      case 0: goto Search_IC;
-      case 3: goto Search_PRCTX;
+      default:
+        return false;
+      case 0:
+        goto Search_IC;
+      case 3:
+        goto Search_PRCTX;
       }
     // Prediction Restriction aliases
     case 3: {
-      Search_PRCTX:
+    Search_PRCTX:
       if (Op1Val != 3 || CnVal != 7 || CmVal != 3)
         return false;
 
@@ -943,18 +947,26 @@ bool AArch64InstPrinter::printSysAlias(const MCInst *MI,
 
       NeedsReg = true;
       switch (Op2Val) {
-      default: return false;
-      case 4: Ins = "cfp\t"; break;
-      case 5: Ins = "dvp\t"; break;
-      case 6: Ins = "cosp\t"; break;
-      case 7: Ins = "cpp\t"; break;
+      default:
+        return false;
+      case 4:
+        Ins = "cfp\t";
+        break;
+      case 5:
+        Ins = "dvp\t";
+        break;
+      case 6:
+        Ins = "cosp\t";
+        break;
+      case 7:
+        Ins = "cpp\t";
+        break;
       }
       Name = "RCTX";
-    }
-    break;
+    } break;
     // IC aliases
     case 5: {
-      Search_IC:
+    Search_IC:
       const AArch64IC::IC *IC = AArch64IC::lookupICByEncoding(Encoding);
       if (!IC || !IC->haveFeatures(STI.getFeatureBits()))
         return false;
@@ -962,11 +974,15 @@ bool AArch64InstPrinter::printSysAlias(const MCInst *MI,
       NeedsReg = IC->NeedsReg;
       Ins = "ic\t";
       Name = std::string(IC->Name);
-    }
-    break;
+    } break;
     // DC aliases
-    case 4: case 6: case 10: case 11: case 12: case 13: case 14:
-    {
+    case 4:
+    case 6:
+    case 10:
+    case 11:
+    case 12:
+    case 13:
+    case 14: {
       const AArch64DC::DC *DC = AArch64DC::lookupDCByEncoding(Encoding);
       if (!DC || !DC->haveFeatures(STI.getFeatureBits()))
         return false;
@@ -974,10 +990,10 @@ bool AArch64InstPrinter::printSysAlias(const MCInst *MI,
       NeedsReg = true;
       Ins = "dc\t";
       Name = std::string(DC->Name);
-    }
-    break;
+    } break;
     // AT aliases
-    case 8: case 9: {
+    case 8:
+    case 9: {
       const AArch64AT::AT *AT = AArch64AT::lookupATByEncoding(Encoding);
       if (!AT || !AT->haveFeatures(STI.getFeatureBits()))
         return false;
@@ -985,8 +1001,7 @@ bool AArch64InstPrinter::printSysAlias(const MCInst *MI,
       NeedsReg = true;
       Ins = "at\t";
       Name = std::string(AT->Name);
-    }
-    break;
+    } break;
     // Overlaps with AT and DC
     case 15: {
       const AArch64AT::AT *AT = AArch64AT::lookupATByEncoding(Encoding);
@@ -1013,8 +1028,7 @@ bool AArch64InstPrinter::printSysAlias(const MCInst *MI,
     NeedsReg = TLBI->NeedsReg;
     Ins = "tlbi\t";
     Name = std::string(TLBI->Name);
-  }
-  else
+  } else
     return false;
 
   std::string Str = Ins + Name;
@@ -1168,8 +1182,7 @@ void AArch64InstPrinter::printOperand(const MCInst *MI, unsigned OpNo,
 }
 
 void AArch64InstPrinter::printImm(const MCInst *MI, unsigned OpNo,
-                                     const MCSubtargetInfo &STI,
-                                     raw_ostream &O) {
+                                  const MCSubtargetInfo &STI, raw_ostream &O) {
   const MCOperand &Op = MI->getOperand(OpNo);
   markup(O, Markup::Immediate) << "#" << formatImm(Op.getImm());
 }
@@ -1181,10 +1194,9 @@ void AArch64InstPrinter::printImmHex(const MCInst *MI, unsigned OpNo,
   markup(O, Markup::Immediate) << format("#%#llx", Op.getImm());
 }
 
-template<int Size>
+template <int Size>
 void AArch64InstPrinter::printSImm(const MCInst *MI, unsigned OpNo,
-                                  const MCSubtargetInfo &STI,
-                                  raw_ostream &O) {
+                                   const MCSubtargetInfo &STI, raw_ostream &O) {
   const MCOperand &Op = MI->getOperand(OpNo);
   if (Size == 8)
     markup(O, Markup::Immediate) << "#" << formatImm((signed char)Op.getImm());
@@ -1295,10 +1307,10 @@ void AArch64InstPrinter::printArithExtend(const MCInst *MI, unsigned OpNum,
   if (ExtType == AArch64_AM::UXTW || ExtType == AArch64_AM::UXTX) {
     MCRegister Dest = MI->getOperand(0).getReg();
     MCRegister Src1 = MI->getOperand(1).getReg();
-    if ( ((Dest == AArch64::SP || Src1 == AArch64::SP) &&
-          ExtType == AArch64_AM::UXTX) ||
-         ((Dest == AArch64::WSP || Src1 == AArch64::WSP) &&
-          ExtType == AArch64_AM::UXTW) ) {
+    if (((Dest == AArch64::SP || Src1 == AArch64::SP) &&
+         ExtType == AArch64_AM::UXTX) ||
+        ((Dest == AArch64::WSP || Src1 == AArch64::WSP) &&
+         ExtType == AArch64_AM::UXTW)) {
       if (ShiftVal != 0) {
         O << ", lsl ";
         markup(O, Markup::Immediate) << "#" << ShiftVal;
@@ -1522,103 +1534,258 @@ static MCRegister getNextVectorRegister(MCRegister Reg, unsigned Stride = 1) {
     switch (Reg.id()) {
     default:
       llvm_unreachable("Vector register expected!");
-    case AArch64::Q0:  Reg = AArch64::Q1;  break;
-    case AArch64::Q1:  Reg = AArch64::Q2;  break;
-    case AArch64::Q2:  Reg = AArch64::Q3;  break;
-    case AArch64::Q3:  Reg = AArch64::Q4;  break;
-    case AArch64::Q4:  Reg = AArch64::Q5;  break;
-    case AArch64::Q5:  Reg = AArch64::Q6;  break;
-    case AArch64::Q6:  Reg = AArch64::Q7;  break;
-    case AArch64::Q7:  Reg = AArch64::Q8;  break;
-    case AArch64::Q8:  Reg = AArch64::Q9;  break;
-    case AArch64::Q9:  Reg = AArch64::Q10; break;
-    case AArch64::Q10: Reg = AArch64::Q11; break;
-    case AArch64::Q11: Reg = AArch64::Q12; break;
-    case AArch64::Q12: Reg = AArch64::Q13; break;
-    case AArch64::Q13: Reg = AArch64::Q14; break;
-    case AArch64::Q14: Reg = AArch64::Q15; break;
-    case AArch64::Q15: Reg = AArch64::Q16; break;
-    case AArch64::Q16: Reg = AArch64::Q17; break;
-    case AArch64::Q17: Reg = AArch64::Q18; break;
-    case AArch64::Q18: Reg = AArch64::Q19; break;
-    case AArch64::Q19: Reg = AArch64::Q20; break;
-    case AArch64::Q20: Reg = AArch64::Q21; break;
-    case AArch64::Q21: Reg = AArch64::Q22; break;
-    case AArch64::Q22: Reg = AArch64::Q23; break;
-    case AArch64::Q23: Reg = AArch64::Q24; break;
-    case AArch64::Q24: Reg = AArch64::Q25; break;
-    case AArch64::Q25: Reg = AArch64::Q26; break;
-    case AArch64::Q26: Reg = AArch64::Q27; break;
-    case AArch64::Q27: Reg = AArch64::Q28; break;
-    case AArch64::Q28: Reg = AArch64::Q29; break;
-    case AArch64::Q29: Reg = AArch64::Q30; break;
-    case AArch64::Q30: Reg = AArch64::Q31; break;
+    case AArch64::Q0:
+      Reg = AArch64::Q1;
+      break;
+    case AArch64::Q1:
+      Reg = AArch64::Q2;
+      break;
+    case AArch64::Q2:
+      Reg = AArch64::Q3;
+      break;
+    case AArch64::Q3:
+      Reg = AArch64::Q4;
+      break;
+    case AArch64::Q4:
+      Reg = AArch64::Q5;
+      break;
+    case AArch64::Q5:
+      Reg = AArch64::Q6;
+      break;
+    case AArch64::Q6:
+      Reg = AArch64::Q7;
+      break;
+    case AArch64::Q7:
+      Reg = AArch64::Q8;
+      break;
+    case AArch64::Q8:
+      Reg = AArch64::Q9;
+      break;
+    case AArch64::Q9:
+      Reg = AArch64::Q10;
+      break;
+    case AArch64::Q10:
+      Reg = AArch64::Q11;
+      break;
+    case AArch64::Q11:
+      Reg = AArch64::Q12;
+      break;
+    case AArch64::Q12:
+      Reg = AArch64::Q13;
+      break;
+    case AArch64::Q13:
+      Reg = AArch64::Q14;
+      break;
+    case AArch64::Q14:
+      Reg = AArch64::Q15;
+      break;
+    case AArch64::Q15:
+      Reg = AArch64::Q16;
+      break;
+    case AArch64::Q16:
+      Reg = AArch64::Q17;
+      break;
+    case AArch64::Q17:
+      Reg = AArch64::Q18;
+      break;
+    case AArch64::Q18:
+      Reg = AArch64::Q19;
+      break;
+    case AArch64::Q19:
+      Reg = AArch64::Q20;
+      break;
+    case AArch64::Q20:
+      Reg = AArch64::Q21;
+      break;
+    case AArch64::Q21:
+      Reg = AArch64::Q22;
+      break;
+    case AArch64::Q22:
+      Reg = AArch64::Q23;
+      break;
+    case AArch64::Q23:
+      Reg = AArch64::Q24;
+      break;
+    case AArch64::Q24:
+      Reg = AArch64::Q25;
+      break;
+    case AArch64::Q25:
+      Reg = AArch64::Q26;
+      break;
+    case AArch64::Q26:
+      Reg = AArch64::Q27;
+      break;
+    case AArch64::Q27:
+      Reg = AArch64::Q28;
+      break;
+    case AArch64::Q28:
+      Reg = AArch64::Q29;
+      break;
+    case AArch64::Q29:
+      Reg = AArch64::Q30;
+      break;
+    case AArch64::Q30:
+      Reg = AArch64::Q31;
+      break;
     // Vector lists can wrap around.
     case AArch64::Q31:
       Reg = AArch64::Q0;
       break;
-    case AArch64::Z0:  Reg = AArch64::Z1;  break;
-    case AArch64::Z1:  Reg = AArch64::Z2;  break;
-    case AArch64::Z2:  Reg = AArch64::Z3;  break;
-    case AArch64::Z3:  Reg = AArch64::Z4;  break;
-    case AArch64::Z4:  Reg = AArch64::Z5;  break;
-    case AArch64::Z5:  Reg = AArch64::Z6;  break;
-    case AArch64::Z6:  Reg = AArch64::Z7;  break;
-    case AArch64::Z7:  Reg = AArch64::Z8;  break;
-    case AArch64::Z8:  Reg = AArch64::Z9;  break;
-    case AArch64::Z9:  Reg = AArch64::Z10; break;
-    case AArch64::Z10: Reg = AArch64::Z11; break;
-    case AArch64::Z11: Reg = AArch64::Z12; break;
-    case AArch64::Z12: Reg = AArch64::Z13; break;
-    case AArch64::Z13: Reg = AArch64::Z14; break;
-    case AArch64::Z14: Reg = AArch64::Z15; break;
-    case AArch64::Z15: Reg = AArch64::Z16; break;
-    case AArch64::Z16: Reg = AArch64::Z17; break;
-    case AArch64::Z17: Reg = AArch64::Z18; break;
-    case AArch64::Z18: Reg = AArch64::Z19; break;
-    case AArch64::Z19: Reg = AArch64::Z20; break;
-    case AArch64::Z20: Reg = AArch64::Z21; break;
-    case AArch64::Z21: Reg = AArch64::Z22; break;
-    case AArch64::Z22: Reg = AArch64::Z23; break;
-    case AArch64::Z23: Reg = AArch64::Z24; break;
-    case AArch64::Z24: Reg = AArch64::Z25; break;
-    case AArch64::Z25: Reg = AArch64::Z26; break;
-    case AArch64::Z26: Reg = AArch64::Z27; break;
-    case AArch64::Z27: Reg = AArch64::Z28; break;
-    case AArch64::Z28: Reg = AArch64::Z29; break;
-    case AArch64::Z29: Reg = AArch64::Z30; break;
-    case AArch64::Z30: Reg = AArch64::Z31; break;
+    case AArch64::Z0:
+      Reg = AArch64::Z1;
+      break;
+    case AArch64::Z1:
+      Reg = AArch64::Z2;
+      break;
+    case AArch64::Z2:
+      Reg = AArch64::Z3;
+      break;
+    case AArch64::Z3:
+      Reg = AArch64::Z4;
+      break;
+    case AArch64::Z4:
+      Reg = AArch64::Z5;
+      break;
+    case AArch64::Z5:
+      Reg = AArch64::Z6;
+      break;
+    case AArch64::Z6:
+      Reg = AArch64::Z7;
+      break;
+    case AArch64::Z7:
+      Reg = AArch64::Z8;
+      break;
+    case AArch64::Z8:
+      Reg = AArch64::Z9;
+      break;
+    case AArch64::Z9:
+      Reg = AArch64::Z10;
+      break;
+    case AArch64::Z10:
+      Reg = AArch64::Z11;
+      break;
+    case AArch64::Z11:
+      Reg = AArch64::Z12;
+      break;
+    case AArch64::Z12:
+      Reg = AArch64::Z13;
+      break;
+    case AArch64::Z13:
+      Reg = AArch64::Z14;
+      break;
+    case AArch64::Z14:
+      Reg = AArch64::Z15;
+      break;
+    case AArch64::Z15:
+      Reg = AArch64::Z16;
+      break;
+    case AArch64::Z16:
+      Reg = AArch64::Z17;
+      break;
+    case AArch64::Z17:
+      Reg = AArch64::Z18;
+      break;
+    case AArch64::Z18:
+      Reg = AArch64::Z19;
+      break;
+    case AArch64::Z19:
+      Reg = AArch64::Z20;
+      break;
+    case AArch64::Z20:
+      Reg = AArch64::Z21;
+      break;
+    case AArch64::Z21:
+      Reg = AArch64::Z22;
+      break;
+    case AArch64::Z22:
+      Reg = AArch64::Z23;
+      break;
+    case AArch64::Z23:
+      Reg = AArch64::Z24;
+      break;
+    case AArch64::Z24:
+      Reg = AArch64::Z25;
+      break;
+    case AArch64::Z25:
+      Reg = AArch64::Z26;
+      break;
+    case AArch64::Z26:
+      Reg = AArch64::Z27;
+      break;
+    case AArch64::Z27:
+      Reg = AArch64::Z28;
+      break;
+    case AArch64::Z28:
+      Reg = AArch64::Z29;
+      break;
+    case AArch64::Z29:
+      Reg = AArch64::Z30;
+      break;
+    case AArch64::Z30:
+      Reg = AArch64::Z31;
+      break;
     // Vector lists can wrap around.
     case AArch64::Z31:
       Reg = AArch64::Z0;
       break;
-    case AArch64::P0:  Reg = AArch64::P1;  break;
-    case AArch64::P1:  Reg = AArch64::P2;  break;
-    case AArch64::P2:  Reg = AArch64::P3;  break;
-    case AArch64::P3:  Reg = AArch64::P4;  break;
-    case AArch64::P4:  Reg = AArch64::P5;  break;
-    case AArch64::P5:  Reg = AArch64::P6;  break;
-    case AArch64::P6:  Reg = AArch64::P7;  break;
-    case AArch64::P7:  Reg = AArch64::P8;  break;
-    case AArch64::P8:  Reg = AArch64::P9;  break;
-    case AArch64::P9:  Reg = AArch64::P10; break;
-    case AArch64::P10: Reg = AArch64::P11; break;
-    case AArch64::P11: Reg = AArch64::P12; break;
-    case AArch64::P12: Reg = AArch64::P13; break;
-    case AArch64::P13: Reg = AArch64::P14; break;
-    case AArch64::P14: Reg = AArch64::P15; break;
+    case AArch64::P0:
+      Reg = AArch64::P1;
+      break;
+    case AArch64::P1:
+      Reg = AArch64::P2;
+      break;
+    case AArch64::P2:
+      Reg = AArch64::P3;
+      break;
+    case AArch64::P3:
+      Reg = AArch64::P4;
+      break;
+    case AArch64::P4:
+      Reg = AArch64::P5;
+      break;
+    case AArch64::P5:
+      Reg = AArch64::P6;
+      break;
+    case AArch64::P6:
+      Reg = AArch64::P7;
+      break;
+    case AArch64::P7:
+      Reg = AArch64::P8;
+      break;
+    case AArch64::P8:
+      Reg = AArch64::P9;
+      break;
+    case AArch64::P9:
+      Reg = AArch64::P10;
+      break;
+    case AArch64::P10:
+      Reg = AArch64::P11;
+      break;
+    case AArch64::P11:
+      Reg = AArch64::P12;
+      break;
+    case AArch64::P12:
+      Reg = AArch64::P13;
+      break;
+    case AArch64::P13:
+      Reg = AArch64::P14;
+      break;
+    case AArch64::P14:
+      Reg = AArch64::P15;
+      break;
     // Vector lists can wrap around.
-    case AArch64::P15: Reg = AArch64::P0; break;
+    case AArch64::P15:
+      Reg = AArch64::P0;
+      break;
     }
   }
   return Reg;
 }
 
-template<unsigned size>
-void AArch64InstPrinter::printGPRSeqPairsClassOperand(const MCInst *MI,
-                                                   unsigned OpNum,
-                                                   const MCSubtargetInfo &STI,
-                                                   raw_ostream &O) {
+template <unsigned size>
+void AArch64InstPrinter::printGPRSeqPairsClassOperand(
+    const MCInst *MI, unsigned OpNum, const MCSubtargetInfo &STI,
+    raw_ostream &O) {
   static_assert(size == 64 || size == 32,
                 "Template parameter must be either 32 or 64");
   MCRegister Reg = MI->getOperand(OpNum).getReg();
@@ -1741,11 +1908,9 @@ void AArch64InstPrinter::printVectorList(const MCInst *MI, unsigned OpNum,
   O << " }";
 }
 
-void
-AArch64InstPrinter::printImplicitlyTypedVectorList(const MCInst *MI,
-                                                   unsigned OpNum,
-                                                   const MCSubtargetInfo &STI,
-                                                   raw_ostream &O) {
+void AArch64InstPrinter::printImplicitlyTypedVectorList(
+    const MCInst *MI, unsigned OpNum, const MCSubtargetInfo &STI,
+    raw_ostream &O) {
   printVectorList(MI, OpNum, STI, O, "");
 }
 
@@ -1972,7 +2137,7 @@ void AArch64InstPrinter::printSIMDType10Operand(const MCInst *MI, unsigned OpNo,
   markup(O, Markup::Immediate) << format("#%#016llx", Val);
 }
 
-template<int64_t Angle, int64_t Remainder>
+template <int64_t Angle, int64_t Remainder>
 void AArch64InstPrinter::printComplexRotationOp(const MCInst *MI, unsigned OpNo,
                                                 const MCSubtargetInfo &STI,
                                                 raw_ostream &O) {
@@ -2017,7 +2182,8 @@ void AArch64InstPrinter::printSVERegOp(const MCInst *MI, unsigned OpNum,
   case 'd':
   case 'q':
     break;
-  default: llvm_unreachable("Invalid kind specifier.");
+  default:
+    llvm_unreachable("Invalid kind specifier.");
   }
 
   MCRegister Reg = MI->getOperand(OpNum).getReg();
@@ -2094,11 +2260,21 @@ void AArch64InstPrinter::printZPRasFPR(const MCInst *MI, unsigned OpNum,
                                        raw_ostream &O) {
   unsigned Base;
   switch (Width) {
-  case 8:   Base = AArch64::B0; break;
-  case 16:  Base = AArch64::H0; break;
-  case 32:  Base = AArch64::S0; break;
-  case 64:  Base = AArch64::D0; break;
-  case 128: Base = AArch64::Q0; break;
+  case 8:
+    Base = AArch64::B0;
+    break;
+  case 16:
+    Base = AArch64::H0;
+    break;
+  case 32:
+    Base = AArch64::S0;
+    break;
+  case 64:
+    Base = AArch64::D0;
+    break;
+  case 128:
+    Base = AArch64::Q0;
+    break;
   default:
     llvm_unreachable("Unsupported width");
   }
@@ -2109,7 +2285,7 @@ void AArch64InstPrinter::printZPRasFPR(const MCInst *MI, unsigned OpNum,
 template <unsigned ImmIs0, unsigned ImmIs1>
 void AArch64InstPrinter::printExactFPImm(const MCInst *MI, unsigned OpNum,
                                          const MCSubtargetInfo &STI,
-                                         raw_ostream  &O) {
+                                         raw_ostream &O) {
   auto *Imm0Desc = AArch64ExactFPImm::lookupExactFPImmByEnum(ImmIs0);
   auto *Imm1Desc = AArch64ExactFPImm::lookupExactFPImmByEnum(ImmIs1);
   unsigned Val = MI->getOperand(OpNum).getImm();
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.h b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.h
index 15ef2ddfc..5388d05de 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.h
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64InstPrinter.h
@@ -13,10 +13,10 @@
 #ifndef LLVM_LIB_TARGET_AARCH64_MCTARGETDESC_AARCH64INSTPRINTER_H
 #define LLVM_LIB_TARGET_AARCH64_MCTARGETDESC_AARCH64INSTPRINTER_H
 
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
+#include "../Utils/AArch64BaseInfo.h"
+#include "AArch64MCTargetDesc.h"
 #include "llvm/ADT/StringRef.h"
 #include "llvm/MC/MCInstPrinter.h"
-#include "../Utils/AArch64BaseInfo.h"
 
 namespace llvm {
 
@@ -194,13 +194,12 @@ protected:
   template <int EltSize>
   void printPredicateAsCounter(const MCInst *MI, unsigned OpNum,
                                const MCSubtargetInfo &STI, raw_ostream &O);
-  template<int64_t Angle, int64_t Remainder>
+  template <int64_t Angle, int64_t Remainder>
   void printComplexRotationOp(const MCInst *MI, unsigned OpNo,
-                            const MCSubtargetInfo &STI, raw_ostream &O);
-  template<unsigned size>
+                              const MCSubtargetInfo &STI, raw_ostream &O);
+  template <unsigned size>
   void printGPRSeqPairsClassOperand(const MCInst *MI, unsigned OpNum,
-                                    const MCSubtargetInfo &STI,
-                                    raw_ostream &O);
+                                    const MCSubtargetInfo &STI, raw_ostream &O);
   template <typename T>
   void printImm8OptLsl(const MCInst *MI, unsigned OpNum,
                        const MCSubtargetInfo &STI, raw_ostream &O);
@@ -224,7 +223,7 @@ protected:
                    raw_ostream &O);
   template <char = 0>
   void printSVERegOp(const MCInst *MI, unsigned OpNum,
-                    const MCSubtargetInfo &STI, raw_ostream &O);
+                     const MCSubtargetInfo &STI, raw_ostream &O);
   void printGPR64as32(const MCInst *MI, unsigned OpNum,
                       const MCSubtargetInfo &STI, raw_ostream &O);
   void printGPR64x8(const MCInst *MI, unsigned OpNum,
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
index 552477ebc..871472673 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
@@ -10,9 +10,9 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/AArch64AddressingModes.h"
-#include "MCTargetDesc/AArch64FixupKinds.h"
-#include "MCTargetDesc/AArch64MCExpr.h"
+#include "AArch64AddressingModes.h"
+#include "AArch64FixupKinds.h"
+#include "AArch64MCExpr.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Statistic.h"
 #include "llvm/BinaryFormat/ELF.h"
@@ -187,9 +187,9 @@ public:
   unsigned fixMulHigh(const MCInst &MI, unsigned EncodedValue,
                       const MCSubtargetInfo &STI) const;
 
-  template<int hasRs, int hasRt2> unsigned
-  fixLoadStoreExclusive(const MCInst &MI, unsigned EncodedValue,
-                        const MCSubtargetInfo &STI) const;
+  template <int hasRs, int hasRt2>
+  unsigned fixLoadStoreExclusive(const MCInst &MI, unsigned EncodedValue,
+                                 const MCSubtargetInfo &STI) const;
 
   unsigned fixOneOperandFPComparison(const MCInst &MI, unsigned EncodedValue,
                                      const MCSubtargetInfo &STI) const;
@@ -236,7 +236,8 @@ AArch64MCCodeEmitter::getMachineOpValue(const MCInst &MI, const MCOperand &MO,
   return static_cast<unsigned>(MO.getImm());
 }
 
-template<unsigned FixupKind> uint32_t
+template <unsigned FixupKind>
+uint32_t
 AArch64MCCodeEmitter::getLdStUImm12OpValue(const MCInst &MI, unsigned OpIdx,
                                            SmallVectorImpl<MCFixup> &Fixups,
                                            const MCSubtargetInfo &STI) const {
@@ -664,10 +665,9 @@ AArch64MCCodeEmitter::encodeMatrixIndexGPR32(const MCInst &MI, unsigned OpIdx,
   return RegOpnd - BaseReg;
 }
 
-uint32_t
-AArch64MCCodeEmitter::getImm8OptLsl(const MCInst &MI, unsigned OpIdx,
-                                    SmallVectorImpl<MCFixup> &Fixups,
-                                    const MCSubtargetInfo &STI) const {
+uint32_t AArch64MCCodeEmitter::getImm8OptLsl(const MCInst &MI, unsigned OpIdx,
+                                             SmallVectorImpl<MCFixup> &Fixups,
+                                             const MCSubtargetInfo &STI) const {
   // Test shift
   auto ShiftOpnd = MI.getOperand(OpIdx + 1).getImm();
   assert(AArch64_AM::getShiftType(ShiftOpnd) == AArch64_AM::LSL &&
@@ -684,8 +684,8 @@ AArch64MCCodeEmitter::getImm8OptLsl(const MCInst &MI, unsigned OpIdx,
 
 uint32_t
 AArch64MCCodeEmitter::getSVEIncDecImm(const MCInst &MI, unsigned OpIdx,
-                                           SmallVectorImpl<MCFixup> &Fixups,
-                                           const MCSubtargetInfo &STI) const {
+                                      SmallVectorImpl<MCFixup> &Fixups,
+                                      const MCSubtargetInfo &STI) const {
   const MCOperand &MO = MI.getOperand(OpIdx);
   assert(MO.isImm() && "Expected an immediate value!");
   // Normalize 1-16 range to 0-15.
@@ -765,22 +765,22 @@ void AArch64MCCodeEmitter::encodeInstruction(const MCInst &MI,
   ++MCNumEmitted; // Keep track of the # of mi's emitted.
 }
 
-unsigned
-AArch64MCCodeEmitter::fixMulHigh(const MCInst &MI,
-                                 unsigned EncodedValue,
-                                 const MCSubtargetInfo &STI) const {
+unsigned AArch64MCCodeEmitter::fixMulHigh(const MCInst &MI,
+                                          unsigned EncodedValue,
+                                          const MCSubtargetInfo &STI) const {
   // The Ra field of SMULH and UMULH is unused: it should be assembled as 31
   // (i.e. all bits 1) but is ignored by the processor.
   EncodedValue |= 0x1f << 10;
   return EncodedValue;
 }
 
-template<int hasRs, int hasRt2> unsigned
-AArch64MCCodeEmitter::fixLoadStoreExclusive(const MCInst &MI,
-                                            unsigned EncodedValue,
-                                            const MCSubtargetInfo &STI) const {
-  if (!hasRs) EncodedValue |= 0x001F0000;
-  if (!hasRt2) EncodedValue |= 0x00007C00;
+template <int hasRs, int hasRt2>
+unsigned AArch64MCCodeEmitter::fixLoadStoreExclusive(
+    const MCInst &MI, unsigned EncodedValue, const MCSubtargetInfo &STI) const {
+  if (!hasRs)
+    EncodedValue |= 0x001F0000;
+  if (!hasRt2)
+    EncodedValue |= 0x00007C00;
 
   return EncodedValue;
 }
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h
index 3f9a85d63..40ef06ff3 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCExpr.h
@@ -14,7 +14,7 @@
 #ifndef LLVM_LIB_TARGET_AARCH64_MCTARGETDESC_AARCH64MCEXPR_H
 #define LLVM_LIB_TARGET_AARCH64_MCTARGETDESC_AARCH64MCEXPR_H
 
-#include "Utils/AArch64BaseInfo.h"
+#include "../Utils/AArch64BaseInfo.h"
 #include "llvm/MC/MCExpr.h"
 #include "llvm/Support/Casting.h"
 #include "llvm/Support/ErrorHandling.h"
@@ -130,14 +130,14 @@ private:
 
 protected:
   explicit AArch64MCExpr(const MCExpr *Expr, VariantKind Kind)
-    : Expr(Expr), Kind(Kind) {}
+      : Expr(Expr), Kind(Kind) {}
 
 public:
   /// @name Construction
   /// @{
 
   static const AArch64MCExpr *create(const MCExpr *Expr, VariantKind Kind,
-                                   MCContext &Ctx);
+                                     MCContext &Ctx);
 
   /// @}
   /// @name Accessors
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCTargetDesc.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCTargetDesc.cpp
index c7f44ec01..250743605 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCTargetDesc.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCTargetDesc.cpp
@@ -11,12 +11,12 @@
 //===----------------------------------------------------------------------===//
 
 #include "AArch64MCTargetDesc.h"
+#include "../TargetInfo/AArch64TargetInfo.h"
+#include "AArch64AddressingModes.h"
 #include "AArch64ELFStreamer.h"
+#include "AArch64InstPrinter.h"
 #include "AArch64MCAsmInfo.h"
 #include "AArch64WinCOFFStreamer.h"
-#include "MCTargetDesc/AArch64AddressingModes.h"
-#include "MCTargetDesc/AArch64InstPrinter.h"
-#include "TargetInfo/AArch64TargetInfo.h"
 #include "llvm/DebugInfo/CodeView/CodeView.h"
 #include "llvm/MC/MCAsmBackend.h"
 #include "llvm/MC/MCCodeEmitter.h"
@@ -490,15 +490,16 @@ public:
       uint64_t Off = 0;
       // Check for optional bti c that prefixes adrp in BTI enabled entries
       if (Insn == 0xd503245f) {
-         Off = 4;
-         Insn = support::endian::read32le(PltContents.data() + Byte + Off);
+        Off = 4;
+        Insn = support::endian::read32le(PltContents.data() + Byte + Off);
       }
       // Check for adrp.
       if ((Insn & 0x9f000000) != 0x90000000)
         continue;
       Off += 4;
       uint64_t Imm = (((PltSectionVA + Byte) >> 12) << 12) +
-            (((Insn >> 29) & 3) << 12) + (((Insn >> 5) & 0x3ffff) << 14);
+                     (((Insn >> 29) & 3) << 12) +
+                     (((Insn >> 5) & 0x3ffff) << 14);
       uint32_t Insn2 =
           support::endian::read32le(PltContents.data() + Byte + Off);
       // Check for: ldr Xt, [Xn, #pimm].
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp
index ed0a972cc..0371fa4cb 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MachObjectWriter.cpp
@@ -6,9 +6,9 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/AArch64FixupKinds.h"
-#include "MCTargetDesc/AArch64MCExpr.h"
-#include "MCTargetDesc/AArch64MCTargetDesc.h"
+#include "AArch64FixupKinds.h"
+#include "AArch64MCExpr.h"
+#include "AArch64MCTargetDesc.h"
 #include "llvm/ADT/Twine.h"
 #include "llvm/BinaryFormat/MachO.h"
 #include "llvm/MC/MCAsmInfo.h"
@@ -34,8 +34,8 @@ namespace {
 
 class AArch64MachObjectWriter : public MCMachObjectTargetWriter {
   bool getAArch64FixupKindMachOInfo(const MCFixup &Fixup, unsigned &RelocType,
-                                  const MCSymbolRefExpr *Sym,
-                                  unsigned &Log2Size, const MCAssembler &Asm);
+                                    const MCSymbolRefExpr *Sym,
+                                    unsigned &Log2Size, const MCAssembler &Asm);
 
 public:
   AArch64MachObjectWriter(uint32_t CPUType, uint32_t CPUSubtype, bool IsILP32)
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64WinCOFFObjectWriter.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64WinCOFFObjectWriter.cpp
index 47228025c..92df2e8f1 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64WinCOFFObjectWriter.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64WinCOFFObjectWriter.cpp
@@ -6,9 +6,9 @@
 //
 //===---------------------------------------------------------------------===//
 
+#include "AArch64FixupKinds.h"
+#include "AArch64MCExpr.h"
 #include "AArch64MCTargetDesc.h"
-#include "MCTargetDesc/AArch64FixupKinds.h"
-#include "MCTargetDesc/AArch64MCExpr.h"
 #include "llvm/ADT/Twine.h"
 #include "llvm/BinaryFormat/COFF.h"
 #include "llvm/MC/MCAsmBackend.h"
diff --git a/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp b/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp
index 52c88fd02..048c68f9c 100644
--- a/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp
+++ b/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp
@@ -1,4 +1,5 @@
-//===-- AArch64TargetInfo.cpp - AArch64 Target Implementation -----------------===//
+//===-- AArch64TargetInfo.cpp - AArch64 Target Implementation
+//-----------------===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
@@ -6,7 +7,7 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "TargetInfo/AArch64TargetInfo.h"
+#include "AArch64TargetInfo.h"
 #include "llvm/MC/TargetRegistry.h"
 
 using namespace llvm;
@@ -34,17 +35,18 @@ Target &llvm::getTheARM64_32Target() {
 extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeAArch64TargetInfo() {
   // Now register the "arm64" name for use with "-march". We don't want it to
   // take possession of the Triple::aarch64 tags though.
-  TargetRegistry::RegisterTarget(getTheARM64Target(), "arm64",
-                                 "ARM64 (little endian)", "AArch64",
-                                 [](Triple::ArchType) { return false; }, true);
-  TargetRegistry::RegisterTarget(getTheARM64_32Target(), "arm64_32",
-                                 "ARM64 (little endian ILP32)", "AArch64",
-                                 [](Triple::ArchType) { return false; }, true);
+  TargetRegistry::RegisterTarget(
+      getTheARM64Target(), "arm64", "ARM64 (little endian)", "AArch64",
+      [](Triple::ArchType) { return false; }, true);
+  TargetRegistry::RegisterTarget(
+      getTheARM64_32Target(), "arm64_32", "ARM64 (little endian ILP32)",
+      "AArch64", [](Triple::ArchType) { return false; }, true);
 
   RegisterTarget<Triple::aarch64, /*HasJIT=*/true> Z(
       getTheAArch64leTarget(), "aarch64", "AArch64 (little endian)", "AArch64");
   RegisterTarget<Triple::aarch64_be, /*HasJIT=*/true> W(
       getTheAArch64beTarget(), "aarch64_be", "AArch64 (big endian)", "AArch64");
   RegisterTarget<Triple::aarch64_32, /*HasJIT=*/true> X(
-      getTheAArch64_32Target(), "aarch64_32", "AArch64 (little endian ILP32)", "AArch64");
+      getTheAArch64_32Target(), "aarch64_32", "AArch64 (little endian ILP32)",
+      "AArch64");
 }
diff --git a/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h b/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
index 9671fa3b3..883556d4c 100644
--- a/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
+++ b/llvm/lib/Target/AArch64/Utils/AArch64BaseInfo.h
@@ -18,7 +18,7 @@
 
 // FIXME: Is it easiest to fix this layering violation by moving the .inc
 // #includes from AArch64MCTargetDesc.h to here?
-#include "MCTargetDesc/AArch64MCTargetDesc.h" // For AArch64::X0 and friends.
+#include "../MCTargetDesc/AArch64MCTargetDesc.h" // For AArch64::X0 and friends.
 #include "llvm/ADT/BitmaskEnum.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/StringSwitch.h"
@@ -29,39 +29,72 @@ namespace llvm {
 
 inline static MCRegister getWRegFromXReg(MCRegister Reg) {
   switch (Reg.id()) {
-  case AArch64::X0: return AArch64::W0;
-  case AArch64::X1: return AArch64::W1;
-  case AArch64::X2: return AArch64::W2;
-  case AArch64::X3: return AArch64::W3;
-  case AArch64::X4: return AArch64::W4;
-  case AArch64::X5: return AArch64::W5;
-  case AArch64::X6: return AArch64::W6;
-  case AArch64::X7: return AArch64::W7;
-  case AArch64::X8: return AArch64::W8;
-  case AArch64::X9: return AArch64::W9;
-  case AArch64::X10: return AArch64::W10;
-  case AArch64::X11: return AArch64::W11;
-  case AArch64::X12: return AArch64::W12;
-  case AArch64::X13: return AArch64::W13;
-  case AArch64::X14: return AArch64::W14;
-  case AArch64::X15: return AArch64::W15;
-  case AArch64::X16: return AArch64::W16;
-  case AArch64::X17: return AArch64::W17;
-  case AArch64::X18: return AArch64::W18;
-  case AArch64::X19: return AArch64::W19;
-  case AArch64::X20: return AArch64::W20;
-  case AArch64::X21: return AArch64::W21;
-  case AArch64::X22: return AArch64::W22;
-  case AArch64::X23: return AArch64::W23;
-  case AArch64::X24: return AArch64::W24;
-  case AArch64::X25: return AArch64::W25;
-  case AArch64::X26: return AArch64::W26;
-  case AArch64::X27: return AArch64::W27;
-  case AArch64::X28: return AArch64::W28;
-  case AArch64::FP: return AArch64::W29;
-  case AArch64::LR: return AArch64::W30;
-  case AArch64::SP: return AArch64::WSP;
-  case AArch64::XZR: return AArch64::WZR;
+  case AArch64::X0:
+    return AArch64::W0;
+  case AArch64::X1:
+    return AArch64::W1;
+  case AArch64::X2:
+    return AArch64::W2;
+  case AArch64::X3:
+    return AArch64::W3;
+  case AArch64::X4:
+    return AArch64::W4;
+  case AArch64::X5:
+    return AArch64::W5;
+  case AArch64::X6:
+    return AArch64::W6;
+  case AArch64::X7:
+    return AArch64::W7;
+  case AArch64::X8:
+    return AArch64::W8;
+  case AArch64::X9:
+    return AArch64::W9;
+  case AArch64::X10:
+    return AArch64::W10;
+  case AArch64::X11:
+    return AArch64::W11;
+  case AArch64::X12:
+    return AArch64::W12;
+  case AArch64::X13:
+    return AArch64::W13;
+  case AArch64::X14:
+    return AArch64::W14;
+  case AArch64::X15:
+    return AArch64::W15;
+  case AArch64::X16:
+    return AArch64::W16;
+  case AArch64::X17:
+    return AArch64::W17;
+  case AArch64::X18:
+    return AArch64::W18;
+  case AArch64::X19:
+    return AArch64::W19;
+  case AArch64::X20:
+    return AArch64::W20;
+  case AArch64::X21:
+    return AArch64::W21;
+  case AArch64::X22:
+    return AArch64::W22;
+  case AArch64::X23:
+    return AArch64::W23;
+  case AArch64::X24:
+    return AArch64::W24;
+  case AArch64::X25:
+    return AArch64::W25;
+  case AArch64::X26:
+    return AArch64::W26;
+  case AArch64::X27:
+    return AArch64::W27;
+  case AArch64::X28:
+    return AArch64::W28;
+  case AArch64::FP:
+    return AArch64::W29;
+  case AArch64::LR:
+    return AArch64::W30;
+  case AArch64::SP:
+    return AArch64::WSP;
+  case AArch64::XZR:
+    return AArch64::WZR;
   }
   // For anything else, return it unchanged.
   return Reg;
@@ -69,39 +102,72 @@ inline static MCRegister getWRegFromXReg(MCRegister Reg) {
 
 inline static MCRegister getXRegFromWReg(MCRegister Reg) {
   switch (Reg.id()) {
-  case AArch64::W0: return AArch64::X0;
-  case AArch64::W1: return AArch64::X1;
-  case AArch64::W2: return AArch64::X2;
-  case AArch64::W3: return AArch64::X3;
-  case AArch64::W4: return AArch64::X4;
-  case AArch64::W5: return AArch64::X5;
-  case AArch64::W6: return AArch64::X6;
-  case AArch64::W7: return AArch64::X7;
-  case AArch64::W8: return AArch64::X8;
-  case AArch64::W9: return AArch64::X9;
-  case AArch64::W10: return AArch64::X10;
-  case AArch64::W11: return AArch64::X11;
-  case AArch64::W12: return AArch64::X12;
-  case AArch64::W13: return AArch64::X13;
-  case AArch64::W14: return AArch64::X14;
-  case AArch64::W15: return AArch64::X15;
-  case AArch64::W16: return AArch64::X16;
-  case AArch64::W17: return AArch64::X17;
-  case AArch64::W18: return AArch64::X18;
-  case AArch64::W19: return AArch64::X19;
-  case AArch64::W20: return AArch64::X20;
-  case AArch64::W21: return AArch64::X21;
-  case AArch64::W22: return AArch64::X22;
-  case AArch64::W23: return AArch64::X23;
-  case AArch64::W24: return AArch64::X24;
-  case AArch64::W25: return AArch64::X25;
-  case AArch64::W26: return AArch64::X26;
-  case AArch64::W27: return AArch64::X27;
-  case AArch64::W28: return AArch64::X28;
-  case AArch64::W29: return AArch64::FP;
-  case AArch64::W30: return AArch64::LR;
-  case AArch64::WSP: return AArch64::SP;
-  case AArch64::WZR: return AArch64::XZR;
+  case AArch64::W0:
+    return AArch64::X0;
+  case AArch64::W1:
+    return AArch64::X1;
+  case AArch64::W2:
+    return AArch64::X2;
+  case AArch64::W3:
+    return AArch64::X3;
+  case AArch64::W4:
+    return AArch64::X4;
+  case AArch64::W5:
+    return AArch64::X5;
+  case AArch64::W6:
+    return AArch64::X6;
+  case AArch64::W7:
+    return AArch64::X7;
+  case AArch64::W8:
+    return AArch64::X8;
+  case AArch64::W9:
+    return AArch64::X9;
+  case AArch64::W10:
+    return AArch64::X10;
+  case AArch64::W11:
+    return AArch64::X11;
+  case AArch64::W12:
+    return AArch64::X12;
+  case AArch64::W13:
+    return AArch64::X13;
+  case AArch64::W14:
+    return AArch64::X14;
+  case AArch64::W15:
+    return AArch64::X15;
+  case AArch64::W16:
+    return AArch64::X16;
+  case AArch64::W17:
+    return AArch64::X17;
+  case AArch64::W18:
+    return AArch64::X18;
+  case AArch64::W19:
+    return AArch64::X19;
+  case AArch64::W20:
+    return AArch64::X20;
+  case AArch64::W21:
+    return AArch64::X21;
+  case AArch64::W22:
+    return AArch64::X22;
+  case AArch64::W23:
+    return AArch64::X23;
+  case AArch64::W24:
+    return AArch64::X24;
+  case AArch64::W25:
+    return AArch64::X25;
+  case AArch64::W26:
+    return AArch64::X26;
+  case AArch64::W27:
+    return AArch64::X27;
+  case AArch64::W28:
+    return AArch64::X28;
+  case AArch64::W29:
+    return AArch64::FP;
+  case AArch64::W30:
+    return AArch64::LR;
+  case AArch64::WSP:
+    return AArch64::SP;
+  case AArch64::WZR:
+    return AArch64::XZR;
   }
   // For anything else, return it unchanged.
   return Reg;
@@ -109,18 +175,30 @@ inline static MCRegister getXRegFromWReg(MCRegister Reg) {
 
 inline static MCRegister getXRegFromXRegTuple(MCRegister RegTuple) {
   switch (RegTuple.id()) {
-  case AArch64::X0_X1_X2_X3_X4_X5_X6_X7: return AArch64::X0;
-  case AArch64::X2_X3_X4_X5_X6_X7_X8_X9: return AArch64::X2;
-  case AArch64::X4_X5_X6_X7_X8_X9_X10_X11: return AArch64::X4;
-  case AArch64::X6_X7_X8_X9_X10_X11_X12_X13: return AArch64::X6;
-  case AArch64::X8_X9_X10_X11_X12_X13_X14_X15: return AArch64::X8;
-  case AArch64::X10_X11_X12_X13_X14_X15_X16_X17: return AArch64::X10;
-  case AArch64::X12_X13_X14_X15_X16_X17_X18_X19: return AArch64::X12;
-  case AArch64::X14_X15_X16_X17_X18_X19_X20_X21: return AArch64::X14;
-  case AArch64::X16_X17_X18_X19_X20_X21_X22_X23: return AArch64::X16;
-  case AArch64::X18_X19_X20_X21_X22_X23_X24_X25: return AArch64::X18;
-  case AArch64::X20_X21_X22_X23_X24_X25_X26_X27: return AArch64::X20;
-  case AArch64::X22_X23_X24_X25_X26_X27_X28_FP: return AArch64::X22;
+  case AArch64::X0_X1_X2_X3_X4_X5_X6_X7:
+    return AArch64::X0;
+  case AArch64::X2_X3_X4_X5_X6_X7_X8_X9:
+    return AArch64::X2;
+  case AArch64::X4_X5_X6_X7_X8_X9_X10_X11:
+    return AArch64::X4;
+  case AArch64::X6_X7_X8_X9_X10_X11_X12_X13:
+    return AArch64::X6;
+  case AArch64::X8_X9_X10_X11_X12_X13_X14_X15:
+    return AArch64::X8;
+  case AArch64::X10_X11_X12_X13_X14_X15_X16_X17:
+    return AArch64::X10;
+  case AArch64::X12_X13_X14_X15_X16_X17_X18_X19:
+    return AArch64::X12;
+  case AArch64::X14_X15_X16_X17_X18_X19_X20_X21:
+    return AArch64::X14;
+  case AArch64::X16_X17_X18_X19_X20_X21_X22_X23:
+    return AArch64::X16;
+  case AArch64::X18_X19_X20_X21_X22_X23_X24_X25:
+    return AArch64::X18;
+  case AArch64::X20_X21_X22_X23_X24_X25_X26_X27:
+    return AArch64::X20;
+  case AArch64::X22_X23_X24_X25_X26_X27_X28_FP:
+    return AArch64::X22;
   }
   // For anything else, return it unchanged.
   return RegTuple;
@@ -128,38 +206,70 @@ inline static MCRegister getXRegFromXRegTuple(MCRegister RegTuple) {
 
 static inline MCRegister getBRegFromDReg(MCRegister Reg) {
   switch (Reg.id()) {
-  case AArch64::D0:  return AArch64::B0;
-  case AArch64::D1:  return AArch64::B1;
-  case AArch64::D2:  return AArch64::B2;
-  case AArch64::D3:  return AArch64::B3;
-  case AArch64::D4:  return AArch64::B4;
-  case AArch64::D5:  return AArch64::B5;
-  case AArch64::D6:  return AArch64::B6;
-  case AArch64::D7:  return AArch64::B7;
-  case AArch64::D8:  return AArch64::B8;
-  case AArch64::D9:  return AArch64::B9;
-  case AArch64::D10: return AArch64::B10;
-  case AArch64::D11: return AArch64::B11;
-  case AArch64::D12: return AArch64::B12;
-  case AArch64::D13: return AArch64::B13;
-  case AArch64::D14: return AArch64::B14;
-  case AArch64::D15: return AArch64::B15;
-  case AArch64::D16: return AArch64::B16;
-  case AArch64::D17: return AArch64::B17;
-  case AArch64::D18: return AArch64::B18;
-  case AArch64::D19: return AArch64::B19;
-  case AArch64::D20: return AArch64::B20;
-  case AArch64::D21: return AArch64::B21;
-  case AArch64::D22: return AArch64::B22;
-  case AArch64::D23: return AArch64::B23;
-  case AArch64::D24: return AArch64::B24;
-  case AArch64::D25: return AArch64::B25;
-  case AArch64::D26: return AArch64::B26;
-  case AArch64::D27: return AArch64::B27;
-  case AArch64::D28: return AArch64::B28;
-  case AArch64::D29: return AArch64::B29;
-  case AArch64::D30: return AArch64::B30;
-  case AArch64::D31: return AArch64::B31;
+  case AArch64::D0:
+    return AArch64::B0;
+  case AArch64::D1:
+    return AArch64::B1;
+  case AArch64::D2:
+    return AArch64::B2;
+  case AArch64::D3:
+    return AArch64::B3;
+  case AArch64::D4:
+    return AArch64::B4;
+  case AArch64::D5:
+    return AArch64::B5;
+  case AArch64::D6:
+    return AArch64::B6;
+  case AArch64::D7:
+    return AArch64::B7;
+  case AArch64::D8:
+    return AArch64::B8;
+  case AArch64::D9:
+    return AArch64::B9;
+  case AArch64::D10:
+    return AArch64::B10;
+  case AArch64::D11:
+    return AArch64::B11;
+  case AArch64::D12:
+    return AArch64::B12;
+  case AArch64::D13:
+    return AArch64::B13;
+  case AArch64::D14:
+    return AArch64::B14;
+  case AArch64::D15:
+    return AArch64::B15;
+  case AArch64::D16:
+    return AArch64::B16;
+  case AArch64::D17:
+    return AArch64::B17;
+  case AArch64::D18:
+    return AArch64::B18;
+  case AArch64::D19:
+    return AArch64::B19;
+  case AArch64::D20:
+    return AArch64::B20;
+  case AArch64::D21:
+    return AArch64::B21;
+  case AArch64::D22:
+    return AArch64::B22;
+  case AArch64::D23:
+    return AArch64::B23;
+  case AArch64::D24:
+    return AArch64::B24;
+  case AArch64::D25:
+    return AArch64::B25;
+  case AArch64::D26:
+    return AArch64::B26;
+  case AArch64::D27:
+    return AArch64::B27;
+  case AArch64::D28:
+    return AArch64::B28;
+  case AArch64::D29:
+    return AArch64::B29;
+  case AArch64::D30:
+    return AArch64::B30;
+  case AArch64::D31:
+    return AArch64::B31;
   }
   // For anything else, return it unchanged.
   return Reg;
@@ -167,38 +277,70 @@ static inline MCRegister getBRegFromDReg(MCRegister Reg) {
 
 static inline MCRegister getDRegFromBReg(MCRegister Reg) {
   switch (Reg.id()) {
-  case AArch64::B0:  return AArch64::D0;
-  case AArch64::B1:  return AArch64::D1;
-  case AArch64::B2:  return AArch64::D2;
-  case AArch64::B3:  return AArch64::D3;
-  case AArch64::B4:  return AArch64::D4;
-  case AArch64::B5:  return AArch64::D5;
-  case AArch64::B6:  return AArch64::D6;
-  case AArch64::B7:  return AArch64::D7;
-  case AArch64::B8:  return AArch64::D8;
-  case AArch64::B9:  return AArch64::D9;
-  case AArch64::B10: return AArch64::D10;
-  case AArch64::B11: return AArch64::D11;
-  case AArch64::B12: return AArch64::D12;
-  case AArch64::B13: return AArch64::D13;
-  case AArch64::B14: return AArch64::D14;
-  case AArch64::B15: return AArch64::D15;
-  case AArch64::B16: return AArch64::D16;
-  case AArch64::B17: return AArch64::D17;
-  case AArch64::B18: return AArch64::D18;
-  case AArch64::B19: return AArch64::D19;
-  case AArch64::B20: return AArch64::D20;
-  case AArch64::B21: return AArch64::D21;
-  case AArch64::B22: return AArch64::D22;
-  case AArch64::B23: return AArch64::D23;
-  case AArch64::B24: return AArch64::D24;
-  case AArch64::B25: return AArch64::D25;
-  case AArch64::B26: return AArch64::D26;
-  case AArch64::B27: return AArch64::D27;
-  case AArch64::B28: return AArch64::D28;
-  case AArch64::B29: return AArch64::D29;
-  case AArch64::B30: return AArch64::D30;
-  case AArch64::B31: return AArch64::D31;
+  case AArch64::B0:
+    return AArch64::D0;
+  case AArch64::B1:
+    return AArch64::D1;
+  case AArch64::B2:
+    return AArch64::D2;
+  case AArch64::B3:
+    return AArch64::D3;
+  case AArch64::B4:
+    return AArch64::D4;
+  case AArch64::B5:
+    return AArch64::D5;
+  case AArch64::B6:
+    return AArch64::D6;
+  case AArch64::B7:
+    return AArch64::D7;
+  case AArch64::B8:
+    return AArch64::D8;
+  case AArch64::B9:
+    return AArch64::D9;
+  case AArch64::B10:
+    return AArch64::D10;
+  case AArch64::B11:
+    return AArch64::D11;
+  case AArch64::B12:
+    return AArch64::D12;
+  case AArch64::B13:
+    return AArch64::D13;
+  case AArch64::B14:
+    return AArch64::D14;
+  case AArch64::B15:
+    return AArch64::D15;
+  case AArch64::B16:
+    return AArch64::D16;
+  case AArch64::B17:
+    return AArch64::D17;
+  case AArch64::B18:
+    return AArch64::D18;
+  case AArch64::B19:
+    return AArch64::D19;
+  case AArch64::B20:
+    return AArch64::D20;
+  case AArch64::B21:
+    return AArch64::D21;
+  case AArch64::B22:
+    return AArch64::D22;
+  case AArch64::B23:
+    return AArch64::D23;
+  case AArch64::B24:
+    return AArch64::D24;
+  case AArch64::B25:
+    return AArch64::D25;
+  case AArch64::B26:
+    return AArch64::D26;
+  case AArch64::B27:
+    return AArch64::D27;
+  case AArch64::B28:
+    return AArch64::D28;
+  case AArch64::B29:
+    return AArch64::D29;
+  case AArch64::B30:
+    return AArch64::D30;
+  case AArch64::B31:
+    return AArch64::D31;
   }
   // For anything else, return it unchanged.
   return Reg;
@@ -206,42 +348,78 @@ static inline MCRegister getDRegFromBReg(MCRegister Reg) {
 
 static inline bool atomicBarrierDroppedOnZero(unsigned Opcode) {
   switch (Opcode) {
-  case AArch64::LDADDAB:   case AArch64::LDADDAH:
-  case AArch64::LDADDAW:   case AArch64::LDADDAX:
-  case AArch64::LDADDALB:  case AArch64::LDADDALH:
-  case AArch64::LDADDALW:  case AArch64::LDADDALX:
-  case AArch64::LDCLRAB:   case AArch64::LDCLRAH:
-  case AArch64::LDCLRAW:   case AArch64::LDCLRAX:
-  case AArch64::LDCLRALB:  case AArch64::LDCLRALH:
-  case AArch64::LDCLRALW:  case AArch64::LDCLRALX:
-  case AArch64::LDEORAB:   case AArch64::LDEORAH:
-  case AArch64::LDEORAW:   case AArch64::LDEORAX:
-  case AArch64::LDEORALB:  case AArch64::LDEORALH:
-  case AArch64::LDEORALW:  case AArch64::LDEORALX:
-  case AArch64::LDSETAB:   case AArch64::LDSETAH:
-  case AArch64::LDSETAW:   case AArch64::LDSETAX:
-  case AArch64::LDSETALB:  case AArch64::LDSETALH:
-  case AArch64::LDSETALW:  case AArch64::LDSETALX:
-  case AArch64::LDSMAXAB:  case AArch64::LDSMAXAH:
-  case AArch64::LDSMAXAW:  case AArch64::LDSMAXAX:
-  case AArch64::LDSMAXALB: case AArch64::LDSMAXALH:
-  case AArch64::LDSMAXALW: case AArch64::LDSMAXALX:
-  case AArch64::LDSMINAB:  case AArch64::LDSMINAH:
-  case AArch64::LDSMINAW:  case AArch64::LDSMINAX:
-  case AArch64::LDSMINALB: case AArch64::LDSMINALH:
-  case AArch64::LDSMINALW: case AArch64::LDSMINALX:
-  case AArch64::LDUMAXAB:  case AArch64::LDUMAXAH:
-  case AArch64::LDUMAXAW:  case AArch64::LDUMAXAX:
-  case AArch64::LDUMAXALB: case AArch64::LDUMAXALH:
-  case AArch64::LDUMAXALW: case AArch64::LDUMAXALX:
-  case AArch64::LDUMINAB:  case AArch64::LDUMINAH:
-  case AArch64::LDUMINAW:  case AArch64::LDUMINAX:
-  case AArch64::LDUMINALB: case AArch64::LDUMINALH:
-  case AArch64::LDUMINALW: case AArch64::LDUMINALX:
-  case AArch64::SWPAB:     case AArch64::SWPAH:
-  case AArch64::SWPAW:     case AArch64::SWPAX:
-  case AArch64::SWPALB:    case AArch64::SWPALH:
-  case AArch64::SWPALW:    case AArch64::SWPALX:
+  case AArch64::LDADDAB:
+  case AArch64::LDADDAH:
+  case AArch64::LDADDAW:
+  case AArch64::LDADDAX:
+  case AArch64::LDADDALB:
+  case AArch64::LDADDALH:
+  case AArch64::LDADDALW:
+  case AArch64::LDADDALX:
+  case AArch64::LDCLRAB:
+  case AArch64::LDCLRAH:
+  case AArch64::LDCLRAW:
+  case AArch64::LDCLRAX:
+  case AArch64::LDCLRALB:
+  case AArch64::LDCLRALH:
+  case AArch64::LDCLRALW:
+  case AArch64::LDCLRALX:
+  case AArch64::LDEORAB:
+  case AArch64::LDEORAH:
+  case AArch64::LDEORAW:
+  case AArch64::LDEORAX:
+  case AArch64::LDEORALB:
+  case AArch64::LDEORALH:
+  case AArch64::LDEORALW:
+  case AArch64::LDEORALX:
+  case AArch64::LDSETAB:
+  case AArch64::LDSETAH:
+  case AArch64::LDSETAW:
+  case AArch64::LDSETAX:
+  case AArch64::LDSETALB:
+  case AArch64::LDSETALH:
+  case AArch64::LDSETALW:
+  case AArch64::LDSETALX:
+  case AArch64::LDSMAXAB:
+  case AArch64::LDSMAXAH:
+  case AArch64::LDSMAXAW:
+  case AArch64::LDSMAXAX:
+  case AArch64::LDSMAXALB:
+  case AArch64::LDSMAXALH:
+  case AArch64::LDSMAXALW:
+  case AArch64::LDSMAXALX:
+  case AArch64::LDSMINAB:
+  case AArch64::LDSMINAH:
+  case AArch64::LDSMINAW:
+  case AArch64::LDSMINAX:
+  case AArch64::LDSMINALB:
+  case AArch64::LDSMINALH:
+  case AArch64::LDSMINALW:
+  case AArch64::LDSMINALX:
+  case AArch64::LDUMAXAB:
+  case AArch64::LDUMAXAH:
+  case AArch64::LDUMAXAW:
+  case AArch64::LDUMAXAX:
+  case AArch64::LDUMAXALB:
+  case AArch64::LDUMAXALH:
+  case AArch64::LDUMAXALW:
+  case AArch64::LDUMAXALX:
+  case AArch64::LDUMINAB:
+  case AArch64::LDUMINAH:
+  case AArch64::LDUMINAW:
+  case AArch64::LDUMINAX:
+  case AArch64::LDUMINALB:
+  case AArch64::LDUMINALH:
+  case AArch64::LDUMINALW:
+  case AArch64::LDUMINALX:
+  case AArch64::SWPAB:
+  case AArch64::SWPAH:
+  case AArch64::SWPAW:
+  case AArch64::SWPAX:
+  case AArch64::SWPALB:
+  case AArch64::SWPALH:
+  case AArch64::SWPALW:
+  case AArch64::SWPALX:
     return true;
   }
   return false;
@@ -251,52 +429,69 @@ namespace AArch64CC {
 
 // The CondCodes constants map directly to the 4-bit encoding of the condition
 // field for predicated instructions.
-enum CondCode {  // Meaning (integer)          Meaning (floating-point)
-  EQ = 0x0,      // Equal                      Equal
-  NE = 0x1,      // Not equal                  Not equal, or unordered
-  HS = 0x2,      // Unsigned higher or same    >, ==, or unordered
-  LO = 0x3,      // Unsigned lower             Less than
-  MI = 0x4,      // Minus, negative            Less than
-  PL = 0x5,      // Plus, positive or zero     >, ==, or unordered
-  VS = 0x6,      // Overflow                   Unordered
-  VC = 0x7,      // No overflow                Not unordered
-  HI = 0x8,      // Unsigned higher            Greater than, or unordered
-  LS = 0x9,      // Unsigned lower or same     Less than or equal
-  GE = 0xa,      // Greater than or equal      Greater than or equal
-  LT = 0xb,      // Less than                  Less than, or unordered
-  GT = 0xc,      // Greater than               Greater than
-  LE = 0xd,      // Less than or equal         <, ==, or unordered
-  AL = 0xe,      // Always (unconditional)     Always (unconditional)
-  NV = 0xf,      // Always (unconditional)     Always (unconditional)
+enum CondCode { // Meaning (integer)          Meaning (floating-point)
+  EQ = 0x0,     // Equal                      Equal
+  NE = 0x1,     // Not equal                  Not equal, or unordered
+  HS = 0x2,     // Unsigned higher or same    >, ==, or unordered
+  LO = 0x3,     // Unsigned lower             Less than
+  MI = 0x4,     // Minus, negative            Less than
+  PL = 0x5,     // Plus, positive or zero     >, ==, or unordered
+  VS = 0x6,     // Overflow                   Unordered
+  VC = 0x7,     // No overflow                Not unordered
+  HI = 0x8,     // Unsigned higher            Greater than, or unordered
+  LS = 0x9,     // Unsigned lower or same     Less than or equal
+  GE = 0xa,     // Greater than or equal      Greater than or equal
+  LT = 0xb,     // Less than                  Less than, or unordered
+  GT = 0xc,     // Greater than               Greater than
+  LE = 0xd,     // Less than or equal         <, ==, or unordered
+  AL = 0xe,     // Always (unconditional)     Always (unconditional)
+  NV = 0xf,     // Always (unconditional)     Always (unconditional)
   // Note the NV exists purely to disassemble 0b1111. Execution is "always".
   Invalid,
 
   // Common aliases used for SVE.
-  ANY_ACTIVE   = NE, // (!Z)
+  ANY_ACTIVE = NE,   // (!Z)
   FIRST_ACTIVE = MI, // ( N)
-  LAST_ACTIVE  = LO, // (!C)
-  NONE_ACTIVE  = EQ  // ( Z)
+  LAST_ACTIVE = LO,  // (!C)
+  NONE_ACTIVE = EQ   // ( Z)
 };
 
 inline static const char *getCondCodeName(CondCode Code) {
   switch (Code) {
-  default: llvm_unreachable("Unknown condition code");
-  case EQ:  return "eq";
-  case NE:  return "ne";
-  case HS:  return "hs";
-  case LO:  return "lo";
-  case MI:  return "mi";
-  case PL:  return "pl";
-  case VS:  return "vs";
-  case VC:  return "vc";
-  case HI:  return "hi";
-  case LS:  return "ls";
-  case GE:  return "ge";
-  case LT:  return "lt";
-  case GT:  return "gt";
-  case LE:  return "le";
-  case AL:  return "al";
-  case NV:  return "nv";
+  default:
+    llvm_unreachable("Unknown condition code");
+  case EQ:
+    return "eq";
+  case NE:
+    return "ne";
+  case HS:
+    return "hs";
+  case LO:
+    return "lo";
+  case MI:
+    return "mi";
+  case PL:
+    return "pl";
+  case VS:
+    return "vs";
+  case VC:
+    return "vc";
+  case HI:
+    return "hi";
+  case LS:
+    return "ls";
+  case GE:
+    return "ge";
+  case LT:
+    return "lt";
+  case GT:
+    return "gt";
+  case LE:
+    return "le";
+  case AL:
+    return "al";
+  case NV:
+    return "nv";
   }
 }
 
@@ -344,21 +539,36 @@ inline static unsigned getNZCVToSatisfyCondCode(CondCode Code) {
   // NZCV flags encoded as expected by ccmp instructions, ARMv8 ISA 5.5.7.
   enum { N = 8, Z = 4, C = 2, V = 1 };
   switch (Code) {
-  default: llvm_unreachable("Unknown condition code");
-  case EQ: return Z; // Z == 1
-  case NE: return 0; // Z == 0
-  case HS: return C; // C == 1
-  case LO: return 0; // C == 0
-  case MI: return N; // N == 1
-  case PL: return 0; // N == 0
-  case VS: return V; // V == 1
-  case VC: return 0; // V == 0
-  case HI: return C; // C == 1 && Z == 0
-  case LS: return 0; // C == 0 || Z == 1
-  case GE: return 0; // N == V
-  case LT: return N; // N != V
-  case GT: return 0; // Z == 0 && N == V
-  case LE: return Z; // Z == 1 || N != V
+  default:
+    llvm_unreachable("Unknown condition code");
+  case EQ:
+    return Z; // Z == 1
+  case NE:
+    return 0; // Z == 0
+  case HS:
+    return C; // C == 1
+  case LO:
+    return 0; // C == 0
+  case MI:
+    return N; // N == 1
+  case PL:
+    return 0; // N == 0
+  case VS:
+    return V; // V == 1
+  case VC:
+    return 0; // V == 0
+  case HI:
+    return C; // C == 1 && Z == 0
+  case LS:
+    return 0; // C == 0 || Z == 1
+  case GE:
+    return 0; // N == V
+  case LT:
+    return N; // N != V
+  case GT:
+    return 0; // Z == 0 && N == V
+  case LE:
+    return Z; // Z == 1 || N != V
   }
 }
 
@@ -398,94 +608,94 @@ struct SysAliasImm : SysAlias {
 };
 
 namespace AArch64SVCR {
-  struct SVCR : SysAlias{
-    using SysAlias::SysAlias;
-  };
+struct SVCR : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_SVCRValues_DECL
 #define GET_SVCRsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64SVCR
 
-namespace AArch64AT{
-  struct AT : SysAlias {
-    using SysAlias::SysAlias;
-  };
+namespace AArch64AT {
+struct AT : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_ATValues_DECL
 #define GET_ATsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64AT
 
 namespace AArch64DB {
-  struct DB : SysAlias {
-    using SysAlias::SysAlias;
-  };
+struct DB : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_DBValues_DECL
 #define GET_DBsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64DB
 
 namespace AArch64DBnXS {
-  struct DBnXS : SysAliasImm {
-    using SysAliasImm::SysAliasImm;
-  };
+struct DBnXS : SysAliasImm {
+  using SysAliasImm::SysAliasImm;
+};
 #define GET_DBnXSValues_DECL
 #define GET_DBnXSsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64DBnXS
 
-namespace  AArch64DC {
-  struct DC : SysAlias {
-    using SysAlias::SysAlias;
-  };
+namespace AArch64DC {
+struct DC : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_DCValues_DECL
 #define GET_DCsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64DC
 
-namespace  AArch64IC {
-  struct IC : SysAliasReg {
-    using SysAliasReg::SysAliasReg;
-  };
+namespace AArch64IC {
+struct IC : SysAliasReg {
+  using SysAliasReg::SysAliasReg;
+};
 #define GET_ICValues_DECL
 #define GET_ICsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64IC
 
-namespace  AArch64ISB {
-  struct ISB : SysAlias {
-    using SysAlias::SysAlias;
-  };
+namespace AArch64ISB {
+struct ISB : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_ISBValues_DECL
 #define GET_ISBsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64ISB
 
-namespace  AArch64TSB {
-  struct TSB : SysAlias {
-    using SysAlias::SysAlias;
-  };
+namespace AArch64TSB {
+struct TSB : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_TSBValues_DECL
 #define GET_TSBsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64TSB
 
 namespace AArch64PRFM {
-  struct PRFM : SysAlias {
-    using SysAlias::SysAlias;
-  };
+struct PRFM : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_PRFMValues_DECL
 #define GET_PRFMsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64PRFM
 
 namespace AArch64SVEPRFM {
-  struct SVEPRFM : SysAlias {
-    using SysAlias::SysAlias;
-  };
+struct SVEPRFM : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_SVEPRFMValues_DECL
 #define GET_SVEPRFMsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64SVEPRFM
 
 namespace AArch64RPRFM {
 struct RPRFM : SysAlias {
@@ -497,20 +707,20 @@ struct RPRFM : SysAlias {
 } // namespace AArch64RPRFM
 
 namespace AArch64SVEPredPattern {
-  struct SVEPREDPAT {
-    const char *Name;
-    uint16_t Encoding;
-  };
+struct SVEPREDPAT {
+  const char *Name;
+  uint16_t Encoding;
+};
 #define GET_SVEPREDPATValues_DECL
 #define GET_SVEPREDPATsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64SVEPredPattern
 
 namespace AArch64SVEVecLenSpecifier {
-  struct SVEVECLENSPECIFIER {
-    const char *Name;
-    uint16_t Encoding;
-  };
+struct SVEVECLENSPECIFIER {
+  const char *Name;
+  uint16_t Encoding;
+};
 #define GET_SVEVECLENSPECIFIERValues_DECL
 #define GET_SVEVECLENSPECIFIERsList_DECL
 #include "AArch64GenSystemOperands.inc"
@@ -601,32 +811,32 @@ struct ExactFPImm {
 #define GET_ExactFPImmValues_DECL
 #define GET_ExactFPImmsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64ExactFPImm
 
 namespace AArch64PState {
-  struct PStateImm0_15 : SysAlias{
-    using SysAlias::SysAlias;
-  };
+struct PStateImm0_15 : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_PStateImm0_15Values_DECL
 #define GET_PStateImm0_15sList_DECL
 #include "AArch64GenSystemOperands.inc"
 
-  struct PStateImm0_1 : SysAlias{
-    using SysAlias::SysAlias;
-  };
+struct PStateImm0_1 : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_PStateImm0_1Values_DECL
 #define GET_PStateImm0_1sList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64PState
 
 namespace AArch64PSBHint {
-  struct PSB : SysAlias {
-    using SysAlias::SysAlias;
-  };
+struct PSB : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_PSBValues_DECL
 #define GET_PSBsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64PSBHint
 
 namespace AArch64PHint {
 struct PHint {
@@ -649,13 +859,13 @@ const PHint *lookupPHintByEncoding(uint16_t);
 } // namespace AArch64PHint
 
 namespace AArch64BTIHint {
-  struct BTI : SysAlias {
-    using SysAlias::SysAlias;
-  };
+struct BTI : SysAlias {
+  using SysAlias::SysAlias;
+};
 #define GET_BTIValues_DECL
 #define GET_BTIsList_DECL
 #include "AArch64GenSystemOperands.inc"
-}
+} // namespace AArch64BTIHint
 
 namespace AArch64SME {
 enum ToggleCondition : unsigned {
@@ -666,114 +876,127 @@ enum ToggleCondition : unsigned {
 }
 
 namespace AArch64SE {
-    enum ShiftExtSpecifiers {
-        Invalid = -1,
-        LSL,
-        MSL,
-        LSR,
-        ASR,
-        ROR,
-
-        UXTB,
-        UXTH,
-        UXTW,
-        UXTX,
-
-        SXTB,
-        SXTH,
-        SXTW,
-        SXTX
-    };
+enum ShiftExtSpecifiers {
+  Invalid = -1,
+  LSL,
+  MSL,
+  LSR,
+  ASR,
+  ROR,
+
+  UXTB,
+  UXTH,
+  UXTW,
+  UXTX,
+
+  SXTB,
+  SXTH,
+  SXTW,
+  SXTX
+};
 }
 
 namespace AArch64Layout {
-    enum VectorLayout {
-        Invalid = -1,
-        VL_8B,
-        VL_4H,
-        VL_2S,
-        VL_1D,
-
-        VL_16B,
-        VL_8H,
-        VL_4S,
-        VL_2D,
-
-        // Bare layout for the 128-bit vector
-        // (only show ".b", ".h", ".s", ".d" without vector number)
-        VL_B,
-        VL_H,
-        VL_S,
-        VL_D
-    };
+enum VectorLayout {
+  Invalid = -1,
+  VL_8B,
+  VL_4H,
+  VL_2S,
+  VL_1D,
+
+  VL_16B,
+  VL_8H,
+  VL_4S,
+  VL_2D,
+
+  // Bare layout for the 128-bit vector
+  // (only show ".b", ".h", ".s", ".d" without vector number)
+  VL_B,
+  VL_H,
+  VL_S,
+  VL_D
+};
 }
 
 inline static const char *
 AArch64VectorLayoutToString(AArch64Layout::VectorLayout Layout) {
   switch (Layout) {
-  case AArch64Layout::VL_8B:  return ".8b";
-  case AArch64Layout::VL_4H:  return ".4h";
-  case AArch64Layout::VL_2S:  return ".2s";
-  case AArch64Layout::VL_1D:  return ".1d";
-  case AArch64Layout::VL_16B:  return ".16b";
-  case AArch64Layout::VL_8H:  return ".8h";
-  case AArch64Layout::VL_4S:  return ".4s";
-  case AArch64Layout::VL_2D:  return ".2d";
-  case AArch64Layout::VL_B:  return ".b";
-  case AArch64Layout::VL_H:  return ".h";
-  case AArch64Layout::VL_S:  return ".s";
-  case AArch64Layout::VL_D:  return ".d";
-  default: llvm_unreachable("Unknown Vector Layout");
+  case AArch64Layout::VL_8B:
+    return ".8b";
+  case AArch64Layout::VL_4H:
+    return ".4h";
+  case AArch64Layout::VL_2S:
+    return ".2s";
+  case AArch64Layout::VL_1D:
+    return ".1d";
+  case AArch64Layout::VL_16B:
+    return ".16b";
+  case AArch64Layout::VL_8H:
+    return ".8h";
+  case AArch64Layout::VL_4S:
+    return ".4s";
+  case AArch64Layout::VL_2D:
+    return ".2d";
+  case AArch64Layout::VL_B:
+    return ".b";
+  case AArch64Layout::VL_H:
+    return ".h";
+  case AArch64Layout::VL_S:
+    return ".s";
+  case AArch64Layout::VL_D:
+    return ".d";
+  default:
+    llvm_unreachable("Unknown Vector Layout");
   }
 }
 
 inline static AArch64Layout::VectorLayout
 AArch64StringToVectorLayout(StringRef LayoutStr) {
   return StringSwitch<AArch64Layout::VectorLayout>(LayoutStr)
-             .Case(".8b", AArch64Layout::VL_8B)
-             .Case(".4h", AArch64Layout::VL_4H)
-             .Case(".2s", AArch64Layout::VL_2S)
-             .Case(".1d", AArch64Layout::VL_1D)
-             .Case(".16b", AArch64Layout::VL_16B)
-             .Case(".8h", AArch64Layout::VL_8H)
-             .Case(".4s", AArch64Layout::VL_4S)
-             .Case(".2d", AArch64Layout::VL_2D)
-             .Case(".b", AArch64Layout::VL_B)
-             .Case(".h", AArch64Layout::VL_H)
-             .Case(".s", AArch64Layout::VL_S)
-             .Case(".d", AArch64Layout::VL_D)
-             .Default(AArch64Layout::Invalid);
+      .Case(".8b", AArch64Layout::VL_8B)
+      .Case(".4h", AArch64Layout::VL_4H)
+      .Case(".2s", AArch64Layout::VL_2S)
+      .Case(".1d", AArch64Layout::VL_1D)
+      .Case(".16b", AArch64Layout::VL_16B)
+      .Case(".8h", AArch64Layout::VL_8H)
+      .Case(".4s", AArch64Layout::VL_4S)
+      .Case(".2d", AArch64Layout::VL_2D)
+      .Case(".b", AArch64Layout::VL_B)
+      .Case(".h", AArch64Layout::VL_H)
+      .Case(".s", AArch64Layout::VL_S)
+      .Case(".d", AArch64Layout::VL_D)
+      .Default(AArch64Layout::Invalid);
 }
 
 namespace AArch64SysReg {
-  struct SysReg {
-    const char Name[32];
-    unsigned Encoding;
-    bool Readable;
-    bool Writeable;
-    FeatureBitset FeaturesRequired;
-
-    bool haveFeatures(FeatureBitset ActiveFeatures) const {
-      return ActiveFeatures[llvm::AArch64::FeatureAll] ||
-             (FeaturesRequired & ActiveFeatures) == FeaturesRequired;
-    }
-  };
+struct SysReg {
+  const char Name[32];
+  unsigned Encoding;
+  bool Readable;
+  bool Writeable;
+  FeatureBitset FeaturesRequired;
+
+  bool haveFeatures(FeatureBitset ActiveFeatures) const {
+    return ActiveFeatures[llvm::AArch64::FeatureAll] ||
+           (FeaturesRequired & ActiveFeatures) == FeaturesRequired;
+  }
+};
 
 #define GET_SysRegsList_DECL
 #define GET_SysRegValues_DECL
 #include "AArch64GenSystemOperands.inc"
 
-  uint32_t parseGenericRegister(StringRef Name);
-  std::string genericRegisterString(uint32_t Bits);
-}
+uint32_t parseGenericRegister(StringRef Name);
+std::string genericRegisterString(uint32_t Bits);
+} // namespace AArch64SysReg
 
 namespace AArch64TLBI {
-  struct TLBI : SysAliasReg {
-    using SysAliasReg::SysAliasReg;
-  };
-  #define GET_TLBITable_DECL
-  #include "AArch64GenSystemOperands.inc"
-}
+struct TLBI : SysAliasReg {
+  using SysAliasReg::SysAliasReg;
+};
+#define GET_TLBITable_DECL
+#include "AArch64GenSystemOperands.inc"
+} // namespace AArch64TLBI
 
 namespace AArch64II {
 /// Target Operand Flag enum.
@@ -871,13 +1094,7 @@ enum TOF {
 //
 
 namespace AArch64PACKey {
-enum ID : uint8_t {
-  IA = 0,
-  IB = 1,
-  DA = 2,
-  DB = 3,
-  LAST = DB
-};
+enum ID : uint8_t { IA = 0, IB = 1, DA = 2, DB = 3, LAST = DB };
 } // namespace AArch64PACKey
 
 /// Return 2-letter identifier string for numeric key ID.
diff --git a/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp b/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp
index a6285a55f..a59bd6d69 100644
--- a/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp
+++ b/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp
@@ -6,13 +6,13 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86BaseInfo.h"
-#include "MCTargetDesc/X86EncodingOptimization.h"
-#include "MCTargetDesc/X86IntelInstPrinter.h"
-#include "MCTargetDesc/X86MCExpr.h"
-#include "MCTargetDesc/X86MCTargetDesc.h"
-#include "MCTargetDesc/X86TargetStreamer.h"
-#include "TargetInfo/X86TargetInfo.h"
+#include "../MCTargetDesc/X86BaseInfo.h"
+#include "../MCTargetDesc/X86EncodingOptimization.h"
+#include "../MCTargetDesc/X86IntelInstPrinter.h"
+#include "../MCTargetDesc/X86MCExpr.h"
+#include "../MCTargetDesc/X86MCTargetDesc.h"
+#include "../MCTargetDesc/X86TargetStreamer.h"
+#include "../TargetInfo/X86TargetInfo.h"
 #include "X86Operand.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallString.h"
@@ -45,7 +45,8 @@ using namespace llvm;
 static cl::opt<bool> LVIInlineAsmHardening(
     "x86-experimental-lvi-inline-asm-hardening",
     cl::desc("Harden inline assembly code that may be vulnerable to Load Value"
-             " Injection (LVI). This feature is experimental."), cl::Hidden);
+             " Injection (LVI). This feature is experimental."),
+    cl::Hidden);
 
 static bool checkScale(unsigned Scale, StringRef &ErrMsg) {
   if (Scale != 1 && Scale != 2 && Scale != 4 && Scale != 8) {
@@ -137,9 +138,9 @@ private:
     // In Code16GCC mode, match as 32-bit.
     if (Code16GCC)
       SwitchMode(X86::Is32Bit);
-    unsigned rv = MatchInstructionImpl(Operands, Inst, ErrorInfo,
-                                       MissingFeatures, matchingInlineAsm,
-                                       VariantID);
+    unsigned rv =
+        MatchInstructionImpl(Operands, Inst, ErrorInfo, MissingFeatures,
+                             matchingInlineAsm, VariantID);
     if (Code16GCC)
       SwitchMode(X86::Is16Bit);
     return rv;
@@ -185,7 +186,7 @@ private:
   };
 
   class InfixCalculator {
-    typedef std::pair< InfixCalculatorTok, int64_t > ICToken;
+    typedef std::pair<InfixCalculatorTok, int64_t> ICToken;
     SmallVector<InfixCalculatorTok, 4> InfixOperatorStack;
     SmallVector<ICToken, 4> PostfixStack;
 
@@ -195,15 +196,14 @@ private:
 
   public:
     int64_t popOperand() {
-      assert (!PostfixStack.empty() && "Poped an empty stack!");
+      assert(!PostfixStack.empty() && "Poped an empty stack!");
       ICToken Op = PostfixStack.pop_back_val();
       if (!(Op.first == IC_IMM || Op.first == IC_REGISTER))
         return -1; // The invalid Scale value will be caught later by checkScale
       return Op.second;
     }
     void pushOperand(InfixCalculatorTok Op, int64_t Val = 0) {
-      assert ((Op == IC_IMM || Op == IC_REGISTER) &&
-              "Unexpected operand!");
+      assert((Op == IC_IMM || Op == IC_REGISTER) && "Unexpected operand!");
       PostfixStack.push_back(std::make_pair(Op, Val));
     }
 
@@ -274,10 +274,9 @@ private:
         if (Op.first == IC_IMM || Op.first == IC_REGISTER) {
           OperandStack.push_back(Op);
         } else if (isUnaryOperator(Op.first)) {
-          assert (OperandStack.size() > 0 && "Too few operands.");
+          assert(OperandStack.size() > 0 && "Too few operands.");
           ICToken Operand = OperandStack.pop_back_val();
-          assert (Operand.first == IC_IMM &&
-                  "Unary operation with a register!");
+          assert(Operand.first == IC_IMM && "Unary operation with a register!");
           switch (Op.first) {
           default:
             report_fatal_error("Unexpected operator!");
@@ -290,7 +289,7 @@ private:
             break;
           }
         } else {
-          assert (OperandStack.size() > 1 && "Too few operands.");
+          assert(OperandStack.size() > 1 && "Too few operands.");
           int64_t Val;
           ICToken Op2 = OperandStack.pop_back_val();
           ICToken Op1 = OperandStack.pop_back_val();
@@ -307,51 +306,51 @@ private:
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_MULTIPLY:
-            assert (Op1.first == IC_IMM && Op2.first == IC_IMM &&
-                    "Multiply operation with an immediate and a register!");
+            assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
+                   "Multiply operation with an immediate and a register!");
             Val = Op1.second * Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_DIVIDE:
-            assert (Op1.first == IC_IMM && Op2.first == IC_IMM &&
-                    "Divide operation with an immediate and a register!");
-            assert (Op2.second != 0 && "Division by zero!");
+            assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
+                   "Divide operation with an immediate and a register!");
+            assert(Op2.second != 0 && "Division by zero!");
             Val = Op1.second / Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_MOD:
-            assert (Op1.first == IC_IMM && Op2.first == IC_IMM &&
-                    "Modulo operation with an immediate and a register!");
+            assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
+                   "Modulo operation with an immediate and a register!");
             Val = Op1.second % Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_OR:
-            assert (Op1.first == IC_IMM && Op2.first == IC_IMM &&
-                    "Or operation with an immediate and a register!");
+            assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
+                   "Or operation with an immediate and a register!");
             Val = Op1.second | Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_XOR:
             assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
-              "Xor operation with an immediate and a register!");
+                   "Xor operation with an immediate and a register!");
             Val = Op1.second ^ Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_AND:
-            assert (Op1.first == IC_IMM && Op2.first == IC_IMM &&
-                    "And operation with an immediate and a register!");
+            assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
+                   "And operation with an immediate and a register!");
             Val = Op1.second & Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_LSHIFT:
-            assert (Op1.first == IC_IMM && Op2.first == IC_IMM &&
-                    "Left shift operation with an immediate and a register!");
+            assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
+                   "Left shift operation with an immediate and a register!");
             Val = Op1.second << Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
           case IC_RSHIFT:
-            assert (Op1.first == IC_IMM && Op2.first == IC_IMM &&
-                    "Right shift operation with an immediate and a register!");
+            assert(Op1.first == IC_IMM && Op2.first == IC_IMM &&
+                   "Right shift operation with an immediate and a register!");
             Val = Op1.second >> Op2.second;
             OperandStack.push_back(std::make_pair(IC_IMM, Val));
             break;
@@ -396,7 +395,7 @@ private:
           }
         }
       }
-      assert (OperandStack.size() == 1 && "Expected a single result.");
+      assert(OperandStack.size() == 1 && "Expected a single result.");
       return OperandStack.pop_back_val().second;
     }
   };
@@ -733,7 +732,7 @@ private:
         State = IES_MINUS;
         // push minus operator if it is not a negate operator
         if (CurrState == IES_REGISTER || CurrState == IES_RPAREN ||
-            CurrState == IES_INTEGER  || CurrState == IES_RBRAC  ||
+            CurrState == IES_INTEGER || CurrState == IES_RBRAC ||
             CurrState == IES_OFFSET)
           IC.pushOperator(IC_MINUS);
         else if (PrevState == IES_REGISTER && CurrState == IES_MULTIPLY) {
@@ -1216,9 +1215,10 @@ private:
 
   bool omitRegisterFromClobberLists(MCRegister Reg) override;
 
-  /// Parses AVX512 specific operand primitives: masked registers ({%k<NUM>}, {z})
-  /// and memory broadcasting ({1to<NUM>}) primitives, updating Operands vector if required.
-  /// return false if no parsing errors occurred, true otherwise.
+  /// Parses AVX512 specific operand primitives: masked registers ({%k<NUM>},
+  /// {z}) and memory broadcasting ({1to<NUM>}) primitives, updating Operands
+  /// vector if required. return false if no parsing errors occurred, true
+  /// otherwise.
   bool HandleAVX512Operand(OperandVector &Operands);
 
   bool ParseZ(std::unique_ptr<X86Operand> &Z, const SMLoc &StartLoc);
@@ -1239,23 +1239,24 @@ private:
     MCSubtargetInfo &STI = copySTI();
     FeatureBitset AllModes({X86::Is64Bit, X86::Is32Bit, X86::Is16Bit});
     FeatureBitset OldMode = STI.getFeatureBits() & AllModes;
-    FeatureBitset FB = ComputeAvailableFeatures(
-      STI.ToggleFeature(OldMode.flip(mode)));
+    FeatureBitset FB =
+        ComputeAvailableFeatures(STI.ToggleFeature(OldMode.flip(mode)));
     setAvailableFeatures(FB);
 
     assert(FeatureBitset({mode}) == (STI.getFeatureBits() & AllModes));
   }
 
   unsigned getPointerWidth() {
-    if (is16BitMode()) return 16;
-    if (is32BitMode()) return 32;
-    if (is64BitMode()) return 64;
+    if (is16BitMode())
+      return 16;
+    if (is32BitMode())
+      return 32;
+    if (is64BitMode())
+      return 64;
     llvm_unreachable("invalid mode");
   }
 
-  bool isParsingIntelSyntax() {
-    return getParser().getAssemblerDialect();
-  }
+  bool isParsingIntelSyntax() { return getParser().getAssemblerDialect(); }
 
   /// @name Auto-generated Matcher Functions
   /// {
@@ -1274,7 +1275,7 @@ public:
 
   X86AsmParser(const MCSubtargetInfo &sti, MCAsmParser &Parser,
                const MCInstrInfo &mii, const MCTargetOptions &Options)
-      : MCTargetAsmParser(Options, sti, mii),  InstInfo(nullptr),
+      : MCTargetAsmParser(Options, sti, mii), InstInfo(nullptr),
         Code16GCC(false) {
 
     Parser.addAliasForDirective(".word", ".2byte");
@@ -1522,9 +1523,9 @@ bool X86AsmParser::ParseRegister(MCRegister &RegNo, SMLoc &StartLoc,
 
   if (Tok.isNot(AsmToken::Identifier)) {
     OnFailure();
-    if (isParsingIntelSyntax()) return true;
-    return Error(StartLoc, "invalid register name",
-                 SMRange(StartLoc, EndLoc));
+    if (isParsingIntelSyntax())
+      return true;
+    return Error(StartLoc, "invalid register name", SMRange(StartLoc, EndLoc));
   }
 
   if (MatchRegisterByName(RegNo, Tok.getString(), StartLoc, EndLoc)) {
@@ -1550,14 +1551,30 @@ bool X86AsmParser::ParseRegister(MCRegister &RegNo, SMLoc &StartLoc,
       return Error(IntTok.getLoc(), "expected stack index");
     }
     switch (IntTok.getIntVal()) {
-    case 0: RegNo = X86::ST0; break;
-    case 1: RegNo = X86::ST1; break;
-    case 2: RegNo = X86::ST2; break;
-    case 3: RegNo = X86::ST3; break;
-    case 4: RegNo = X86::ST4; break;
-    case 5: RegNo = X86::ST5; break;
-    case 6: RegNo = X86::ST6; break;
-    case 7: RegNo = X86::ST7; break;
+    case 0:
+      RegNo = X86::ST0;
+      break;
+    case 1:
+      RegNo = X86::ST1;
+      break;
+    case 2:
+      RegNo = X86::ST2;
+      break;
+    case 3:
+      RegNo = X86::ST3;
+      break;
+    case 4:
+      RegNo = X86::ST4;
+      break;
+    case 5:
+      RegNo = X86::ST5;
+      break;
+    case 6:
+      RegNo = X86::ST6;
+      break;
+    case 7:
+      RegNo = X86::ST7;
+      break;
     default:
       OnFailure();
       return Error(IntTok.getLoc(), "invalid stack index");
@@ -1580,9 +1597,9 @@ bool X86AsmParser::ParseRegister(MCRegister &RegNo, SMLoc &StartLoc,
 
   if (!RegNo) {
     OnFailure();
-    if (isParsingIntelSyntax()) return true;
-    return Error(StartLoc, "invalid register name",
-                 SMRange(StartLoc, EndLoc));
+    if (isParsingIntelSyntax())
+      return true;
+    return Error(StartLoc, "invalid register name", SMRange(StartLoc, EndLoc));
   }
 
   Parser.Lex(); // Eat identifier token.
@@ -1628,7 +1645,8 @@ std::unique_ptr<X86Operand> X86AsmParser::DefaultMemDIOperand(SMLoc Loc) {
 
 bool X86AsmParser::IsSIReg(MCRegister Reg) {
   switch (Reg.id()) {
-  default: llvm_unreachable("Only (R|E)SI and (R|E)DI are expected!");
+  default:
+    llvm_unreachable("Only (R|E)SI and (R|E)DI are expected!");
   case X86::RSI:
   case X86::ESI:
   case X86::SI:
@@ -1642,7 +1660,8 @@ bool X86AsmParser::IsSIReg(MCRegister Reg) {
 
 MCRegister X86AsmParser::GetSIDIForRegClass(unsigned RegClassID, bool IsSIReg) {
   switch (RegClassID) {
-  default: llvm_unreachable("Unexpected register class");
+  default:
+    llvm_unreachable("Unexpected register class");
   case X86::GR64RegClassID:
     return IsSIReg ? X86::RSI : X86::RDI;
   case X86::GR32RegClassID:
@@ -1653,13 +1672,12 @@ MCRegister X86AsmParser::GetSIDIForRegClass(unsigned RegClassID, bool IsSIReg) {
 }
 
 void X86AsmParser::AddDefaultSrcDestOperands(
-    OperandVector& Operands, std::unique_ptr<llvm::MCParsedAsmOperand> &&Src,
+    OperandVector &Operands, std::unique_ptr<llvm::MCParsedAsmOperand> &&Src,
     std::unique_ptr<llvm::MCParsedAsmOperand> &&Dst) {
   if (isParsingIntelSyntax()) {
     Operands.push_back(std::move(Dst));
     Operands.push_back(std::move(Src));
-  }
-  else {
+  } else {
     Operands.push_back(std::move(Src));
     Operands.push_back(std::move(Dst));
   }
@@ -2146,17 +2164,33 @@ bool X86AsmParser::ParseIntelExpression(IntelExprStateMachine &SM, SMLoc &End) {
       if (SM.onMinus(ErrMsg))
         return Error(getTok().getLoc(), ErrMsg);
       break;
-    case AsmToken::Tilde:   SM.onNot(); break;
-    case AsmToken::Star:    SM.onStar(); break;
-    case AsmToken::Slash:   SM.onDivide(); break;
-    case AsmToken::Percent: SM.onMod(); break;
-    case AsmToken::Pipe:    SM.onOr(); break;
-    case AsmToken::Caret:   SM.onXor(); break;
-    case AsmToken::Amp:     SM.onAnd(); break;
+    case AsmToken::Tilde:
+      SM.onNot();
+      break;
+    case AsmToken::Star:
+      SM.onStar();
+      break;
+    case AsmToken::Slash:
+      SM.onDivide();
+      break;
+    case AsmToken::Percent:
+      SM.onMod();
+      break;
+    case AsmToken::Pipe:
+      SM.onOr();
+      break;
+    case AsmToken::Caret:
+      SM.onXor();
+      break;
+    case AsmToken::Amp:
+      SM.onAnd();
+      break;
     case AsmToken::LessLess:
-                            SM.onLShift(); break;
+      SM.onLShift();
+      break;
     case AsmToken::GreaterGreater:
-                            SM.onRShift(); break;
+      SM.onRShift();
+      break;
     case AsmToken::LBrac:
       if (SM.onLBrac())
         return Error(Tok.getLoc(), "unexpected bracket encountered");
@@ -2167,8 +2201,12 @@ bool X86AsmParser::ParseIntelExpression(IntelExprStateMachine &SM, SMLoc &End) {
         return Error(Tok.getLoc(), ErrMsg);
       }
       break;
-    case AsmToken::LParen:  SM.onLParen(); break;
-    case AsmToken::RParen:  SM.onRParen(); break;
+    case AsmToken::LParen:
+      SM.onLParen();
+      break;
+    case AsmToken::RParen:
+      SM.onRParen();
+      break;
     }
     if (SM.hadError())
       return Error(Tok.getLoc(), "unknown token in expression");
@@ -2243,14 +2281,13 @@ bool X86AsmParser::ParseIntelInlineAsmIdentifier(
   // failed parsing.
   assert((End.getPointer() == EndPtr ||
           Info.isKind(InlineAsmIdentifierInfo::IK_Invalid)) &&
-          "frontend claimed part of a token?");
+         "frontend claimed part of a token?");
 
   // If the identifier lookup was unsuccessful, assume that we are dealing with
   // a label.
   if (Info.isKind(InlineAsmIdentifierInfo::IK_Invalid)) {
-    StringRef InternalName =
-      SemaCallback->LookupInlineAsmLabel(Identifier, getSourceManager(),
-                                         Loc, false);
+    StringRef InternalName = SemaCallback->LookupInlineAsmLabel(
+        Identifier, getSourceManager(), Loc, false);
     assert(InternalName.size() && "We should have an internal name here.");
     // Push a rewrite for replacing the identifier name with the internal name,
     // unless we are parsing the operand of an offset operator
@@ -2268,7 +2305,7 @@ bool X86AsmParser::ParseIntelInlineAsmIdentifier(
   return false;
 }
 
-//ParseRoundingModeOp - Parse AVX-512 rounding mode operand
+// ParseRoundingModeOp - Parse AVX-512 rounding mode operand
 bool X86AsmParser::ParseRoundingModeOp(SMLoc Start, OperandVector &Operands) {
   MCAsmParser &Parser = getParser();
   const AsmToken &Tok = Parser.getTok();
@@ -2278,32 +2315,32 @@ bool X86AsmParser::ParseRoundingModeOp(SMLoc Start, OperandVector &Operands) {
     return Error(Tok.getLoc(), "Expected an identifier after {");
   if (Tok.getIdentifier().starts_with("r")) {
     int rndMode = StringSwitch<int>(Tok.getIdentifier())
-      .Case("rn", X86::STATIC_ROUNDING::TO_NEAREST_INT)
-      .Case("rd", X86::STATIC_ROUNDING::TO_NEG_INF)
-      .Case("ru", X86::STATIC_ROUNDING::TO_POS_INF)
-      .Case("rz", X86::STATIC_ROUNDING::TO_ZERO)
-      .Default(-1);
+                      .Case("rn", X86::STATIC_ROUNDING::TO_NEAREST_INT)
+                      .Case("rd", X86::STATIC_ROUNDING::TO_NEG_INF)
+                      .Case("ru", X86::STATIC_ROUNDING::TO_POS_INF)
+                      .Case("rz", X86::STATIC_ROUNDING::TO_ZERO)
+                      .Default(-1);
     if (-1 == rndMode)
       return Error(Tok.getLoc(), "Invalid rounding mode.");
-     Parser.Lex();  // Eat "r*" of r*-sae
+    Parser.Lex(); // Eat "r*" of r*-sae
     if (!getLexer().is(AsmToken::Minus))
       return Error(Tok.getLoc(), "Expected - at this point");
-    Parser.Lex();  // Eat "-"
-    Parser.Lex();  // Eat the sae
+    Parser.Lex(); // Eat "-"
+    Parser.Lex(); // Eat the sae
     if (!getLexer().is(AsmToken::RCurly))
       return Error(Tok.getLoc(), "Expected } at this point");
     SMLoc End = Tok.getEndLoc();
-    Parser.Lex();  // Eat "}"
+    Parser.Lex(); // Eat "}"
     const MCExpr *RndModeOp =
-      MCConstantExpr::create(rndMode, Parser.getContext());
+        MCConstantExpr::create(rndMode, Parser.getContext());
     Operands.push_back(X86Operand::CreateImm(RndModeOp, Start, End));
     return false;
   }
   if (Tok.getIdentifier() == "sae") {
-    Parser.Lex();  // Eat the sae
+    Parser.Lex(); // Eat the sae
     if (!getLexer().is(AsmToken::RCurly))
       return Error(Tok.getLoc(), "Expected } at this point");
-    Parser.Lex();  // Eat "}"
+    Parser.Lex(); // Eat "}"
     Operands.push_back(X86Operand::CreateToken("{sae}", consumedToken));
     return false;
   }
@@ -2443,10 +2480,10 @@ bool X86AsmParser::ParseIntelOffsetOperator(const MCExpr *&Val, StringRef &ID,
 // Report back its kind, or IOK_INVALID if does not evaluated as a known one
 unsigned X86AsmParser::IdentifyIntelInlineAsmOperator(StringRef Name) {
   return StringSwitch<unsigned>(Name)
-    .Cases("TYPE","type",IOK_TYPE)
-    .Cases("SIZE","size",IOK_SIZE)
-    .Cases("LENGTH","length",IOK_LENGTH)
-    .Default(IOK_INVALID);
+      .Cases("TYPE", "type", IOK_TYPE)
+      .Cases("SIZE", "size", IOK_SIZE)
+      .Cases("LENGTH", "length", IOK_LENGTH)
+      .Default(IOK_INVALID);
 }
 
 /// Parse the 'LENGTH', 'TYPE' and 'SIZE' operators.  The LENGTH operator
@@ -2474,11 +2511,18 @@ unsigned X86AsmParser::ParseIntelInlineAsmOperator(unsigned OpKind) {
   }
 
   unsigned CVal = 0;
-  switch(OpKind) {
-  default: llvm_unreachable("Unexpected operand kind!");
-  case IOK_LENGTH: CVal = Info.Var.Length; break;
-  case IOK_SIZE: CVal = Info.Var.Size; break;
-  case IOK_TYPE: CVal = Info.Var.Type; break;
+  switch (OpKind) {
+  default:
+    llvm_unreachable("Unexpected operand kind!");
+  case IOK_LENGTH:
+    CVal = Info.Var.Length;
+    break;
+  case IOK_SIZE:
+    CVal = Info.Var.Size;
+    break;
+  case IOK_TYPE:
+    CVal = Info.Var.Type;
+    break;
   }
 
   return CVal;
@@ -2553,21 +2597,21 @@ bool X86AsmParser::ParseMasmOperator(unsigned OpKind, int64_t &Val) {
 
 bool X86AsmParser::ParseIntelMemoryOperandSize(unsigned &Size) {
   Size = StringSwitch<unsigned>(getTok().getString())
-    .Cases("BYTE", "byte", 8)
-    .Cases("WORD", "word", 16)
-    .Cases("DWORD", "dword", 32)
-    .Cases("FLOAT", "float", 32)
-    .Cases("LONG", "long", 32)
-    .Cases("FWORD", "fword", 48)
-    .Cases("DOUBLE", "double", 64)
-    .Cases("QWORD", "qword", 64)
-    .Cases("MMWORD","mmword", 64)
-    .Cases("XWORD", "xword", 80)
-    .Cases("TBYTE", "tbyte", 80)
-    .Cases("XMMWORD", "xmmword", 128)
-    .Cases("YMMWORD", "ymmword", 256)
-    .Cases("ZMMWORD", "zmmword", 512)
-    .Default(0);
+             .Cases("BYTE", "byte", 8)
+             .Cases("WORD", "word", 16)
+             .Cases("DWORD", "dword", 32)
+             .Cases("FLOAT", "float", 32)
+             .Cases("LONG", "long", 32)
+             .Cases("FWORD", "fword", 48)
+             .Cases("DOUBLE", "double", 64)
+             .Cases("QWORD", "qword", 64)
+             .Cases("MMWORD", "mmword", 64)
+             .Cases("XWORD", "xword", 80)
+             .Cases("TBYTE", "tbyte", 80)
+             .Cases("XMMWORD", "xmmword", 128)
+             .Cases("YMMWORD", "ymmword", 256)
+             .Cases("ZMMWORD", "zmmword", 512)
+             .Default(0);
   if (Size) {
     const AsmToken &Tok = Lex(); // Eat operand size (e.g., byte, word).
     if (!(Tok.getString() == "PTR" || Tok.getString() == "ptr"))
@@ -2793,9 +2837,8 @@ bool X86AsmParser::parseATTOperand(OperandVector &Operands) {
 
         // Check the register.
         if (Reg == X86::EIZ || Reg == X86::RIZ)
-          return Error(
-              Loc, "%eiz and %riz can only be used as index registers",
-              SMRange(Loc, EndLoc));
+          return Error(Loc, "%eiz and %riz can only be used as index registers",
+                       SMRange(Loc, EndLoc));
         if (Reg == X86::RIP)
           return Error(Loc, "%rip can only be used as a base register",
                        SMRange(Loc, EndLoc));
@@ -2869,7 +2912,7 @@ bool X86AsmParser::HandleAVX512Operand(OperandVector &Operands) {
     // Eat "{" and mark the current place.
     const SMLoc consumedToken = consumeToken();
     // Distinguish {1to<NUM>} from {%k<NUM>}.
-    if(getLexer().is(AsmToken::Integer)) {
+    if (getLexer().is(AsmToken::Integer)) {
       // Parse memory broadcasting ({1to<NUM>}).
       if (getLexer().getTok().getIntVal() != 1)
         return TokError("Expected 1to<NUM> at this point");
@@ -2896,9 +2939,9 @@ bool X86AsmParser::HandleAVX512Operand(OperandVector &Operands) {
       Parser.Lex(); // Eat trailing token of 1toN
       if (!getLexer().is(AsmToken::RCurly))
         return TokError("Expected } at this point");
-      Parser.Lex();  // Eat "}"
-      Operands.push_back(X86Operand::CreateToken(BroadcastPrimitive,
-                                                 consumedToken));
+      Parser.Lex(); // Eat "}"
+      Operands.push_back(
+          X86Operand::CreateToken(BroadcastPrimitive, consumedToken));
       // No AVX512 specific primitives can pass
       // after memory broadcasting, so return.
       return false;
@@ -2925,12 +2968,11 @@ bool X86AsmParser::HandleAVX512Operand(OperandVector &Operands) {
           if (!getLexer().is(AsmToken::RCurly))
             return Error(getLexer().getLoc(), "Expected } at this point");
           Operands.push_back(X86Operand::CreateToken("{", StartLoc));
-          Operands.push_back(
-              X86Operand::CreateReg(RegNo, StartLoc, StartLoc));
+          Operands.push_back(X86Operand::CreateReg(RegNo, StartLoc, StartLoc));
           Operands.push_back(X86Operand::CreateToken("}", consumeToken()));
         } else
           return Error(getLexer().getLoc(),
-                        "Expected an op-mask register at this point");
+                       "Expected an op-mask register at this point");
         // {%k<NUM>} mark is found, inquire for {z}
         if (getLexer().is(AsmToken::LCurly) && !Z) {
           // Have we've found a parsing error, or found no (expected) {z} mark
@@ -2938,7 +2980,6 @@ bool X86AsmParser::HandleAVX512Operand(OperandVector &Operands) {
           if (ParseZ(Z, consumeToken()) || !Z)
             return Error(getLexer().getLoc(),
                          "Expected a {z} mark at this point");
-
         }
         // '{z}' on its own is meaningless, hence should be ignored.
         // on the contrary - have it been accompanied by a K register,
@@ -3298,7 +3339,7 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
   if (PatchedName.starts_with("set") && PatchedName.ends_with("b") &&
       PatchedName != "setzub" && PatchedName != "setzunb" &&
       PatchedName != "setb" && PatchedName != "setnb")
-    PatchedName = PatchedName.substr(0, Name.size()-1);
+    PatchedName = PatchedName.substr(0, Name.size() - 1);
 
   unsigned ComparisonPredicate = ~0U;
 
@@ -3311,56 +3352,57 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
     bool IsVCMP = PatchedName[0] == 'v';
     unsigned CCIdx = IsVCMP ? 4 : 3;
     unsigned suffixLength = PatchedName.ends_with("bf16") ? 5 : 2;
-    unsigned CC = StringSwitch<unsigned>(
-      PatchedName.slice(CCIdx, PatchedName.size() - suffixLength))
-      .Case("eq",       0x00)
-      .Case("eq_oq",    0x00)
-      .Case("lt",       0x01)
-      .Case("lt_os",    0x01)
-      .Case("le",       0x02)
-      .Case("le_os",    0x02)
-      .Case("unord",    0x03)
-      .Case("unord_q",  0x03)
-      .Case("neq",      0x04)
-      .Case("neq_uq",   0x04)
-      .Case("nlt",      0x05)
-      .Case("nlt_us",   0x05)
-      .Case("nle",      0x06)
-      .Case("nle_us",   0x06)
-      .Case("ord",      0x07)
-      .Case("ord_q",    0x07)
-      /* AVX only from here */
-      .Case("eq_uq",    0x08)
-      .Case("nge",      0x09)
-      .Case("nge_us",   0x09)
-      .Case("ngt",      0x0A)
-      .Case("ngt_us",   0x0A)
-      .Case("false",    0x0B)
-      .Case("false_oq", 0x0B)
-      .Case("neq_oq",   0x0C)
-      .Case("ge",       0x0D)
-      .Case("ge_os",    0x0D)
-      .Case("gt",       0x0E)
-      .Case("gt_os",    0x0E)
-      .Case("true",     0x0F)
-      .Case("true_uq",  0x0F)
-      .Case("eq_os",    0x10)
-      .Case("lt_oq",    0x11)
-      .Case("le_oq",    0x12)
-      .Case("unord_s",  0x13)
-      .Case("neq_us",   0x14)
-      .Case("nlt_uq",   0x15)
-      .Case("nle_uq",   0x16)
-      .Case("ord_s",    0x17)
-      .Case("eq_us",    0x18)
-      .Case("nge_uq",   0x19)
-      .Case("ngt_uq",   0x1A)
-      .Case("false_os", 0x1B)
-      .Case("neq_os",   0x1C)
-      .Case("ge_oq",    0x1D)
-      .Case("gt_oq",    0x1E)
-      .Case("true_us",  0x1F)
-      .Default(~0U);
+    unsigned CC =
+        StringSwitch<unsigned>(
+            PatchedName.slice(CCIdx, PatchedName.size() - suffixLength))
+            .Case("eq", 0x00)
+            .Case("eq_oq", 0x00)
+            .Case("lt", 0x01)
+            .Case("lt_os", 0x01)
+            .Case("le", 0x02)
+            .Case("le_os", 0x02)
+            .Case("unord", 0x03)
+            .Case("unord_q", 0x03)
+            .Case("neq", 0x04)
+            .Case("neq_uq", 0x04)
+            .Case("nlt", 0x05)
+            .Case("nlt_us", 0x05)
+            .Case("nle", 0x06)
+            .Case("nle_us", 0x06)
+            .Case("ord", 0x07)
+            .Case("ord_q", 0x07)
+            /* AVX only from here */
+            .Case("eq_uq", 0x08)
+            .Case("nge", 0x09)
+            .Case("nge_us", 0x09)
+            .Case("ngt", 0x0A)
+            .Case("ngt_us", 0x0A)
+            .Case("false", 0x0B)
+            .Case("false_oq", 0x0B)
+            .Case("neq_oq", 0x0C)
+            .Case("ge", 0x0D)
+            .Case("ge_os", 0x0D)
+            .Case("gt", 0x0E)
+            .Case("gt_os", 0x0E)
+            .Case("true", 0x0F)
+            .Case("true_uq", 0x0F)
+            .Case("eq_os", 0x10)
+            .Case("lt_oq", 0x11)
+            .Case("le_oq", 0x12)
+            .Case("unord_s", 0x13)
+            .Case("neq_us", 0x14)
+            .Case("nlt_uq", 0x15)
+            .Case("nle_uq", 0x16)
+            .Case("ord_s", 0x17)
+            .Case("eq_us", 0x18)
+            .Case("nge_uq", 0x19)
+            .Case("ngt_uq", 0x1A)
+            .Case("false_os", 0x1B)
+            .Case("neq_os", 0x1C)
+            .Case("ge_oq", 0x1D)
+            .Case("gt_oq", 0x1E)
+            .Case("true_us", 0x1F)
+            .Default(~0U);
     if (CC != ~0U && (IsVCMP || CC < 8) &&
         (IsVCMP || PatchedName.back() != 'h')) {
       if (PatchedName.ends_with("ss"))
@@ -3389,24 +3431,34 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
       (PatchedName.back() == 'b' || PatchedName.back() == 'w' ||
        PatchedName.back() == 'd' || PatchedName.back() == 'q')) {
     unsigned SuffixSize = PatchedName.drop_back().back() == 'u' ? 2 : 1;
-    unsigned CC = StringSwitch<unsigned>(
-      PatchedName.slice(5, PatchedName.size() - SuffixSize))
-      .Case("eq",    0x0) // Only allowed on unsigned. Checked below.
-      .Case("lt",    0x1)
-      .Case("le",    0x2)
-      //.Case("false", 0x3) // Not a documented alias.
-      .Case("neq",   0x4)
-      .Case("nlt",   0x5)
-      .Case("nle",   0x6)
-      //.Case("true",  0x7) // Not a documented alias.
-      .Default(~0U);
+    unsigned CC =
+        StringSwitch<unsigned>(
+            PatchedName.slice(5, PatchedName.size() - SuffixSize))
+            .Case("eq", 0x0) // Only allowed on unsigned. Checked below.
+            .Case("lt", 0x1)
+            .Case("le", 0x2)
+            //.Case("false", 0x3) // Not a documented alias.
+            .Case("neq", 0x4)
+            .Case("nlt", 0x5)
+            .Case("nle", 0x6)
+            //.Case("true",  0x7) // Not a documented alias.
+            .Default(~0U);
     if (CC != ~0U && (CC != 0 || SuffixSize == 2)) {
       switch (PatchedName.back()) {
-      default: llvm_unreachable("Unexpected character!");
-      case 'b': PatchedName = SuffixSize == 2 ? "vpcmpub" : "vpcmpb"; break;
-      case 'w': PatchedName = SuffixSize == 2 ? "vpcmpuw" : "vpcmpw"; break;
-      case 'd': PatchedName = SuffixSize == 2 ? "vpcmpud" : "vpcmpd"; break;
-      case 'q': PatchedName = SuffixSize == 2 ? "vpcmpuq" : "vpcmpq"; break;
+      default:
+        llvm_unreachable("Unexpected character!");
+      case 'b':
+        PatchedName = SuffixSize == 2 ? "vpcmpub" : "vpcmpb";
+        break;
+      case 'w':
+        PatchedName = SuffixSize == 2 ? "vpcmpuw" : "vpcmpw";
+        break;
+      case 'd':
+        PatchedName = SuffixSize == 2 ? "vpcmpud" : "vpcmpd";
+        break;
+      case 'q':
+        PatchedName = SuffixSize == 2 ? "vpcmpuq" : "vpcmpq";
+        break;
       }
       // Set up the immediate to push into the operands later.
       ComparisonPredicate = CC;
@@ -3419,23 +3471,32 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
        PatchedName.back() == 'd' || PatchedName.back() == 'q')) {
     unsigned SuffixSize = PatchedName.drop_back().back() == 'u' ? 2 : 1;
     unsigned CC = StringSwitch<unsigned>(
-      PatchedName.slice(5, PatchedName.size() - SuffixSize))
-      .Case("lt",    0x0)
-      .Case("le",    0x1)
-      .Case("gt",    0x2)
-      .Case("ge",    0x3)
-      .Case("eq",    0x4)
-      .Case("neq",   0x5)
-      .Case("false", 0x6)
-      .Case("true",  0x7)
-      .Default(~0U);
+                      PatchedName.slice(5, PatchedName.size() - SuffixSize))
+                      .Case("lt", 0x0)
+                      .Case("le", 0x1)
+                      .Case("gt", 0x2)
+                      .Case("ge", 0x3)
+                      .Case("eq", 0x4)
+                      .Case("neq", 0x5)
+                      .Case("false", 0x6)
+                      .Case("true", 0x7)
+                      .Default(~0U);
     if (CC != ~0U) {
       switch (PatchedName.back()) {
-      default: llvm_unreachable("Unexpected character!");
-      case 'b': PatchedName = SuffixSize == 2 ? "vpcomub" : "vpcomb"; break;
-      case 'w': PatchedName = SuffixSize == 2 ? "vpcomuw" : "vpcomw"; break;
-      case 'd': PatchedName = SuffixSize == 2 ? "vpcomud" : "vpcomd"; break;
-      case 'q': PatchedName = SuffixSize == 2 ? "vpcomuq" : "vpcomq"; break;
+      default:
+        llvm_unreachable("Unexpected character!");
+      case 'b':
+        PatchedName = SuffixSize == 2 ? "vpcomub" : "vpcomb";
+        break;
+      case 'w':
+        PatchedName = SuffixSize == 2 ? "vpcomuw" : "vpcomw";
+        break;
+      case 'd':
+        PatchedName = SuffixSize == 2 ? "vpcomud" : "vpcomd";
+        break;
+      case 'q':
+        PatchedName = SuffixSize == 2 ? "vpcomuq" : "vpcomq";
+        break;
       }
       // Set up the immediate to push into the operands later.
       ComparisonPredicate = CC;
@@ -3532,8 +3593,8 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
 
   // Push the immediate if we extracted one from the mnemonic.
   if (ComparisonPredicate != ~0U && !isParsingIntelSyntax()) {
-    const MCExpr *ImmOp = MCConstantExpr::create(ComparisonPredicate,
-                                                 getParser().getContext());
+    const MCExpr *ImmOp =
+        MCConstantExpr::create(ComparisonPredicate, getParser().getContext());
     Operands.push_back(X86Operand::CreateImm(ImmOp, NameLoc, NameLoc));
   }
 
@@ -3563,7 +3624,7 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
         Parser.Lex();
       else
         break;
-     }
+    }
 
     // In MS inline asm curly braces mark the beginning/end of a block,
     // therefore they should be interepreted as end of statement
@@ -3576,8 +3637,8 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
 
   // Push the immediate if we extracted one from the mnemonic.
   if (ComparisonPredicate != ~0U && isParsingIntelSyntax()) {
-    const MCExpr *ImmOp = MCConstantExpr::create(ComparisonPredicate,
-                                                 getParser().getContext());
+    const MCExpr *ImmOp =
+        MCConstantExpr::create(ComparisonPredicate, getParser().getContext());
     Operands.push_back(X86Operand::CreateImm(ImmOp, NameLoc, NameLoc));
   }
 
@@ -3594,13 +3655,13 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
   // Adding "p" for some floating point with no argument.
   // For example: fsub --> fsubp
   bool IsFp =
-    Name == "fsub" || Name == "fdiv" || Name == "fsubr" || Name == "fdivr";
+      Name == "fsub" || Name == "fdiv" || Name == "fsubr" || Name == "fdivr";
   if (IsFp && Operands.size() == 1) {
     const char *Repl = StringSwitch<const char *>(Name)
-      .Case("fsub", "fsubp")
-      .Case("fdiv", "fdivp")
-      .Case("fsubr", "fsubrp")
-      .Case("fdivr", "fdivrp");
+                           .Case("fsub", "fsubp")
+                           .Case("fdiv", "fdivp")
+                           .Case("fsubr", "fsubrp")
+                           .Case("fdivr", "fdivrp");
     static_cast<X86Operand &>(*Operands[0]).setTokenValue(Repl);
   }
 
@@ -3636,8 +3697,8 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
       Operands.size() == 3) {
     X86Operand &Op = (X86Operand &)*Operands.back();
     if (Op.isDXReg())
-      Operands.back() = X86Operand::CreateReg(X86::DX, Op.getStartLoc(),
-                                              Op.getEndLoc());
+      Operands.back() =
+          X86Operand::CreateReg(X86::DX, Op.getStartLoc(), Op.getEndLoc());
   }
   // Same hack for "in[s]?[bwl]? (%dx), %al" -> "inb %dx, %al".
   if ((Name == "inb" || Name == "insb" || Name == "inw" || Name == "insw" ||
@@ -3645,8 +3706,8 @@ bool X86AsmParser::parseInstruction(ParseInstructionInfo &Info, StringRef Name,
       Operands.size() == 3) {
     X86Operand &Op = (X86Operand &)*Operands[1];
     if (Op.isDXReg())
-      Operands[1] = X86Operand::CreateReg(X86::DX, Op.getStartLoc(),
-                                          Op.getEndLoc());
+      Operands[1] =
+          X86Operand::CreateReg(X86::DX, Op.getStartLoc(), Op.getEndLoc());
   }
 
   SmallVector<std::unique_ptr<MCParsedAsmOperand>, 2> TmpOperands;
@@ -3793,7 +3854,8 @@ bool X86AsmParser::processInstruction(MCInst &Inst, const OperandVector &Ops) {
   };
 
   switch (Inst.getOpcode()) {
-  default: return false;
+  default:
+    return false;
   case X86::JMP_1:
     // {disp32} forces a larger displacement as if the instruction was relaxed.
     // NOTE: 16-bit mode uses 16-bit displacement even though it says {disp32}.
@@ -3919,9 +3981,9 @@ bool X86AsmParser::validateInstruction(MCInst &Inst, const OperandVector &Ops) {
       unsigned GroupEnd = GroupStart + 3;
       return Warning(Ops[0]->getStartLoc(),
                      "source register '" + RegName + "' implicitly denotes '" +
-                     RegName.take_front(3) + Twine(GroupStart) + "' to '" +
-                     RegName.take_front(3) + Twine(GroupEnd) +
-                     "' source group");
+                         RegName.take_front(3) + Twine(GroupStart) + "' to '" +
+                         RegName.take_front(3) + Twine(GroupEnd) +
+                         "' source group");
     }
   } else if (isVGATHERDPD(Opcode) || isVGATHERDPS(Opcode) ||
              isVGATHERQPD(Opcode) || isVGATHERQPS(Opcode) ||
@@ -3974,9 +4036,9 @@ bool X86AsmParser::validateInstruction(MCInst &Inst, const OperandVector &Ops) {
 
     if (UsesRex && HReg) {
       StringRef RegName = X86IntelInstPrinter::getRegisterName(HReg);
-      return Error(Ops[0]->getStartLoc(),
-                   "can't encode '" + RegName + "' in an instruction requiring "
-                   "REX prefix");
+      return Error(Ops[0]->getStartLoc(), "can't encode '" + RegName +
+                                              "' in an instruction requiring "
+                                              "REX prefix");
     }
   }
 
@@ -4007,7 +4069,8 @@ void X86AsmParser::emitWarningForSpecialLVIInstruction(SMLoc Loc) {
 /// branch instructions, with an LFENCE in between. For more details, see:
 /// - X86LoadValueInjectionRetHardening.cpp
 /// - X86LoadValueInjectionIndirectThunks.cpp
-/// - https://software.intel.com/security-software-guidance/insights/deep-dive-load-value-injection
+/// -
+/// https://software.intel.com/security-software-guidance/insights/deep-dive-load-value-injection
 ///
 /// Returns `true` if a mitigation was applied or warning was emitted.
 void X86AsmParser::applyLVICFIMitigation(MCInst &Inst, MCStreamer &Out) {
@@ -4125,7 +4188,8 @@ bool X86AsmParser::matchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
                                            MCStreamer &Out, uint64_t &ErrorInfo,
                                            bool MatchingInlineAsm) {
   assert(!Operands.empty() && "Unexpect empty operand list!");
-  assert((*Operands[0]).isToken() && "Leading operand should always be a mnemonic!");
+  assert((*Operands[0]).isToken() &&
+         "Leading operand should always be a mnemonic!");
 
   // First, handle aliases that expand to multiple instructions.
   MatchFPUWaitAlias(IDLoc, static_cast<X86Operand &>(*Operands[0]), Operands,
@@ -4258,15 +4322,16 @@ bool X86AsmParser::matchAndEmitATTInstruction(
     SwitchMode(X86::Is32Bit);
   // First, try a direct match.
   FeatureBitset MissingFeatures;
-  unsigned OriginalError = MatchInstruction(Operands, Inst, ErrorInfo,
-                                            MissingFeatures, MatchingInlineAsm,
-                                            isParsingIntelSyntax());
+  unsigned OriginalError =
+      MatchInstruction(Operands, Inst, ErrorInfo, MissingFeatures,
+                       MatchingInlineAsm, isParsingIntelSyntax());
   if (ForcedDataPrefix == X86::Is32Bit) {
     SwitchMode(X86::Is16Bit);
     ForcedDataPrefix = 0;
   }
   switch (OriginalError) {
-  default: llvm_unreachable("Unexpected match result!");
+  default:
+    llvm_unreachable("Unexpected match result!");
   case Match_Success:
     if (!MatchingInlineAsm && validateInstruction(Inst, Operands))
       return true;
@@ -4520,9 +4585,10 @@ bool X86AsmParser::matchAndEmitIntelInstruction(
           (isIntN(Size, CE->getValue()) || isUIntN(Size, CE->getValue()))) {
         SmallString<16> Tmp;
         Tmp += Base;
-        Tmp += (is64BitMode())
-                   ? "q"
-                   : (is32BitMode()) ? "l" : (is16BitMode()) ? "w" : " ";
+        Tmp += (is64BitMode())   ? "q"
+               : (is32BitMode()) ? "l"
+               : (is16BitMode()) ? "w"
+                                 : " ";
         Op.setTokenValue(Tmp);
         // Do match in ATT mode to allow explicit suffix usage.
         Match.push_back(MatchInstruction(Operands, Inst, ErrorInfo,
@@ -4542,9 +4608,9 @@ bool X86AsmParser::matchAndEmitIntelInstruction(
       UnsizedMemOp->Mem.Size = Size;
       uint64_t ErrorInfoIgnore;
       unsigned LastOpcode = Inst.getOpcode();
-      unsigned M = MatchInstruction(Operands, Inst, ErrorInfoIgnore,
-                                    MissingFeatures, MatchingInlineAsm,
-                                    isParsingIntelSyntax());
+      unsigned M =
+          MatchInstruction(Operands, Inst, ErrorInfoIgnore, MissingFeatures,
+                           MatchingInlineAsm, isParsingIntelSyntax());
       if (Match.empty() || LastOpcode != Inst.getOpcode())
         Match.push_back(M);
 
@@ -4561,9 +4627,9 @@ bool X86AsmParser::matchAndEmitIntelInstruction(
   // operation.  There shouldn't be any ambiguity in our mnemonic table, so try
   // matching with the unsized operand.
   if (Match.empty()) {
-    Match.push_back(MatchInstruction(
-        Operands, Inst, ErrorInfo, MissingFeatures, MatchingInlineAsm,
-        isParsingIntelSyntax()));
+    Match.push_back(MatchInstruction(Operands, Inst, ErrorInfo, MissingFeatures,
+                                     MatchingInlineAsm,
+                                     isParsingIntelSyntax()));
     // If this returned as a missing feature failure, remember that.
     if (Match.back() == Match_MissingFeature)
       ErrorInfoMissingFeatures = MissingFeatures;
@@ -4586,9 +4652,8 @@ bool X86AsmParser::matchAndEmitIntelInstruction(
   if (UnsizedMemOp && NumSuccessfulMatches > 1 &&
       UnsizedMemOp->getMemFrontendSize()) {
     UnsizedMemOp->Mem.Size = UnsizedMemOp->getMemFrontendSize();
-    unsigned M = MatchInstruction(
-        Operands, Inst, ErrorInfo, MissingFeatures, MatchingInlineAsm,
-        isParsingIntelSyntax());
+    unsigned M = MatchInstruction(Operands, Inst, ErrorInfo, MissingFeatures,
+                                  MatchingInlineAsm, isParsingIntelSyntax());
     if (M == Match_Success)
       NumSuccessfulMatches = 1;
 
@@ -4739,7 +4804,7 @@ bool X86AsmParser::parseDirectiveArch() {
 bool X86AsmParser::parseDirectiveNops(SMLoc L) {
   int64_t NumBytes = 0, Control = 0;
   SMLoc NumBytesLoc, ControlLoc;
-  const MCSubtargetInfo& STI = getSTI();
+  const MCSubtargetInfo &STI = getSTI();
   NumBytesLoc = getTok().getLoc();
   if (getParser().checkForValidSection() ||
       getParser().parseAbsoluteExpression(NumBytes))
diff --git a/llvm/lib/Target/X86/AsmParser/X86Operand.h b/llvm/lib/Target/X86/AsmParser/X86Operand.h
index d715fd190..dde9b4324 100644
--- a/llvm/lib/Target/X86/AsmParser/X86Operand.h
+++ b/llvm/lib/Target/X86/AsmParser/X86Operand.h
@@ -9,8 +9,8 @@
 #ifndef LLVM_LIB_TARGET_X86_ASMPARSER_X86OPERAND_H
 #define LLVM_LIB_TARGET_X86_ASMPARSER_X86OPERAND_H
 
-#include "MCTargetDesc/X86IntelInstPrinter.h"
-#include "MCTargetDesc/X86MCTargetDesc.h"
+#include "../MCTargetDesc/X86IntelInstPrinter.h"
+#include "../MCTargetDesc/X86MCTargetDesc.h"
 #include "X86AsmParserCommon.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/StringRef.h"
@@ -219,7 +219,7 @@ struct X86Operand final : public MCParsedAsmOperand {
     return Mem.MaybeDirectBranchDest;
   }
 
-  bool isToken() const override {return Kind == Token; }
+  bool isToken() const override { return Kind == Token; }
 
   bool isImm() const override { return Kind == Immediate; }
 
@@ -281,20 +281,24 @@ struct X86Operand final : public MCParsedAsmOperand {
   }
 
   bool isImmUnsignedi4() const {
-    if (!isImm()) return false;
+    if (!isImm())
+      return false;
     // If this isn't a constant expr, reject it. The immediate byte is shared
     // with a register encoding. We can't have it affected by a relocation.
     const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(getImm());
-    if (!CE) return false;
+    if (!CE)
+      return false;
     return isImmUnsignedi4Value(CE->getValue());
   }
 
   bool isImmUnsignedi8() const {
-    if (!isImm()) return false;
+    if (!isImm())
+      return false;
     // If this isn't a constant expr, just assume it fits and let relaxation
     // handle it.
     const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(getImm());
-    if (!CE) return true;
+    if (!CE)
+      return true;
     return isImmUnsignedi8Value(CE->getValue());
   }
 
@@ -303,12 +307,8 @@ struct X86Operand final : public MCParsedAsmOperand {
   bool needAddressOf() const override { return AddressOf; }
 
   bool isMem() const override { return Kind == Memory; }
-  bool isMemUnsized() const {
-    return Kind == Memory && Mem.Size == 0;
-  }
-  bool isMem8() const {
-    return Kind == Memory && (!Mem.Size || Mem.Size == 8);
-  }
+  bool isMemUnsized() const { return Kind == Memory && Mem.Size == 0; }
+  bool isMem8() const { return Kind == Memory && (!Mem.Size || Mem.Size == 8); }
   bool isMem16() const {
     return Kind == Memory && (!Mem.Size || Mem.Size == 16);
   }
@@ -412,34 +412,23 @@ struct X86Operand final : public MCParsedAsmOperand {
            !getMemIndexReg() && getMemScale() == 1 && isMaybeDirectBranchDest();
   }
 
-  bool isAVX512RC() const{
-      return isImm();
-  }
+  bool isAVX512RC() const { return isImm(); }
 
-  bool isAbsMem16() const {
-    return isAbsMem() && Mem.ModeSize == 16;
-  }
+  bool isAbsMem16() const { return isAbsMem() && Mem.ModeSize == 16; }
 
   bool isMemUseUpRegs() const override { return UseUpRegs; }
 
   bool isSrcIdx() const {
     return !getMemIndexReg() && getMemScale() == 1 &&
-      (getMemBaseReg() == X86::RSI || getMemBaseReg() == X86::ESI ||
-       getMemBaseReg() == X86::SI) && isa<MCConstantExpr>(getMemDisp()) &&
-      cast<MCConstantExpr>(getMemDisp())->getValue() == 0;
-  }
-  bool isSrcIdx8() const {
-    return isMem8() && isSrcIdx();
-  }
-  bool isSrcIdx16() const {
-    return isMem16() && isSrcIdx();
-  }
-  bool isSrcIdx32() const {
-    return isMem32() && isSrcIdx();
-  }
-  bool isSrcIdx64() const {
-    return isMem64() && isSrcIdx();
+           (getMemBaseReg() == X86::RSI || getMemBaseReg() == X86::ESI ||
+            getMemBaseReg() == X86::SI) &&
+           isa<MCConstantExpr>(getMemDisp()) &&
+           cast<MCConstantExpr>(getMemDisp())->getValue() == 0;
   }
+  bool isSrcIdx8() const { return isMem8() && isSrcIdx(); }
+  bool isSrcIdx16() const { return isMem16() && isSrcIdx(); }
+  bool isSrcIdx32() const { return isMem32() && isSrcIdx(); }
+  bool isSrcIdx64() const { return isMem64() && isSrcIdx(); }
 
   bool isDstIdx() const {
     return !getMemIndexReg() && getMemScale() == 1 &&
@@ -449,22 +438,14 @@ struct X86Operand final : public MCParsedAsmOperand {
            isa<MCConstantExpr>(getMemDisp()) &&
            cast<MCConstantExpr>(getMemDisp())->getValue() == 0;
   }
-  bool isDstIdx8() const {
-    return isMem8() && isDstIdx();
-  }
-  bool isDstIdx16() const {
-    return isMem16() && isDstIdx();
-  }
-  bool isDstIdx32() const {
-    return isMem32() && isDstIdx();
-  }
-  bool isDstIdx64() const {
-    return isMem64() && isDstIdx();
-  }
+  bool isDstIdx8() const { return isMem8() && isDstIdx(); }
+  bool isDstIdx16() const { return isMem16() && isDstIdx(); }
+  bool isDstIdx32() const { return isMem32() && isDstIdx(); }
+  bool isDstIdx64() const { return isMem64() && isDstIdx(); }
 
   bool isMemOffs() const {
     return Kind == Memory && !getMemBaseReg() && !getMemIndexReg() &&
-      getMemScale() == 1;
+           getMemScale() == 1;
   }
 
   bool isMemOffs16_8() const {
@@ -507,15 +488,15 @@ struct X86Operand final : public MCParsedAsmOperand {
 
   bool isGR32orGR64() const {
     return Kind == Register &&
-      (X86MCRegisterClasses[X86::GR32RegClassID].contains(getReg()) ||
-       X86MCRegisterClasses[X86::GR64RegClassID].contains(getReg()));
+           (X86MCRegisterClasses[X86::GR32RegClassID].contains(getReg()) ||
+            X86MCRegisterClasses[X86::GR64RegClassID].contains(getReg()));
   }
 
   bool isGR16orGR32orGR64() const {
     return Kind == Register &&
-      (X86MCRegisterClasses[X86::GR16RegClassID].contains(getReg()) ||
-       X86MCRegisterClasses[X86::GR32RegClassID].contains(getReg()) ||
-       X86MCRegisterClasses[X86::GR64RegClassID].contains(getReg()));
+           (X86MCRegisterClasses[X86::GR16RegClassID].contains(getReg()) ||
+            X86MCRegisterClasses[X86::GR32RegClassID].contains(getReg()) ||
+            X86MCRegisterClasses[X86::GR64RegClassID].contains(getReg()));
   }
 
   bool isVectorReg() const {
@@ -528,27 +509,27 @@ struct X86Operand final : public MCParsedAsmOperand {
 
   bool isVK1Pair() const {
     return Kind == Register &&
-      X86MCRegisterClasses[X86::VK1RegClassID].contains(getReg());
+           X86MCRegisterClasses[X86::VK1RegClassID].contains(getReg());
   }
 
   bool isVK2Pair() const {
     return Kind == Register &&
-      X86MCRegisterClasses[X86::VK2RegClassID].contains(getReg());
+           X86MCRegisterClasses[X86::VK2RegClassID].contains(getReg());
   }
 
   bool isVK4Pair() const {
     return Kind == Register &&
-      X86MCRegisterClasses[X86::VK4RegClassID].contains(getReg());
+           X86MCRegisterClasses[X86::VK4RegClassID].contains(getReg());
   }
 
   bool isVK8Pair() const {
     return Kind == Register &&
-      X86MCRegisterClasses[X86::VK8RegClassID].contains(getReg());
+           X86MCRegisterClasses[X86::VK8RegClassID].contains(getReg());
   }
 
   bool isVK16Pair() const {
     return Kind == Register &&
-      X86MCRegisterClasses[X86::VK16RegClassID].contains(getReg());
+           X86MCRegisterClasses[X86::VK16RegClassID].contains(getReg());
   }
 
   void addExpr(MCInst &Inst, const MCExpr *Expr) const {
@@ -709,8 +690,7 @@ struct X86Operand final : public MCParsedAsmOperand {
     return Res;
   }
 
-  static std::unique_ptr<X86Operand>
-  CreateDXReg(SMLoc StartLoc, SMLoc EndLoc) {
+  static std::unique_ptr<X86Operand> CreateDXReg(SMLoc StartLoc, SMLoc EndLoc) {
     return std::make_unique<X86Operand>(DXRegister, StartLoc, EndLoc);
   }
 
@@ -727,11 +707,11 @@ struct X86Operand final : public MCParsedAsmOperand {
                                                void *OpDecl = nullptr,
                                                bool GlobalRef = true) {
     auto Res = std::make_unique<X86Operand>(Immediate, StartLoc, EndLoc);
-    Res->Imm.Val      = Val;
+    Res->Imm.Val = Val;
     Res->Imm.LocalRef = !GlobalRef;
-    Res->SymName      = SymName;
-    Res->OpDecl       = OpDecl;
-    Res->AddressOf    = true;
+    Res->SymName = SymName;
+    Res->OpDecl = OpDecl;
+    Res->AddressOf = true;
     return Res;
   }
 
@@ -743,19 +723,19 @@ struct X86Operand final : public MCParsedAsmOperand {
             bool UseUpRegs = false, bool MaybeDirectBranchDest = true) {
     auto Res = std::make_unique<X86Operand>(Memory, StartLoc, EndLoc);
     Res->Mem.SegReg = MCRegister();
-    Res->Mem.Disp     = Disp;
+    Res->Mem.Disp = Disp;
     Res->Mem.BaseReg = MCRegister();
     Res->Mem.DefaultBaseReg = MCRegister();
     Res->Mem.IndexReg = MCRegister();
-    Res->Mem.Scale    = 1;
-    Res->Mem.Size     = Size;
+    Res->Mem.Scale = 1;
+    Res->Mem.Size = Size;
     Res->Mem.ModeSize = ModeSize;
     Res->Mem.FrontendSize = FrontendSize;
     Res->Mem.MaybeDirectBranchDest = MaybeDirectBranchDest;
     Res->UseUpRegs = UseUpRegs;
-    Res->SymName      = SymName;
-    Res->OpDecl       = OpDecl;
-    Res->AddressOf    = false;
+    Res->SymName = SymName;
+    Res->OpDecl = OpDecl;
+    Res->AddressOf = false;
     return Res;
   }
 
@@ -777,20 +757,20 @@ struct X86Operand final : public MCParsedAsmOperand {
     assert(((Scale == 1 || Scale == 2 || Scale == 4 || Scale == 8)) &&
            "Invalid scale!");
     auto Res = std::make_unique<X86Operand>(Memory, StartLoc, EndLoc);
-    Res->Mem.SegReg   = SegReg;
-    Res->Mem.Disp     = Disp;
-    Res->Mem.BaseReg  = BaseReg;
+    Res->Mem.SegReg = SegReg;
+    Res->Mem.Disp = Disp;
+    Res->Mem.BaseReg = BaseReg;
     Res->Mem.DefaultBaseReg = DefaultBaseReg;
     Res->Mem.IndexReg = IndexReg;
-    Res->Mem.Scale    = Scale;
-    Res->Mem.Size     = Size;
+    Res->Mem.Scale = Scale;
+    Res->Mem.Size = Size;
     Res->Mem.ModeSize = ModeSize;
     Res->Mem.FrontendSize = FrontendSize;
     Res->Mem.MaybeDirectBranchDest = MaybeDirectBranchDest;
     Res->UseUpRegs = UseUpRegs;
-    Res->SymName      = SymName;
-    Res->OpDecl       = OpDecl;
-    Res->AddressOf    = false;
+    Res->SymName = SymName;
+    Res->OpDecl = OpDecl;
+    Res->AddressOf = false;
     return Res;
   }
 };
diff --git a/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp b/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp
index c27177484..2e619a78c 100644
--- a/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp
+++ b/llvm/lib/Target/X86/Disassembler/X86Disassembler.cpp
@@ -73,9 +73,9 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86BaseInfo.h"
-#include "MCTargetDesc/X86MCTargetDesc.h"
-#include "TargetInfo/X86TargetInfo.h"
+#include "../MCTargetDesc/X86BaseInfo.h"
+#include "../MCTargetDesc/X86MCTargetDesc.h"
+#include "../TargetInfo/X86TargetInfo.h"
 #include "X86DisassemblerDecoder.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCDisassembler/MCDisassembler.h"
@@ -273,7 +273,7 @@ static int readPrefixes(struct InternalInstruction *insn) {
     case 0xf0: // LOCK
       insn->hasLockPrefix = true;
       break;
-    case 0xf2: // REPNE/REPNZ
+    case 0xf2:   // REPNE/REPNZ
     case 0xf3: { // REP or REPE/REPZ
       uint8_t nextByte;
       if (peek(insn, nextByte))
@@ -1354,7 +1354,6 @@ static int getInstructionID(struct InternalInstruction *insn,
       attrMask |= ATTR_OPSIZE;
   }
 
-
   if (getInstructionIDWithAttrMask(&instructionID, insn, attrMask))
     return -1;
 
@@ -1814,21 +1813,20 @@ namespace llvm {
 // assigned; they are just filler to make an automatically-generated switch
 // statement work.
 namespace X86 {
-  enum {
-    BX_SI = 500,
-    BX_DI = 501,
-    BP_SI = 502,
-    BP_DI = 503,
-    sib   = 504,
-    sib64 = 505
-  };
+enum {
+  BX_SI = 500,
+  BX_DI = 501,
+  BP_SI = 502,
+  BP_DI = 503,
+  sib = 504,
+  sib64 = 505
+};
 } // namespace X86
 
 } // namespace llvm
 
-static bool translateInstruction(MCInst &target,
-                                InternalInstruction &source,
-                                const MCDisassembler *Dis);
+static bool translateInstruction(MCInst &target, InternalInstruction &source,
+                                 const MCDisassembler *Dis);
 
 namespace {
 
@@ -1837,25 +1835,26 @@ namespace {
 /// disassemblerMode value.
 class X86GenericDisassembler : public MCDisassembler {
   std::unique_ptr<const MCInstrInfo> MII;
+
 public:
   X86GenericDisassembler(const MCSubtargetInfo &STI, MCContext &Ctx,
                          std::unique_ptr<const MCInstrInfo> MII);
+
 public:
   DecodeStatus getInstruction(MCInst &instr, uint64_t &size,
                               ArrayRef<uint8_t> Bytes, uint64_t Address,
                               raw_ostream &cStream) const override;
 
 private:
-  DisassemblerMode              fMode;
+  DisassemblerMode fMode;
 };
 
 } // namespace
 
 X86GenericDisassembler::X86GenericDisassembler(
-                                         const MCSubtargetInfo &STI,
-                                         MCContext &Ctx,
-                                         std::unique_ptr<const MCInstrInfo> MII)
-  : MCDisassembler(STI, Ctx), MII(std::move(MII)) {
+    const MCSubtargetInfo &STI, MCContext &Ctx,
+    std::unique_ptr<const MCInstrInfo> MII)
+    : MCDisassembler(STI, Ctx), MII(std::move(MII)) {
   const FeatureBitset &FB = STI.getFeatureBits();
   if (FB[X86::Is16Bit]) {
     fMode = MODE_16BIT;
@@ -1937,14 +1936,8 @@ static void translateRegister(MCInst &mcInst, Reg reg) {
 }
 
 static const uint8_t segmentRegnums[SEG_OVERRIDE_max] = {
-  0,        // SEG_OVERRIDE_NONE
-  X86::CS,
-  X86::SS,
-  X86::DS,
-  X86::ES,
-  X86::FS,
-  X86::GS
-};
+    0, // SEG_OVERRIDE_NONE
+    X86::CS, X86::SS, X86::DS, X86::ES, X86::FS, X86::GS};
 
 /// translateSrcIndex   - Appends a source index operand to an MCInst.
 ///
@@ -2018,15 +2011,15 @@ static void translateImmediate(MCInst &mcInst, uint64_t immediate,
       default:
         break;
       case 1:
-        if(immediate & 0x80)
+        if (immediate & 0x80)
           immediate |= ~(0xffull);
         break;
       case 2:
-        if(immediate & 0x8000)
+        if (immediate & 0x8000)
           immediate |= ~(0xffffull);
         break;
       case 4:
-        if(immediate & 0x80000000)
+        if (immediate & 0x80000000)
           immediate |= ~(0xffffffffull);
         break;
       case 8:
@@ -2034,15 +2027,15 @@ static void translateImmediate(MCInst &mcInst, uint64_t immediate,
       }
       break;
     case ENCODING_IB:
-      if(immediate & 0x80)
+      if (immediate & 0x80)
         immediate |= ~(0xffull);
       break;
     case ENCODING_IW:
-      if(immediate & 0x8000)
+      if (immediate & 0x8000)
         immediate |= ~(0xffffull);
       break;
     case ENCODING_ID:
-      if(immediate & 0x80000000)
+      if (immediate & 0x80000000)
         immediate |= ~(0xffffffffull);
       break;
     }
@@ -2053,15 +2046,15 @@ static void translateImmediate(MCInst &mcInst, uint64_t immediate,
     default:
       break;
     case ENCODING_IB:
-      if(immediate & 0x80)
+      if (immediate & 0x80)
         immediate |= ~(0xffull);
       break;
     case ENCODING_IW:
-      if(immediate & 0x8000)
+      if (immediate & 0x8000)
         immediate |= ~(0xffffull);
       break;
     case ENCODING_ID:
-      if(immediate & 0x80000000)
+      if (immediate & 0x80000000)
         immediate |= ~(0xffffffffull);
       break;
     case ENCODING_IO:
@@ -2102,8 +2095,7 @@ static void translateImmediate(MCInst &mcInst, uint64_t immediate,
 /// @param insn         - The internal instruction to extract the R/M field
 ///                       from.
 /// @return             - 0 on success; -1 otherwise
-static bool translateRMRegister(MCInst &mcInst,
-                                InternalInstruction &insn) {
+static bool translateRMRegister(MCInst &mcInst, InternalInstruction &insn) {
   if (insn.eaBase == EA_BASE_sib || insn.eaBase == EA_BASE_sib64) {
     debug("A R/M register operand may not have a SIB byte");
     return true;
@@ -2117,15 +2109,16 @@ static bool translateRMRegister(MCInst &mcInst,
     debug("EA_BASE_NONE for ModR/M base");
     return true;
 #define ENTRY(x) case EA_BASE_##x:
-  ALL_EA_BASES
+    ALL_EA_BASES
 #undef ENTRY
     debug("A R/M register operand may not have a base; "
           "the operand must be a register.");
     return true;
-#define ENTRY(x)                                                      \
-  case EA_REG_##x:                                                    \
-    mcInst.addOperand(MCOperand::createReg(X86::x)); break;
-  ALL_REGS
+#define ENTRY(x)                                                               \
+  case EA_REG_##x:                                                             \
+    mcInst.addOperand(MCOperand::createReg(X86::x));                           \
+    break;
+    ALL_REGS
 #undef ENTRY
   }
 
@@ -2169,10 +2162,11 @@ static bool translateRMMemory(MCInst &mcInst, InternalInstruction &insn,
       default:
         debug("Unexpected sibBase");
         return true;
-#define ENTRY(x)                                          \
-      case SIB_BASE_##x:                                  \
-        baseReg = MCOperand::createReg(X86::x); break;
-      ALL_SIB_BASES
+#define ENTRY(x)                                                               \
+  case SIB_BASE_##x:                                                           \
+    baseReg = MCOperand::createReg(X86::x);                                    \
+    break;
+        ALL_SIB_BASES
 #undef ENTRY
       }
     } else {
@@ -2184,14 +2178,15 @@ static bool translateRMMemory(MCInst &mcInst, InternalInstruction &insn,
       default:
         debug("Unexpected sibIndex");
         return true;
-#define ENTRY(x)                                          \
-      case SIB_INDEX_##x:                                 \
-        indexReg = MCOperand::createReg(X86::x); break;
-      EA_BASES_32BIT
-      EA_BASES_64BIT
-      REGS_XMM
-      REGS_YMM
-      REGS_ZMM
+#define ENTRY(x)                                                               \
+  case SIB_INDEX_##x:                                                          \
+    indexReg = MCOperand::createReg(X86::x);                                   \
+    break;
+        EA_BASES_32BIT
+        EA_BASES_64BIT
+        REGS_XMM
+        REGS_YMM
+        REGS_ZMM
 #undef ENTRY
       }
     } else {
@@ -2205,11 +2200,11 @@ static bool translateRMMemory(MCInst &mcInst, InternalInstruction &insn,
       if (!ForceSIB &&
           (insn.sibScale != 1 ||
            (insn.sibBase == SIB_BASE_NONE && insn.mode != MODE_64BIT) ||
-           (insn.sibBase != SIB_BASE_NONE &&
-            insn.sibBase != SIB_BASE_ESP && insn.sibBase != SIB_BASE_RSP &&
-            insn.sibBase != SIB_BASE_R12D && insn.sibBase != SIB_BASE_R12))) {
-        indexReg = MCOperand::createReg(insn.addressSize == 4 ? X86::EIZ :
-                                                                X86::RIZ);
+           (insn.sibBase != SIB_BASE_NONE && insn.sibBase != SIB_BASE_ESP &&
+            insn.sibBase != SIB_BASE_RSP && insn.sibBase != SIB_BASE_R12D &&
+            insn.sibBase != SIB_BASE_R12))) {
+        indexReg =
+            MCOperand::createReg(insn.addressSize == 4 ? X86::EIZ : X86::RIZ);
       } else
         indexReg = MCOperand::createReg(X86::NoRegister);
     }
@@ -2222,16 +2217,15 @@ static bool translateRMMemory(MCInst &mcInst, InternalInstruction &insn,
         debug("EA_BASE_NONE and EA_DISP_NONE for ModR/M base");
         return true;
       }
-      if (insn.mode == MODE_64BIT){
+      if (insn.mode == MODE_64BIT) {
         pcrel = insn.startLocation + insn.length;
         Dis->tryAddingPcLoadReferenceComment(insn.displacement + pcrel,
                                              insn.startLocation +
                                                  insn.displacementOffset);
         // Section 2.2.1.6
-        baseReg = MCOperand::createReg(insn.addressSize == 4 ? X86::EIP :
-                                                               X86::RIP);
-      }
-      else
+        baseReg =
+            MCOperand::createReg(insn.addressSize == 4 ? X86::EIP : X86::RIP);
+      } else
         baseReg = MCOperand::createReg(X86::NoRegister);
 
       indexReg = MCOperand::createReg(X86::NoRegister);
@@ -2262,13 +2256,14 @@ static bool translateRMMemory(MCInst &mcInst, InternalInstruction &insn,
         //   BX_SI, BX_DI, BP_SI, and BP_DI are all handled above and
         //   sib and sib64 were handled in the top-level if, so they're only
         //   placeholders to keep the compiler happy.
-#define ENTRY(x)                                        \
-      case EA_BASE_##x:                                 \
-        baseReg = MCOperand::createReg(X86::x); break;
-      ALL_EA_BASES
+#define ENTRY(x)                                                               \
+  case EA_BASE_##x:                                                            \
+    baseReg = MCOperand::createReg(X86::x);                                    \
+    break;
+        ALL_EA_BASES
 #undef ENTRY
 #define ENTRY(x) case EA_REG_##x:
-      ALL_REGS
+        ALL_REGS
 #undef ENTRY
         debug("A R/M memory operand may not be a register; "
               "the base field must be a base.");
@@ -2344,8 +2339,7 @@ static bool translateRM(MCInst &mcInst, const OperandSpecifier &operand,
 ///
 /// @param mcInst       - The MCInst to append to.
 /// @param stackPos     - The stack position to translate.
-static void translateFPRegister(MCInst &mcInst,
-                                uint8_t stackPos) {
+static void translateFPRegister(MCInst &mcInst, uint8_t stackPos) {
   mcInst.addOperand(MCOperand::createReg(X86::ST0 + stackPos));
 }
 
@@ -2355,8 +2349,7 @@ static void translateFPRegister(MCInst &mcInst,
 /// @param mcInst       - The MCInst to append to.
 /// @param maskRegNum   - Number of mask register from 0 to 7.
 /// @return             - false on success; true otherwise.
-static bool translateMaskRegister(MCInst &mcInst,
-                                uint8_t maskRegNum) {
+static bool translateMaskRegister(MCInst &mcInst, uint8_t maskRegNum) {
   if (maskRegNum >= 8) {
     debug("Invalid mask register number");
     return true;
@@ -2395,11 +2388,8 @@ static bool translateOperand(MCInst &mcInst, const OperandSpecifier &operand,
   case ENCODING_IO:
   case ENCODING_Iv:
   case ENCODING_Ia:
-    translateImmediate(mcInst,
-                       insn.immediates[insn.numImmediatesTranslated++],
-                       operand,
-                       insn,
-                       Dis);
+    translateImmediate(mcInst, insn.immediates[insn.numImmediatesTranslated++],
+                       operand, insn, Dis);
     return false;
   case ENCODING_IRC:
     mcInst.addOperand(MCOperand::createImm(insn.RC));
@@ -2442,9 +2432,8 @@ static bool translateOperand(MCInst &mcInst, const OperandSpecifier &operand,
 /// @param mcInst       - The MCInst to populate with the instruction's data.
 /// @param insn         - The internal instruction.
 /// @return             - false on success; true otherwise.
-static bool translateInstruction(MCInst &mcInst,
-                                InternalInstruction &insn,
-                                const MCDisassembler *Dis) {
+static bool translateInstruction(MCInst &mcInst, InternalInstruction &insn,
+                                 const MCDisassembler *Dis) {
   if (!insn.spec) {
     debug("Instruction has no specification");
     return true;
@@ -2456,9 +2445,9 @@ static bool translateInstruction(MCInst &mcInst,
   // prefix bytes should be disassembled as xrelease and xacquire then set the
   // opcode to those instead of the rep and repne opcodes.
   if (insn.xAcquireRelease) {
-    if(mcInst.getOpcode() == X86::REP_PREFIX)
+    if (mcInst.getOpcode() == X86::REP_PREFIX)
       mcInst.setOpcode(X86::XRELEASE_PREFIX);
-    else if(mcInst.getOpcode() == X86::REPNE_PREFIX)
+    else if (mcInst.getOpcode() == X86::REPNE_PREFIX)
       mcInst.setOpcode(X86::XACQUIRE_PREFIX);
   }
 
diff --git a/llvm/lib/Target/X86/GISel/X86CallLowering.cpp b/llvm/lib/Target/X86/GISel/X86CallLowering.cpp
index e84ee879a..da2ede321 100644
--- a/llvm/lib/Target/X86/GISel/X86CallLowering.cpp
+++ b/llvm/lib/Target/X86/GISel/X86CallLowering.cpp
@@ -13,12 +13,12 @@
 //===----------------------------------------------------------------------===//
 
 #include "X86CallLowering.h"
-#include "X86CallingConv.h"
-#include "X86ISelLowering.h"
-#include "X86InstrInfo.h"
-#include "X86MachineFunctionInfo.h"
-#include "X86RegisterInfo.h"
-#include "X86Subtarget.h"
+#include "../X86CallingConv.h"
+#include "../X86ISelLowering.h"
+#include "../X86InstrInfo.h"
+#include "../X86MachineFunctionInfo.h"
+#include "../X86RegisterInfo.h"
+#include "../X86Subtarget.h"
 #include "llvm/ADT/ArrayRef.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/CodeGen/Analysis.h"
diff --git a/llvm/lib/Target/X86/GISel/X86InstructionSelector.cpp b/llvm/lib/Target/X86/GISel/X86InstructionSelector.cpp
index ee456a11d..dd80d8101 100644
--- a/llvm/lib/Target/X86/GISel/X86InstructionSelector.cpp
+++ b/llvm/lib/Target/X86/GISel/X86InstructionSelector.cpp
@@ -11,14 +11,14 @@
 /// \todo This should be generated by TableGen.
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86BaseInfo.h"
-#include "X86.h"
-#include "X86InstrBuilder.h"
-#include "X86InstrInfo.h"
+#include "../MCTargetDesc/X86BaseInfo.h"
+#include "../X86.h"
+#include "../X86InstrBuilder.h"
+#include "../X86InstrInfo.h"
+#include "../X86RegisterInfo.h"
+#include "../X86Subtarget.h"
+#include "../X86TargetMachine.h"
 #include "X86RegisterBankInfo.h"
-#include "X86RegisterInfo.h"
-#include "X86Subtarget.h"
-#include "X86TargetMachine.h"
 #include "llvm/CodeGen/GlobalISel/GIMatchTableExecutorImpl.h"
 #include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
 #include "llvm/CodeGen/GlobalISel/InstructionSelector.h"
@@ -464,62 +464,62 @@ unsigned X86InstructionSelector::getLoadStoreOp(const LLT &Ty,
     if (X86::GPRRegBankID == RB.getID())
       return Isload ? X86::MOV32rm : X86::MOV32mr;
     if (X86::VECRRegBankID == RB.getID())
-      return Isload ? (HasAVX512 ? X86::VMOVSSZrm_alt :
-                       HasAVX    ? X86::VMOVSSrm_alt :
-                                   X86::MOVSSrm_alt)
-                    : (HasAVX512 ? X86::VMOVSSZmr :
-                       HasAVX    ? X86::VMOVSSmr :
-                                   X86::MOVSSmr);
+      return Isload ? (HasAVX512 ? X86::VMOVSSZrm_alt
+                       : HasAVX  ? X86::VMOVSSrm_alt
+                                 : X86::MOVSSrm_alt)
+                    : (HasAVX512 ? X86::VMOVSSZmr
+                       : HasAVX  ? X86::VMOVSSmr
+                                 : X86::MOVSSmr);
     if (X86::PSRRegBankID == RB.getID())
       return Isload ? X86::LD_Fp32m : X86::ST_Fp32m;
   } else if (Ty == LLT::scalar(64) || Ty == LLT::pointer(0, 64)) {
     if (X86::GPRRegBankID == RB.getID())
       return Isload ? X86::MOV64rm : X86::MOV64mr;
     if (X86::VECRRegBankID == RB.getID())
-      return Isload ? (HasAVX512 ? X86::VMOVSDZrm_alt :
-                       HasAVX    ? X86::VMOVSDrm_alt :
-                                   X86::MOVSDrm_alt)
-                    : (HasAVX512 ? X86::VMOVSDZmr :
-                       HasAVX    ? X86::VMOVSDmr :
-                                   X86::MOVSDmr);
+      return Isload ? (HasAVX512 ? X86::VMOVSDZrm_alt
+                       : HasAVX  ? X86::VMOVSDrm_alt
+                                 : X86::MOVSDrm_alt)
+                    : (HasAVX512 ? X86::VMOVSDZmr
+                       : HasAVX  ? X86::VMOVSDmr
+                                 : X86::MOVSDmr);
     if (X86::PSRRegBankID == RB.getID())
       return Isload ? X86::LD_Fp64m : X86::ST_Fp64m;
   } else if (Ty == LLT::scalar(80)) {
     return Isload ? X86::LD_Fp80m : X86::ST_FpP80m;
   } else if (Ty.isVector() && Ty.getSizeInBits() == 128) {
     if (Alignment >= Align(16))
-      return Isload ? (HasVLX ? X86::VMOVAPSZ128rm
-                              : HasAVX512
-                                    ? X86::VMOVAPSZ128rm_NOVLX
-                                    : HasAVX ? X86::VMOVAPSrm : X86::MOVAPSrm)
-                    : (HasVLX ? X86::VMOVAPSZ128mr
-                              : HasAVX512
-                                    ? X86::VMOVAPSZ128mr_NOVLX
-                                    : HasAVX ? X86::VMOVAPSmr : X86::MOVAPSmr);
+      return Isload ? (HasVLX      ? X86::VMOVAPSZ128rm
+                       : HasAVX512 ? X86::VMOVAPSZ128rm_NOVLX
+                       : HasAVX    ? X86::VMOVAPSrm
+                                   : X86::MOVAPSrm)
+                    : (HasVLX      ? X86::VMOVAPSZ128mr
+                       : HasAVX512 ? X86::VMOVAPSZ128mr_NOVLX
+                       : HasAVX    ? X86::VMOVAPSmr
+                                   : X86::MOVAPSmr);
     else
-      return Isload ? (HasVLX ? X86::VMOVUPSZ128rm
-                              : HasAVX512
-                                    ? X86::VMOVUPSZ128rm_NOVLX
-                                    : HasAVX ? X86::VMOVUPSrm : X86::MOVUPSrm)
-                    : (HasVLX ? X86::VMOVUPSZ128mr
-                              : HasAVX512
-                                    ? X86::VMOVUPSZ128mr_NOVLX
-                                    : HasAVX ? X86::VMOVUPSmr : X86::MOVUPSmr);
+      return Isload ? (HasVLX      ? X86::VMOVUPSZ128rm
+                       : HasAVX512 ? X86::VMOVUPSZ128rm_NOVLX
+                       : HasAVX    ? X86::VMOVUPSrm
+                                   : X86::MOVUPSrm)
+                    : (HasVLX      ? X86::VMOVUPSZ128mr
+                       : HasAVX512 ? X86::VMOVUPSZ128mr_NOVLX
+                       : HasAVX    ? X86::VMOVUPSmr
+                                   : X86::MOVUPSmr);
   } else if (Ty.isVector() && Ty.getSizeInBits() == 256) {
     if (Alignment >= Align(32))
-      return Isload ? (HasVLX ? X86::VMOVAPSZ256rm
-                              : HasAVX512 ? X86::VMOVAPSZ256rm_NOVLX
-                                          : X86::VMOVAPSYrm)
-                    : (HasVLX ? X86::VMOVAPSZ256mr
-                              : HasAVX512 ? X86::VMOVAPSZ256mr_NOVLX
-                                          : X86::VMOVAPSYmr);
+      return Isload ? (HasVLX      ? X86::VMOVAPSZ256rm
+                       : HasAVX512 ? X86::VMOVAPSZ256rm_NOVLX
+                                   : X86::VMOVAPSYrm)
+                    : (HasVLX      ? X86::VMOVAPSZ256mr
+                       : HasAVX512 ? X86::VMOVAPSZ256mr_NOVLX
+                                   : X86::VMOVAPSYmr);
     else
-      return Isload ? (HasVLX ? X86::VMOVUPSZ256rm
-                              : HasAVX512 ? X86::VMOVUPSZ256rm_NOVLX
-                                          : X86::VMOVUPSYrm)
-                    : (HasVLX ? X86::VMOVUPSZ256mr
-                              : HasAVX512 ? X86::VMOVUPSZ256mr_NOVLX
-                                          : X86::VMOVUPSYmr);
+      return Isload ? (HasVLX      ? X86::VMOVUPSZ256rm
+                       : HasAVX512 ? X86::VMOVUPSZ256rm_NOVLX
+                                   : X86::VMOVUPSYrm)
+                    : (HasVLX      ? X86::VMOVUPSZ256mr
+                       : HasAVX512 ? X86::VMOVUPSZ256mr_NOVLX
+                                   : X86::VMOVUPSYmr);
   } else if (Ty.isVector() && Ty.getSizeInBits() == 512) {
     if (Alignment >= Align(64))
       return Isload ? X86::VMOVAPSZrm : X86::VMOVAPSZmr;
@@ -644,8 +644,9 @@ bool X86InstructionSelector::selectFrameIndexOrGep(MachineInstr &I,
                                                    MachineFunction &MF) const {
   unsigned Opc = I.getOpcode();
 
-  assert((Opc == TargetOpcode::G_FRAME_INDEX || Opc == TargetOpcode::G_PTR_ADD) &&
-         "unexpected instruction");
+  assert(
+      (Opc == TargetOpcode::G_FRAME_INDEX || Opc == TargetOpcode::G_PTR_ADD) &&
+      "unexpected instruction");
 
   const Register DefReg = I.getOperand(0).getReg();
   LLT Ty = MRI.getType(DefReg);
@@ -1012,8 +1013,10 @@ bool X86InstructionSelector::selectCmp(MachineInstr &I,
            .addReg(LHS)
            .addReg(RHS);
 
-  MachineInstr &SetInst = *BuildMI(*I.getParent(), I, I.getDebugLoc(),
-                                   TII.get(X86::SETCCr), I.getOperand(0).getReg()).addImm(CC);
+  MachineInstr &SetInst =
+      *BuildMI(*I.getParent(), I, I.getDebugLoc(), TII.get(X86::SETCCr),
+               I.getOperand(0).getReg())
+           .addImm(CC);
 
   constrainSelectedInstRegOperands(CmpInst, TII, TRI, RBI);
   constrainSelectedInstRegOperands(SetInst, TII, TRI, RBI);
@@ -1075,9 +1078,11 @@ bool X86InstructionSelector::selectFCmp(MachineInstr &I,
     Register FlagReg1 = MRI.createVirtualRegister(&X86::GR8RegClass);
     Register FlagReg2 = MRI.createVirtualRegister(&X86::GR8RegClass);
     MachineInstr &Set1 = *BuildMI(*I.getParent(), I, I.getDebugLoc(),
-                                  TII.get(X86::SETCCr), FlagReg1).addImm(SETFOpc[0]);
+                                  TII.get(X86::SETCCr), FlagReg1)
+                              .addImm(SETFOpc[0]);
     MachineInstr &Set2 = *BuildMI(*I.getParent(), I, I.getDebugLoc(),
-                                  TII.get(X86::SETCCr), FlagReg2).addImm(SETFOpc[1]);
+                                  TII.get(X86::SETCCr), FlagReg2)
+                              .addImm(SETFOpc[1]);
     MachineInstr &Set3 = *BuildMI(*I.getParent(), I, I.getDebugLoc(),
                                   TII.get(SETFOpc[2]), ResultReg)
                               .addReg(FlagReg1)
@@ -1105,8 +1110,9 @@ bool X86InstructionSelector::selectFCmp(MachineInstr &I,
            .addReg(LhsReg)
            .addReg(RhsReg);
 
-  MachineInstr &Set =
-      *BuildMI(*I.getParent(), I, I.getDebugLoc(), TII.get(X86::SETCCr), ResultReg).addImm(CC);
+  MachineInstr &Set = *BuildMI(*I.getParent(), I, I.getDebugLoc(),
+                               TII.get(X86::SETCCr), ResultReg)
+                           .addImm(CC);
   constrainSelectedInstRegOperands(CmpInst, TII, TRI, RBI);
   constrainSelectedInstRegOperands(Set, TII, TRI, RBI);
   I.eraseFromParent();
@@ -1410,8 +1416,9 @@ bool X86InstructionSelector::selectInsert(MachineInstr &I,
   return constrainSelectedInstRegOperands(I, TII, TRI, RBI);
 }
 
-bool X86InstructionSelector::selectUnmergeValues(
-    MachineInstr &I, MachineRegisterInfo &MRI, MachineFunction &MF) {
+bool X86InstructionSelector::selectUnmergeValues(MachineInstr &I,
+                                                 MachineRegisterInfo &MRI,
+                                                 MachineFunction &MF) {
   assert((I.getOpcode() == TargetOpcode::G_UNMERGE_VALUES) &&
          "unexpected instruction");
 
@@ -1435,8 +1442,9 @@ bool X86InstructionSelector::selectUnmergeValues(
   return true;
 }
 
-bool X86InstructionSelector::selectMergeValues(
-    MachineInstr &I, MachineRegisterInfo &MRI, MachineFunction &MF) {
+bool X86InstructionSelector::selectMergeValues(MachineInstr &I,
+                                               MachineRegisterInfo &MRI,
+                                               MachineFunction &MF) {
   assert((I.getOpcode() == TargetOpcode::G_MERGE_VALUES ||
           I.getOpcode() == TargetOpcode::G_CONCAT_VECTORS) &&
          "unexpected instruction");
@@ -1497,7 +1505,8 @@ bool X86InstructionSelector::selectCondBranch(MachineInstr &I,
            .addReg(CondReg)
            .addImm(1);
   BuildMI(*I.getParent(), I, I.getDebugLoc(), TII.get(X86::JCC_1))
-      .addMBB(DestMBB).addImm(X86::COND_NE);
+      .addMBB(DestMBB)
+      .addImm(X86::COND_NE);
 
   constrainSelectedInstRegOperands(TestInst, TII, TRI, RBI);
 
@@ -1646,13 +1655,13 @@ bool X86InstructionSelector::selectMulDivRem(MachineInstr &I,
     unsigned HighInReg; // high part of the register pair
     // The following portion depends on both the data type and the operation.
     struct MulDivRemResult {
-      unsigned OpMulDivRem;     // The specific MUL/DIV opcode to use.
-      unsigned OpSignExtend;    // Opcode for sign-extending lowreg into
-                                // highreg, or copying a zero into highreg.
-      unsigned OpCopy;          // Opcode for copying dividend into lowreg, or
-                                // zero/sign-extending into lowreg for i8.
-      unsigned ResultReg;       // Register containing the desired result.
-      bool IsOpSigned;          // Whether to use signed or unsigned form.
+      unsigned OpMulDivRem;  // The specific MUL/DIV opcode to use.
+      unsigned OpSignExtend; // Opcode for sign-extending lowreg into
+                             // highreg, or copying a zero into highreg.
+      unsigned OpCopy;       // Opcode for copying dividend into lowreg, or
+                             // zero/sign-extending into lowreg for i8.
+      unsigned ResultReg;    // Register containing the desired result.
+      bool IsOpSigned;       // Whether to use signed or unsigned form.
     } ResultTable[NumOps];
   } OpTable[NumTypes] = {
       {8,
@@ -1695,10 +1704,10 @@ bool X86InstructionSelector::selectMulDivRem(MachineInstr &I,
        X86::RAX,
        X86::RDX,
        {
-           {X86::IDIV64r, X86::CQO, Copy, X86::RAX, S},    // SDiv
-           {X86::IDIV64r, X86::CQO, Copy, X86::RDX, S},    // SRem
-           {X86::DIV64r, X86::MOV32r0, Copy, X86::RAX, U}, // UDiv
-           {X86::DIV64r, X86::MOV32r0, Copy, X86::RDX, U}, // URem
+           {X86::IDIV64r, X86::CQO, Copy, X86::RAX, S},     // SDiv
+           {X86::IDIV64r, X86::CQO, Copy, X86::RDX, S},     // SRem
+           {X86::DIV64r, X86::MOV32r0, Copy, X86::RAX, U},  // UDiv
+           {X86::DIV64r, X86::MOV32r0, Copy, X86::RDX, U},  // URem
            {X86::IMUL64r, X86::MOV32r0, Copy, X86::RAX, S}, // Mul
            {X86::IMUL64r, X86::MOV32r0, Copy, X86::RDX, S}, // SMulH
            {X86::MUL64r, X86::MOV32r0, Copy, X86::RDX, U},  // UMulH
diff --git a/llvm/lib/Target/X86/GISel/X86LegalizerInfo.cpp b/llvm/lib/Target/X86/GISel/X86LegalizerInfo.cpp
index bab7fe9d2..e7ec49ada 100644
--- a/llvm/lib/Target/X86/GISel/X86LegalizerInfo.cpp
+++ b/llvm/lib/Target/X86/GISel/X86LegalizerInfo.cpp
@@ -11,8 +11,8 @@
 //===----------------------------------------------------------------------===//
 
 #include "X86LegalizerInfo.h"
-#include "X86Subtarget.h"
-#include "X86TargetMachine.h"
+#include "../X86Subtarget.h"
+#include "../X86TargetMachine.h"
 #include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
 #include "llvm/CodeGen/GlobalISel/LegalizerHelper.h"
 #include "llvm/CodeGen/GlobalISel/MachineIRBuilder.h"
@@ -56,7 +56,6 @@ X86LegalizerInfo::X86LegalizerInfo(const X86Subtarget &STI,
   const LLT v2s32 = LLT::fixed_vector(2, 32);
   const LLT v4s8 = LLT::fixed_vector(4, 8);
 
-
   const LLT v16s8 = LLT::fixed_vector(16, 8);
   const LLT v8s16 = LLT::fixed_vector(8, 16);
   const LLT v4s32 = LLT::fixed_vector(4, 32);
@@ -403,13 +402,11 @@ X86LegalizerInfo::X86LegalizerInfo(const X86Subtarget &STI,
 
   for (unsigned Op : {G_SEXTLOAD, G_ZEXTLOAD}) {
     auto &Action = getActionDefinitionsBuilder(Op);
-    Action.legalForTypesWithMemDesc({{s16, p0, s8, 1},
-                                     {s32, p0, s8, 1},
-                                     {s32, p0, s16, 1}});
+    Action.legalForTypesWithMemDesc(
+        {{s16, p0, s8, 1}, {s32, p0, s8, 1}, {s32, p0, s16, 1}});
     if (Is64Bit)
-      Action.legalForTypesWithMemDesc({{s64, p0, s8, 1},
-                                       {s64, p0, s16, 1},
-                                       {s64, p0, s32, 1}});
+      Action.legalForTypesWithMemDesc(
+          {{s64, p0, s8, 1}, {s64, p0, s16, 1}, {s64, p0, s32, 1}});
     // TODO - SSE41/AVX2/AVX512F/AVX512BW vector extensions
   }
 
@@ -417,8 +414,8 @@ X86LegalizerInfo::X86LegalizerInfo(const X86Subtarget &STI,
   getActionDefinitionsBuilder({G_SEXT, G_ZEXT, G_ANYEXT})
       .legalIf([=](const LegalityQuery &Query) {
         return typeInSet(0, {s8, s16, s32})(Query) ||
-          (Query.Opcode == G_ANYEXT && Query.Types[0] == s128) ||
-          (Is64Bit && Query.Types[0] == s64);
+               (Query.Opcode == G_ANYEXT && Query.Types[0] == s128) ||
+               (Is64Bit && Query.Types[0] == s64);
       })
       .widenScalarToNextPow2(0, /*Min=*/8)
       .clampScalar(0, s8, sMaxScalar)
@@ -617,9 +614,8 @@ X86LegalizerInfo::X86LegalizerInfo(const X86Subtarget &STI,
   // memory intrinsics
   getActionDefinitionsBuilder({G_MEMCPY, G_MEMMOVE, G_MEMSET}).libcall();
 
-  getActionDefinitionsBuilder({G_DYN_STACKALLOC,
-                               G_STACKSAVE,
-                               G_STACKRESTORE}).lower();
+  getActionDefinitionsBuilder({G_DYN_STACKALLOC, G_STACKSAVE, G_STACKRESTORE})
+      .lower();
 
   // fp intrinsics
   getActionDefinitionsBuilder(G_INTRINSIC_ROUNDEVEN)
@@ -628,9 +624,9 @@ X86LegalizerInfo::X86LegalizerInfo(const X86Subtarget &STI,
       .libcall();
 
   getActionDefinitionsBuilder({G_FREEZE, G_CONSTANT_FOLD_BARRIER})
-    .legalFor({s8, s16, s32, s64, p0})
-    .widenScalarToNextPow2(0, /*Min=*/8)
-    .clampScalar(0, s8, sMaxScalar);
+      .legalFor({s8, s16, s32, s64, p0})
+      .widenScalarToNextPow2(0, /*Min=*/8)
+      .clampScalar(0, s8, sMaxScalar);
 
   getLegacyLegalizerInfo().computeTables();
   verify(*STI.getInstrInfo());
diff --git a/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.cpp b/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.cpp
index 43c0145ec..5dc3d5ddc 100644
--- a/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.cpp
+++ b/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.cpp
@@ -11,8 +11,8 @@
 //===----------------------------------------------------------------------===//
 
 #include "X86RegisterBankInfo.h"
-#include "X86InstrInfo.h"
-#include "X86Subtarget.h"
+#include "../X86InstrInfo.h"
+#include "../X86Subtarget.h"
 #include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
 #include "llvm/CodeGen/GlobalISel/Utils.h"
 #include "llvm/CodeGen/MachineRegisterInfo.h"
@@ -27,7 +27,7 @@
 using namespace llvm;
 // This file will be TableGen'ed at some point.
 #define GET_TARGET_REGBANK_INFO_IMPL
-#include "X86GenRegisterBankInfo.def"
+#include "../X86GenRegisterBankInfo.def"
 
 X86RegisterBankInfo::X86RegisterBankInfo(const TargetRegisterInfo &TRI) {
 
diff --git a/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.h b/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.h
index f30e9fea1..0b8ba33ce 100644
--- a/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.h
+++ b/llvm/lib/Target/X86/GISel/X86RegisterBankInfo.h
@@ -27,7 +27,7 @@ protected:
 #define GET_TARGET_REGBANK_CLASS
 #include "X86GenRegisterBank.inc"
 #define GET_TARGET_REGBANK_INFO_CLASS
-#include "X86GenRegisterBankInfo.def"
+#include "../X86GenRegisterBankInfo.def"
 
   static RegisterBankInfo::PartialMapping PartMappings[];
   static RegisterBankInfo::ValueMapping ValMappings[];
diff --git a/llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp b/llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp
index a04689f3c..00a0c4208 100644
--- a/llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp
+++ b/llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp
@@ -12,8 +12,8 @@
 //===----------------------------------------------------------------------===//
 
 #include "X86CustomBehaviour.h"
-#include "MCTargetDesc/X86BaseInfo.h"
-#include "TargetInfo/X86TargetInfo.h"
+#include "../MCTargetDesc/X86BaseInfo.h"
+#include "../TargetInfo/X86TargetInfo.h"
 #include "llvm/MC/TargetRegistry.h"
 
 namespace llvm {
diff --git a/llvm/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp b/llvm/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp
index e234d320b..9834dab73 100644
--- a/llvm/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp
+++ b/llvm/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp
@@ -6,9 +6,9 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86BaseInfo.h"
-#include "MCTargetDesc/X86EncodingOptimization.h"
-#include "MCTargetDesc/X86FixupKinds.h"
+#include "X86BaseInfo.h"
+#include "X86EncodingOptimization.h"
+#include "X86FixupKinds.h"
 #include "llvm/ADT/StringSwitch.h"
 #include "llvm/BinaryFormat/ELF.h"
 #include "llvm/BinaryFormat/MachO.h"
@@ -87,12 +87,12 @@ cl::opt<X86AlignBranchKind, true, cl::parser<std::string>> X86AlignBranch(
     "x86-align-branch",
     cl::desc(
         "Specify types of branches to align (plus separated list of types):"
-             "\njcc      indicates conditional jumps"
-             "\nfused    indicates fused conditional jumps"
-             "\njmp      indicates direct unconditional jumps"
-             "\ncall     indicates direct and indirect calls"
-             "\nret      indicates rets"
-             "\nindirect indicates indirect unconditional jumps"),
+        "\njcc      indicates conditional jumps"
+        "\nfused    indicates fused conditional jumps"
+        "\njmp      indicates direct unconditional jumps"
+        "\ncall     indicates direct and indirect calls"
+        "\nret      indicates rets"
+        "\nindirect indicates indirect unconditional jumps"),
     cl::location(X86AlignBranchKindLoc));
 
 cl::opt<bool> X86AlignBranchWithin32BBoundaries(
@@ -504,7 +504,8 @@ bool X86AsmBackend::needAlign(const MCInst &Inst) const {
 
 /// Insert BoundaryAlignFragment before instructions to align branches.
 void X86AsmBackend::emitInstructionBegin(MCObjectStreamer &OS,
-                                         const MCInst &Inst, const MCSubtargetInfo &STI) {
+                                         const MCInst &Inst,
+                                         const MCSubtargetInfo &STI) {
   // Used by canPadInst. Done here, because in emitInstructionEnd, the current
   // fragment will have changed.
   IsRightAfterData =
@@ -703,8 +704,8 @@ static unsigned getFixupKindSize(unsigned Kind) {
 
 void X86AsmBackend::applyFixup(const MCAssembler &Asm, const MCFixup &Fixup,
                                const MCValue &Target,
-                               MutableArrayRef<char> Data,
-                               uint64_t Value, bool IsResolved,
+                               MutableArrayRef<char> Data, uint64_t Value,
+                               bool IsResolved,
                                const MCSubtargetInfo *STI) const {
   unsigned Kind = Fixup.getKind();
   if (Kind >= FirstLiteralRelocationKind)
@@ -715,14 +716,13 @@ void X86AsmBackend::applyFixup(const MCAssembler &Asm, const MCFixup &Fixup,
 
   int64_t SignedValue = static_cast<int64_t>(Value);
   if ((Target.isAbsolute() || IsResolved) &&
-      getFixupKindInfo(Fixup.getKind()).Flags &
-      MCFixupKindInfo::FKF_IsPCRel) {
+      getFixupKindInfo(Fixup.getKind()).Flags & MCFixupKindInfo::FKF_IsPCRel) {
     // check that PC relative fixup fits into the fixup size.
     if (Size > 0 && !isIntN(Size * 8, SignedValue))
       Asm.getContext().reportError(
-                                   Fixup.getLoc(), "value of " + Twine(SignedValue) +
-                                   " is too large for field of " + Twine(Size) +
-                                   ((Size == 1) ? " byte." : " bytes."));
+          Fixup.getLoc(), "value of " + Twine(SignedValue) +
+                              " is too large for field of " + Twine(Size) +
+                              ((Size == 1) ? " byte." : " bytes."));
   } else {
     // Check that uppper bits are either all zeros or all ones.
     // Specifically ignore overflow/underflow as long as the leakage is
@@ -1032,15 +1032,14 @@ bool X86AsmBackend::writeNopData(raw_ostream &OS, uint64_t Count,
       "\x8d\xb4\x00\x00",
   };
 
-  const char(*Nops)[11] =
-      STI->hasFeature(X86::Is16Bit) ? Nops16Bit : Nops32Bit;
+  const char(*Nops)[11] = STI->hasFeature(X86::Is16Bit) ? Nops16Bit : Nops32Bit;
 
   uint64_t MaxNopLength = (uint64_t)getMaximumNopSize(*STI);
 
   // Emit as many MaxNopLength NOPs as needed, then emit a NOP of the remaining
   // length.
   do {
-    const uint8_t ThisNopLength = (uint8_t) std::min(Count, MaxNopLength);
+    const uint8_t ThisNopLength = (uint8_t)std::min(Count, MaxNopLength);
     const uint8_t Prefixes = ThisNopLength <= 10 ? 0 : ThisNopLength - 10;
     for (uint8_t i = 0; i < Prefixes; i++)
       OS << '\x66';
@@ -1068,7 +1067,7 @@ class ELFX86_32AsmBackend : public ELFX86AsmBackend {
 public:
   ELFX86_32AsmBackend(const Target &T, uint8_t OSABI,
                       const MCSubtargetInfo &STI)
-    : ELFX86AsmBackend(T, OSABI, STI) {}
+      : ELFX86AsmBackend(T, OSABI, STI) {}
 
   std::unique_ptr<MCObjectTargetWriter>
   createObjectTargetWriter() const override {
@@ -1084,8 +1083,7 @@ public:
 
   std::unique_ptr<MCObjectTargetWriter>
   createObjectTargetWriter() const override {
-    return createX86ELFObjectWriter(/*IsELF64*/ false, OSABI,
-                                    ELF::EM_X86_64);
+    return createX86ELFObjectWriter(/*IsELF64*/ false, OSABI, ELF::EM_X86_64);
   }
 };
 
@@ -1097,8 +1095,7 @@ public:
 
   std::unique_ptr<MCObjectTargetWriter>
   createObjectTargetWriter() const override {
-    return createX86ELFObjectWriter(/*IsELF64*/ false, OSABI,
-                                    ELF::EM_IAMCU);
+    return createX86ELFObjectWriter(/*IsELF64*/ false, OSABI, ELF::EM_IAMCU);
   }
 };
 
@@ -1106,7 +1103,7 @@ class ELFX86_64AsmBackend : public ELFX86AsmBackend {
 public:
   ELFX86_64AsmBackend(const Target &T, uint8_t OSABI,
                       const MCSubtargetInfo &STI)
-    : ELFX86AsmBackend(T, OSABI, STI) {}
+      : ELFX86AsmBackend(T, OSABI, STI) {}
 
   std::unique_ptr<MCObjectTargetWriter>
   createObjectTargetWriter() const override {
@@ -1120,9 +1117,7 @@ class WindowsX86AsmBackend : public X86AsmBackend {
 public:
   WindowsX86AsmBackend(const Target &T, bool is64Bit,
                        const MCSubtargetInfo &STI)
-    : X86AsmBackend(T, STI)
-    , Is64Bit(is64Bit) {
-  }
+      : X86AsmBackend(T, STI), Is64Bit(is64Bit) {}
 
   std::optional<MCFixupKind> getFixupKind(StringRef Name) const override {
     return StringSwitch<std::optional<MCFixupKind>>(Name)
@@ -1140,27 +1135,27 @@ public:
 
 namespace CU {
 
-  /// Compact unwind encoding values.
-  enum CompactUnwindEncodings {
-    /// [RE]BP based frame where [RE]BP is pused on the stack immediately after
-    /// the return address, then [RE]SP is moved to [RE]BP.
-    UNWIND_MODE_BP_FRAME                   = 0x01000000,
+/// Compact unwind encoding values.
+enum CompactUnwindEncodings {
+  /// [RE]BP based frame where [RE]BP is pused on the stack immediately after
+  /// the return address, then [RE]SP is moved to [RE]BP.
+  UNWIND_MODE_BP_FRAME = 0x01000000,
 
-    /// A frameless function with a small constant stack size.
-    UNWIND_MODE_STACK_IMMD                 = 0x02000000,
+  /// A frameless function with a small constant stack size.
+  UNWIND_MODE_STACK_IMMD = 0x02000000,
 
-    /// A frameless function with a large constant stack size.
-    UNWIND_MODE_STACK_IND                  = 0x03000000,
+  /// A frameless function with a large constant stack size.
+  UNWIND_MODE_STACK_IND = 0x03000000,
 
-    /// No compact unwind encoding is available.
-    UNWIND_MODE_DWARF                      = 0x04000000,
+  /// No compact unwind encoding is available.
+  UNWIND_MODE_DWARF = 0x04000000,
 
-    /// Mask for encoding the frame registers.
-    UNWIND_BP_FRAME_REGISTERS              = 0x00007FFF,
+  /// Mask for encoding the frame registers.
+  UNWIND_BP_FRAME_REGISTERS = 0x00007FFF,
 
-    /// Mask for encoding the frameless registers.
-    UNWIND_FRAMELESS_STACK_REG_PERMUTATION = 0x000003FF
-  };
+  /// Mask for encoding the frameless registers.
+  UNWIND_FRAMELESS_STACK_REG_PERMUTATION = 0x000003FF
+};
 
 } // namespace CU
 
@@ -1174,27 +1169,27 @@ class DarwinX86AsmBackend : public X86AsmBackend {
   Triple TT;
   bool Is64Bit;
 
-  unsigned OffsetSize;                   ///< Offset of a "push" instruction.
-  unsigned MoveInstrSize;                ///< Size of a "move" instruction.
-  unsigned StackDivide;                  ///< Amount to adjust stack size by.
+  unsigned OffsetSize;    ///< Offset of a "push" instruction.
+  unsigned MoveInstrSize; ///< Size of a "move" instruction.
+  unsigned StackDivide;   ///< Amount to adjust stack size by.
 protected:
   /// Size of a "push" instruction for the given register.
   unsigned PushInstrSize(unsigned Reg) const {
     switch (Reg) {
-      case X86::EBX:
-      case X86::ECX:
-      case X86::EDX:
-      case X86::EDI:
-      case X86::ESI:
-      case X86::EBP:
-      case X86::RBX:
-      case X86::RBP:
-        return 1;
-      case X86::R12:
-      case X86::R13:
-      case X86::R14:
-      case X86::R15:
-        return 2;
+    case X86::EBX:
+    case X86::ECX:
+    case X86::EDX:
+    case X86::EDI:
+    case X86::ESI:
+    case X86::EBP:
+    case X86::RBX:
+    case X86::RBP:
+      return 1;
+    case X86::R12:
+    case X86::R13:
+    case X86::R14:
+    case X86::R15:
+      return 2;
     }
     return 1;
   }
@@ -1204,11 +1199,9 @@ private:
   /// corresponds to the enum lists in compact_unwind_encoding.h.
   int getCompactUnwindRegNum(unsigned Reg) const {
     static const MCPhysReg CU32BitRegs[7] = {
-      X86::EBX, X86::ECX, X86::EDX, X86::EDI, X86::ESI, X86::EBP, 0
-    };
+        X86::EBX, X86::ECX, X86::EDX, X86::EDI, X86::ESI, X86::EBP, 0};
     static const MCPhysReg CU64BitRegs[] = {
-      X86::RBX, X86::R12, X86::R13, X86::R14, X86::R15, X86::RBP, 0
-    };
+        X86::RBX, X86::R12, X86::R13, X86::R14, X86::R15, X86::RBP, 0};
     const MCPhysReg *CURegs = Is64Bit ? CU64BitRegs : CU32BitRegs;
     for (int Idx = 1; *CURegs; ++CURegs, ++Idx)
       if (*CURegs == Reg)
@@ -1226,10 +1219,12 @@ private:
     uint32_t RegEnc = 0;
     for (int i = 0, Idx = 0; i != CU_NUM_SAVED_REGS; ++i) {
       unsigned Reg = SavedRegs[i];
-      if (Reg == 0) break;
+      if (Reg == 0)
+        break;
 
       int CURegNum = getCompactUnwindRegNum(Reg);
-      if (CURegNum == -1) return ~0U;
+      if (CURegNum == -1)
+        return ~0U;
 
       // Encode the 3-bit register number in order, skipping over 3-bits for
       // each register.
@@ -1260,7 +1255,8 @@ private:
     //
     for (unsigned i = 0; i < RegCount; ++i) {
       int CUReg = getCompactUnwindRegNum(SavedRegs[i]);
-      if (CUReg == -1) return ~0U;
+      if (CUReg == -1)
+        return ~0U;
       SavedRegs[i] = CUReg;
     }
 
@@ -1268,7 +1264,8 @@ private:
     std::reverse(&SavedRegs[0], &SavedRegs[CU_NUM_SAVED_REGS]);
 
     uint32_t RenumRegs[CU_NUM_SAVED_REGS];
-    for (unsigned i = CU_NUM_SAVED_REGS - RegCount; i < CU_NUM_SAVED_REGS; ++i){
+    for (unsigned i = CU_NUM_SAVED_REGS - RegCount; i < CU_NUM_SAVED_REGS;
+         ++i) {
       unsigned Countless = 0;
       for (unsigned j = CU_NUM_SAVED_REGS - RegCount; j < i; ++j)
         if (SavedRegs[j] < SavedRegs[i])
@@ -1281,28 +1278,26 @@ private:
     uint32_t permutationEncoding = 0;
     switch (RegCount) {
     case 6:
-      permutationEncoding |= 120 * RenumRegs[0] + 24 * RenumRegs[1]
-                             + 6 * RenumRegs[2] +  2 * RenumRegs[3]
-                             +     RenumRegs[4];
+      permutationEncoding |= 120 * RenumRegs[0] + 24 * RenumRegs[1] +
+                             6 * RenumRegs[2] + 2 * RenumRegs[3] + RenumRegs[4];
       break;
     case 5:
-      permutationEncoding |= 120 * RenumRegs[1] + 24 * RenumRegs[2]
-                             + 6 * RenumRegs[3] +  2 * RenumRegs[4]
-                             +     RenumRegs[5];
+      permutationEncoding |= 120 * RenumRegs[1] + 24 * RenumRegs[2] +
+                             6 * RenumRegs[3] + 2 * RenumRegs[4] + RenumRegs[5];
       break;
     case 4:
-      permutationEncoding |=  60 * RenumRegs[2] + 12 * RenumRegs[3]
-                             + 3 * RenumRegs[4] +      RenumRegs[5];
+      permutationEncoding |= 60 * RenumRegs[2] + 12 * RenumRegs[3] +
+                             3 * RenumRegs[4] + RenumRegs[5];
       break;
     case 3:
-      permutationEncoding |=  20 * RenumRegs[3] +  4 * RenumRegs[4]
-                             +     RenumRegs[5];
+      permutationEncoding |=
+          20 * RenumRegs[3] + 4 * RenumRegs[4] + RenumRegs[5];
       break;
     case 2:
-      permutationEncoding |=   5 * RenumRegs[4] +      RenumRegs[5];
+      permutationEncoding |= 5 * RenumRegs[4] + RenumRegs[5];
       break;
     case 1:
-      permutationEncoding |=       RenumRegs[5];
+      permutationEncoding |= RenumRegs[5];
       break;
     }
 
@@ -1334,7 +1329,8 @@ public:
   uint64_t generateCompactUnwindEncoding(const MCDwarfFrameInfo *FI,
                                          const MCContext *Ctxt) const override {
     ArrayRef<MCCFIInstruction> Instrs = FI->Instructions;
-    if (Instrs.empty()) return 0;
+    if (Instrs.empty())
+      return 0;
     if (!isDarwinCanonicalPersonality(FI->Personality) &&
         !Ctxt->emitCompactUnwindNonCanonical())
       return CU::UNWIND_MODE_DWARF;
@@ -1443,7 +1439,8 @@ public:
 
       // Get the encoding of the saved registers when we have a frame pointer.
       uint32_t RegEnc = encodeCompactUnwindRegistersWithFrame();
-      if (RegEnc == ~0U) return CU::UNWIND_MODE_DWARF;
+      if (RegEnc == ~0U)
+        return CU::UNWIND_MODE_DWARF;
 
       CompactUnwindEncoding |= CU::UNWIND_MODE_BP_FRAME;
       CompactUnwindEncoding |= (StackAdjust & 0xFF) << 16;
@@ -1481,11 +1478,12 @@ public:
       // Get the encoding of the saved registers when we don't have a frame
       // pointer.
       uint32_t RegEnc = encodeCompactUnwindRegistersWithoutFrame(SavedRegIdx);
-      if (RegEnc == ~0U) return CU::UNWIND_MODE_DWARF;
+      if (RegEnc == ~0U)
+        return CU::UNWIND_MODE_DWARF;
 
       // Encode the register encoding.
       CompactUnwindEncoding |=
-        RegEnc & CU::UNWIND_FRAMELESS_STACK_REG_PERMUTATION;
+          RegEnc & CU::UNWIND_FRAMELESS_STACK_REG_PERMUTATION;
     }
 
     return CompactUnwindEncoding;
@@ -1526,7 +1524,7 @@ MCAsmBackend *llvm::createX86_64AsmBackend(const Target &T,
 
   if (TheTriple.isUEFI()) {
     assert(TheTriple.isOSBinFormatCOFF() &&
-         "Only COFF format is supported in UEFI environment.");
+           "Only COFF format is supported in UEFI environment.");
     return new WindowsX86AsmBackend(T, true, STI);
   }
 
diff --git a/llvm/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp b/llvm/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp
index cc712055a..c6405e83e 100644
--- a/llvm/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp
+++ b/llvm/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86FixupKinds.h"
-#include "MCTargetDesc/X86MCTargetDesc.h"
+#include "X86FixupKinds.h"
+#include "X86MCTargetDesc.h"
 #include "llvm/BinaryFormat/ELF.h"
 #include "llvm/MC/MCAsmInfo.h"
 #include "llvm/MC/MCContext.h"
diff --git a/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp b/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp
index 55fb9f54c..0b7acab31 100644
--- a/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp
+++ b/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp
@@ -10,9 +10,9 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86BaseInfo.h"
-#include "MCTargetDesc/X86FixupKinds.h"
-#include "MCTargetDesc/X86MCTargetDesc.h"
+#include "X86BaseInfo.h"
+#include "X86FixupKinds.h"
+#include "X86MCTargetDesc.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/MC/MCCodeEmitter.h"
 #include "llvm/MC/MCContext.h"
@@ -1504,7 +1504,7 @@ PrefixKind X86MCCodeEmitter::emitOpcodePrefix(int MemOperand, const MCInst &MI,
 
   // 0x0F escape code must be emitted just before the opcode.
   switch (TSFlags & X86II::OpMapMask) {
-  case X86II::TB:        // Two-byte opcode map
+  case X86II::TB: // Two-byte opcode map
     // Encoded by M bit in REX2
     if (Kind == REX2)
       break;
diff --git a/llvm/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.cpp b/llvm/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.cpp
index 1c4d68d54..4f69a69f5 100644
--- a/llvm/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.cpp
+++ b/llvm/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.cpp
@@ -11,7 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "X86MCTargetDesc.h"
-#include "TargetInfo/X86TargetInfo.h"
+#include "../TargetInfo/X86TargetInfo.h"
 #include "X86ATTInstPrinter.h"
 #include "X86BaseInfo.h"
 #include "X86IntelInstPrinter.h"
@@ -585,7 +585,7 @@ static std::vector<std::pair<uint64_t, uint64_t>>
 findX86PltEntries(uint64_t PltSectionVA, ArrayRef<uint8_t> PltContents) {
   // Do a lightweight parsing of PLT entries.
   std::vector<std::pair<uint64_t, uint64_t>> Result;
-  for (uint64_t Byte = 0, End = PltContents.size(); Byte + 6 < End; ) {
+  for (uint64_t Byte = 0, End = PltContents.size(); Byte + 6 < End;) {
     // Recognize a jmp.
     if (PltContents[Byte] == 0xff && PltContents[Byte + 1] == 0xa3) {
       // The jmp instruction at the beginning of each PLT entry jumps to the
@@ -612,7 +612,7 @@ static std::vector<std::pair<uint64_t, uint64_t>>
 findX86_64PltEntries(uint64_t PltSectionVA, ArrayRef<uint8_t> PltContents) {
   // Do a lightweight parsing of PLT entries.
   std::vector<std::pair<uint64_t, uint64_t>> Result;
-  for (uint64_t Byte = 0, End = PltContents.size(); Byte + 6 < End; ) {
+  for (uint64_t Byte = 0, End = PltContents.size(); Byte + 6 < End;) {
     // Recognize a jmp.
     if (PltContents[Byte] == 0xff && PltContents[Byte + 1] == 0x25) {
       // The jmp instruction at the beginning of each PLT entry jumps to the
diff --git a/llvm/lib/Target/X86/MCTargetDesc/X86MachObjectWriter.cpp b/llvm/lib/Target/X86/MCTargetDesc/X86MachObjectWriter.cpp
index 69eeb09eb..8e806540a 100644
--- a/llvm/lib/Target/X86/MCTargetDesc/X86MachObjectWriter.cpp
+++ b/llvm/lib/Target/X86/MCTargetDesc/X86MachObjectWriter.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86FixupKinds.h"
-#include "MCTargetDesc/X86MCTargetDesc.h"
+#include "X86FixupKinds.h"
+#include "X86MCTargetDesc.h"
 #include "llvm/ADT/Twine.h"
 #include "llvm/BinaryFormat/MachO.h"
 #include "llvm/MC/MCAsmInfo.h"
@@ -27,23 +27,15 @@ class X86MachObjectWriter : public MCMachObjectTargetWriter {
   bool recordScatteredRelocation(MachObjectWriter *Writer,
                                  const MCAssembler &Asm,
                                  const MCFragment *Fragment,
-                                 const MCFixup &Fixup,
-                                 MCValue Target,
-                                 unsigned Log2Size,
-                                 uint64_t &FixedValue);
-  void recordTLVPRelocation(MachObjectWriter *Writer,
-                            const MCAssembler &Asm,
-                            const MCFragment *Fragment,
-                            const MCFixup &Fixup,
-                            MCValue Target,
-                            uint64_t &FixedValue);
-
-  void RecordX86Relocation(MachObjectWriter *Writer,
-                              const MCAssembler &Asm,
-                              const MCFragment *Fragment,
-                              const MCFixup &Fixup,
-                              MCValue Target,
-                              uint64_t &FixedValue);
+                                 const MCFixup &Fixup, MCValue Target,
+                                 unsigned Log2Size, uint64_t &FixedValue);
+  void recordTLVPRelocation(MachObjectWriter *Writer, const MCAssembler &Asm,
+                            const MCFragment *Fragment, const MCFixup &Fixup,
+                            MCValue Target, uint64_t &FixedValue);
+
+  void RecordX86Relocation(MachObjectWriter *Writer, const MCAssembler &Asm,
+                           const MCFragment *Fragment, const MCFixup &Fixup,
+                           MCValue Target, uint64_t &FixedValue);
   void RecordX86_64Relocation(MachObjectWriter *Writer, MCAssembler &Asm,
                               const MCFragment *Fragment, const MCFixup &Fixup,
                               MCValue Target, uint64_t &FixedValue);
@@ -78,9 +70,11 @@ static unsigned getFixupKindLog2Size(unsigned Kind) {
   default:
     llvm_unreachable("invalid fixup kind!");
   case FK_PCRel_1:
-  case FK_Data_1: return 0;
+  case FK_Data_1:
+    return 0;
   case FK_PCRel_2:
-  case FK_Data_2: return 1;
+  case FK_Data_2:
+    return 1;
   case FK_PCRel_4:
     // FIXME: Remove these!!!
   case X86::reloc_riprel_4byte:
@@ -93,8 +87,10 @@ static unsigned getFixupKindLog2Size(unsigned Kind) {
   case X86::reloc_signed_4byte_relax:
   case X86::reloc_branch_4byte_pcrel:
   case X86::reloc_riprel_4byte_relax_evex:
-  case FK_Data_4: return 2;
-  case FK_Data_8: return 3;
+  case FK_Data_4:
+    return 2;
+  case FK_Data_8:
+    return 3;
   }
 }
 
@@ -184,9 +180,10 @@ void X86MachObjectWriter::RecordX86_64Relocation(
     // non-relocatable expression.
     if (A->isUndefined() || B->isUndefined()) {
       StringRef Name = A->isUndefined() ? A->getName() : B->getName();
-      Asm.getContext().reportError(Fixup.getLoc(),
-        "unsupported relocation with subtraction expression, symbol '" +
-        Name + "' can not be undefined in a subtraction expression");
+      Asm.getContext().reportError(
+          Fixup.getLoc(),
+          "unsupported relocation with subtraction expression, symbol '" +
+              Name + "' can not be undefined in a subtraction expression");
       return;
     }
 
@@ -276,9 +273,9 @@ void X86MachObjectWriter::RecordX86_64Relocation(
             Type = MachO::X86_64_RELOC_GOT_LOAD;
           else
             Type = MachO::X86_64_RELOC_GOT;
-        }  else if (Modifier == MCSymbolRefExpr::VK_TLVP) {
+        } else if (Modifier == MCSymbolRefExpr::VK_TLVP) {
           Type = MachO::X86_64_RELOC_TLV;
-        }  else if (Modifier != MCSymbolRefExpr::VK_None) {
+        } else if (Modifier != MCSymbolRefExpr::VK_None) {
           Asm.getContext().reportError(
               Fixup.getLoc(), "unsupported symbol modifier in relocation");
           return;
@@ -300,9 +297,15 @@ void X86MachObjectWriter::RecordX86_64Relocation(
           // (the additional bias), but instead appear to just look at the final
           // offset.
           switch (-(Target.getConstant() + (1LL << Log2Size))) {
-          case 1: Type = MachO::X86_64_RELOC_SIGNED_1; break;
-          case 2: Type = MachO::X86_64_RELOC_SIGNED_2; break;
-          case 4: Type = MachO::X86_64_RELOC_SIGNED_4; break;
+          case 1:
+            Type = MachO::X86_64_RELOC_SIGNED_1;
+            break;
+          case 2:
+            Type = MachO::X86_64_RELOC_SIGNED_2;
+            break;
+          case 4:
+            Type = MachO::X86_64_RELOC_SIGNED_4;
+            break;
           }
         }
       } else {
@@ -356,13 +359,10 @@ void X86MachObjectWriter::RecordX86_64Relocation(
   Writer->addRelocation(RelSymbol, Fragment->getParent(), MRE);
 }
 
-bool X86MachObjectWriter::recordScatteredRelocation(MachObjectWriter *Writer,
-                                                    const MCAssembler &Asm,
-                                                    const MCFragment *Fragment,
-                                                    const MCFixup &Fixup,
-                                                    MCValue Target,
-                                                    unsigned Log2Size,
-                                                    uint64_t &FixedValue) {
+bool X86MachObjectWriter::recordScatteredRelocation(
+    MachObjectWriter *Writer, const MCAssembler &Asm,
+    const MCFragment *Fragment, const MCFixup &Fixup, MCValue Target,
+    unsigned Log2Size, uint64_t &FixedValue) {
   uint64_t OriginalFixedValue = FixedValue;
   uint32_t FixupOffset = Asm.getFragmentOffset(*Fragment) + Fixup.getOffset();
   unsigned IsPCRel = Writer->isFixupKindPCRel(Asm, Fixup.getKind());
@@ -415,19 +415,18 @@ bool X86MachObjectWriter::recordScatteredRelocation(MachObjectWriter *Writer,
       char Buffer[32];
       format("0x%x", FixupOffset).print(Buffer, sizeof(Buffer));
       Asm.getContext().reportError(Fixup.getLoc(),
-                         Twine("Section too large, can't encode "
-                                "r_address (") + Buffer +
-                         ") into 24 bits of scattered "
-                         "relocation entry.");
+                                   Twine("Section too large, can't encode "
+                                         "r_address (") +
+                                       Buffer +
+                                       ") into 24 bits of scattered "
+                                       "relocation entry.");
       return false;
     }
 
     MachO::any_relocation_info MRE;
-    MRE.r_word0 = ((0                         <<  0) | // r_address
+    MRE.r_word0 = ((0 << 0) |                          // r_address
                    (MachO::GENERIC_RELOC_PAIR << 24) | // r_type
-                   (Log2Size                  << 28) |
-                   (IsPCRel                   << 30) |
-                   MachO::R_SCATTERED);
+                   (Log2Size << 28) | (IsPCRel << 30) | MachO::R_SCATTERED);
     MRE.r_word1 = Value2;
     Writer->addRelocation(nullptr, Fragment->getParent(), MRE);
   } else {
@@ -445,11 +444,8 @@ bool X86MachObjectWriter::recordScatteredRelocation(MachObjectWriter *Writer,
   }
 
   MachO::any_relocation_info MRE;
-  MRE.r_word0 = ((FixupOffset <<  0) |
-                 (Type        << 24) |
-                 (Log2Size    << 28) |
-                 (IsPCRel     << 30) |
-                 MachO::R_SCATTERED);
+  MRE.r_word0 = ((FixupOffset << 0) | (Type << 24) | (Log2Size << 28) |
+                 (IsPCRel << 30) | MachO::R_SCATTERED);
   MRE.r_word1 = Value;
   Writer->addRelocation(nullptr, Fragment->getParent(), MRE);
   return true;
diff --git a/llvm/lib/Target/X86/MCTargetDesc/X86WinCOFFObjectWriter.cpp b/llvm/lib/Target/X86/MCTargetDesc/X86WinCOFFObjectWriter.cpp
index f4ac242e9..b353a72b3 100644
--- a/llvm/lib/Target/X86/MCTargetDesc/X86WinCOFFObjectWriter.cpp
+++ b/llvm/lib/Target/X86/MCTargetDesc/X86WinCOFFObjectWriter.cpp
@@ -6,8 +6,8 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "MCTargetDesc/X86FixupKinds.h"
-#include "MCTargetDesc/X86MCTargetDesc.h"
+#include "X86FixupKinds.h"
+#include "X86MCTargetDesc.h"
 #include "llvm/BinaryFormat/COFF.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCExpr.h"
@@ -58,8 +58,9 @@ unsigned X86WinCOFFObjectWriter::getRelocType(MCContext &Ctx,
     }
   }
 
-  MCSymbolRefExpr::VariantKind Modifier = Target.isAbsolute() ?
-    MCSymbolRefExpr::VK_None : Target.getSymA()->getKind();
+  MCSymbolRefExpr::VariantKind Modifier = Target.isAbsolute()
+                                              ? MCSymbolRefExpr::VK_None
+                                              : Target.getSymA()->getKind();
 
   if (Is64Bit) {
     switch (FixupKind) {
diff --git a/llvm/lib/Target/X86/TargetInfo/X86TargetInfo.cpp b/llvm/lib/Target/X86/TargetInfo/X86TargetInfo.cpp
index c8df3c29b..391793ea8 100644
--- a/llvm/lib/Target/X86/TargetInfo/X86TargetInfo.cpp
+++ b/llvm/lib/Target/X86/TargetInfo/X86TargetInfo.cpp
@@ -6,7 +6,7 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "TargetInfo/X86TargetInfo.h"
+#include "X86TargetInfo.h"
 #include "llvm/MC/TargetRegistry.h"
 using namespace llvm;
 
diff --git a/llvm/lib/Target/X86/X86CodeGenPassBuilder.cpp b/llvm/lib/Target/X86/X86CodeGenPassBuilder.cpp
index d979517e1..cdc40c40f 100644
--- a/llvm/lib/Target/X86/X86CodeGenPassBuilder.cpp
+++ b/llvm/lib/Target/X86/X86CodeGenPassBuilder.cpp
@@ -51,7 +51,7 @@ Error X86CodeGenPassBuilder::addInstSelector(AddMachinePass &addPass) const {
 } // namespace
 
 void X86TargetMachine::registerPassBuilderCallbacks(PassBuilder &PB) {
-#define GET_PASS_REGISTRY "X86PassRegistry.def"
+#define GET_PASS_REGISTRY "llvm/lib/Target/X86/X86PassRegistry.def"
 #include "llvm/Passes/TargetPassRegistry.inc"
 }
 
diff --git a/llvm/projects/CMakeLists.txt b/llvm/projects/CMakeLists.txt
index 08f2fa522..8b3e38b18 100644
--- a/llvm/projects/CMakeLists.txt
+++ b/llvm/projects/CMakeLists.txt
@@ -41,6 +41,7 @@ endif()
 
 add_llvm_external_project(dragonegg)
 add_llvm_external_project(openmp)
+add_llvm_external_project(clang)
 
 if(LLVM_INCLUDE_TESTS)
   add_llvm_external_project(cross-project-tests)
diff --git a/llvm/utils/TableGen/Basic/CMakeLists.txt b/llvm/utils/TableGen/Basic/CMakeLists.txt
index b058fba78..a7541d4ee 100644
--- a/llvm/utils/TableGen/Basic/CMakeLists.txt
+++ b/llvm/utils/TableGen/Basic/CMakeLists.txt
@@ -8,7 +8,7 @@ set(LLVM_LINK_COMPONENTS
   TableGen
   )
 
-add_llvm_library(LLVMTableGenBasic OBJECT EXCLUDE_FROM_ALL DISABLE_LLVM_LINK_LLVM_DYLIB
+add_llvm_library(LLVMTableGenBasic STATIC EXCLUDE_FROM_ALL DISABLE_LLVM_LINK_LLVM_DYLIB
   ARMTargetDefEmitter.cpp
   Attributes.cpp
   CodeGenIntrinsics.cpp
diff --git a/llvm/utils/TableGen/CMakeLists.txt b/llvm/utils/TableGen/CMakeLists.txt
index 67291214c..2d4f66489 100644
--- a/llvm/utils/TableGen/CMakeLists.txt
+++ b/llvm/utils/TableGen/CMakeLists.txt
@@ -18,7 +18,15 @@ set(LLVM_LINK_COMPONENTS Support)
 # E.g. CMake derives which linker to use from the types of sources added.
 add_tablegen(llvm-min-tblgen LLVM_HEADERS
   llvm-min-tblgen.cpp
-  $<TARGET_OBJECTS:obj.LLVMTableGenBasic>
+  Basic/ARMTargetDefEmitter.cpp
+  Basic/Attributes.cpp
+  Basic/CodeGenIntrinsics.cpp
+  Basic/DirectiveEmitter.cpp
+  Basic/IntrinsicEmitter.cpp
+  Basic/RISCVTargetDefEmitter.cpp
+  Basic/SDNodeProperties.cpp
+  Basic/TableGen.cpp
+  Basic/VTEmitter.cpp
 
   PARTIAL_SOURCES_INTENDED
   )
@@ -70,11 +78,42 @@ add_tablegen(llvm-tblgen LLVM
   X86MnemonicTables.cpp
   X86ModRMFilters.cpp
   X86RecognizableInstr.cpp
-  $<TARGET_OBJECTS:obj.LLVMTableGenBasic>
-  $<TARGET_OBJECTS:obj.LLVMTableGenCommon>
+  Basic/ARMTargetDefEmitter.cpp
+  Basic/Attributes.cpp
+  Basic/CodeGenIntrinsics.cpp
+  Basic/DirectiveEmitter.cpp
+  Basic/IntrinsicEmitter.cpp
+  Basic/RISCVTargetDefEmitter.cpp
+  Basic/SDNodeProperties.cpp
+  Basic/TableGen.cpp
+  Basic/VTEmitter.cpp
+  Common/GlobalISel/CodeExpander.cpp
+  Common/GlobalISel/CombinerUtils.cpp
+  Common/GlobalISel/CXXPredicates.cpp
+  Common/GlobalISel/GlobalISelMatchTable.cpp
+  Common/GlobalISel/GlobalISelMatchTableExecutorEmitter.cpp
+  Common/GlobalISel/PatternParser.cpp
+  Common/GlobalISel/Patterns.cpp
+  Common/AsmWriterInst.cpp
+  Common/CodeGenDAGPatterns.cpp
+  Common/CodeGenHwModes.cpp
+  Common/CodeGenInstAlias.cpp
+  Common/CodeGenInstruction.cpp
+  Common/CodeGenRegisters.cpp
+  Common/CodeGenSchedule.cpp
+  Common/CodeGenTarget.cpp
+  Common/DAGISelMatcher.cpp
+  Common/InfoByHwMode.cpp
+  Common/OptEmitter.cpp
+  Common/PredicateExpander.cpp
+  Common/SubtargetFeatureInfo.cpp
+  Common/Types.cpp
+  Common/Utils.cpp
+  Common/VarLenCodeEmitterGen.cpp
 
   PARTIAL_SOURCES_INTENDED
 
   DEPENDS
+  vt_gen
   intrinsics_gen # via llvm-min-tablegen
   )
diff --git a/llvm/utils/TableGen/Common/CMakeLists.txt b/llvm/utils/TableGen/Common/CMakeLists.txt
index 734215698..604627587 100644
--- a/llvm/utils/TableGen/Common/CMakeLists.txt
+++ b/llvm/utils/TableGen/Common/CMakeLists.txt
@@ -10,7 +10,7 @@ set(LLVM_LINK_COMPONENTS
   TableGen
   )
 
-add_llvm_library(LLVMTableGenCommon STATIC OBJECT EXCLUDE_FROM_ALL DISABLE_LLVM_LINK_LLVM_DYLIB
+add_llvm_library(LLVMTableGenCommon STATIC EXCLUDE_FROM_ALL DISABLE_LLVM_LINK_LLVM_DYLIB
   GlobalISel/CodeExpander.cpp
   GlobalISel/CombinerUtils.cpp
   GlobalISel/CXXPredicates.cpp
diff --git a/llvm/utils/TableGen/Common/CodeGenDAGPatterns.h b/llvm/utils/TableGen/Common/CodeGenDAGPatterns.h
index 6a6f1a6ac..ecdfd189a 100644
--- a/llvm/utils/TableGen/Common/CodeGenDAGPatterns.h
+++ b/llvm/utils/TableGen/Common/CodeGenDAGPatterns.h
@@ -14,8 +14,8 @@
 #ifndef LLVM_UTILS_TABLEGEN_COMMON_CODEGENDAGPATTERNS_H
 #define LLVM_UTILS_TABLEGEN_COMMON_CODEGENDAGPATTERNS_H
 
-#include "Basic/CodeGenIntrinsics.h"
-#include "Basic/SDNodeProperties.h"
+#include "../Basic/CodeGenIntrinsics.h"
+#include "../Basic/SDNodeProperties.h"
 #include "CodeGenTarget.h"
 #include "llvm/ADT/IntrusiveRefCntPtr.h"
 #include "llvm/ADT/MapVector.h"
diff --git a/llvm/utils/TableGen/Common/CodeGenTarget.cpp b/llvm/utils/TableGen/Common/CodeGenTarget.cpp
index 96829a185..c60f8c198 100644
--- a/llvm/utils/TableGen/Common/CodeGenTarget.cpp
+++ b/llvm/utils/TableGen/Common/CodeGenTarget.cpp
@@ -52,7 +52,7 @@ StringRef llvm::getEnumName(MVT::SimpleValueType T) {
   switch (T) {
 #define GET_VT_ATTR(Ty, N, Sz, Any, Int, FP, Vec, Sc, Tup, NF, NElem, EltTy)   \
   case MVT::Ty: return "MVT::" # Ty;
-#include "llvm/CodeGen/GenVT.inc"
+#include "GenVT.inc"
   default: llvm_unreachable("ILLEGAL VALUE TYPE!");
   }
   // clang-format on
diff --git a/llvm/utils/TableGen/Common/CodeGenTarget.h b/llvm/utils/TableGen/Common/CodeGenTarget.h
index 682cc4e2b..2b9c7f772 100644
--- a/llvm/utils/TableGen/Common/CodeGenTarget.h
+++ b/llvm/utils/TableGen/Common/CodeGenTarget.h
@@ -16,8 +16,8 @@
 #ifndef LLVM_UTILS_TABLEGEN_COMMON_CODEGENTARGET_H
 #define LLVM_UTILS_TABLEGEN_COMMON_CODEGENTARGET_H
 
-#include "Basic/CodeGenIntrinsics.h"
-#include "Basic/SDNodeProperties.h"
+#include "../Basic/CodeGenIntrinsics.h"
+#include "../Basic/SDNodeProperties.h"
 #include "CodeGenHwModes.h"
 #include "CodeGenInstruction.h"
 #include "InfoByHwMode.h"
diff --git a/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.cpp b/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.cpp
index 8564bf8d2..fd54fb606 100644
--- a/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.cpp
+++ b/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.cpp
@@ -7,8 +7,8 @@
 //===----------------------------------------------------------------------===//
 
 #include "GlobalISelMatchTable.h"
-#include "Common/CodeGenInstruction.h"
-#include "Common/CodeGenRegisters.h"
+#include "../CodeGenInstruction.h"
+#include "../CodeGenRegisters.h"
 #include "llvm/ADT/Statistic.h"
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/LEB128.h"
diff --git a/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.h b/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.h
index 77c8bc290..b786c8281 100644
--- a/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.h
+++ b/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTable.h
@@ -16,7 +16,7 @@
 #ifndef LLVM_UTILS_TABLEGEN_COMMON_GLOBALISEL_GLOBALISELMATCHTABLE_H
 #define LLVM_UTILS_TABLEGEN_COMMON_GLOBALISEL_GLOBALISELMATCHTABLE_H
 
-#include "Common/CodeGenDAGPatterns.h"
+#include "../CodeGenDAGPatterns.h"
 #include "llvm/ADT/ArrayRef.h"
 #include "llvm/ADT/DenseMap.h"
 #include "llvm/ADT/MapVector.h"
diff --git a/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTableExecutorEmitter.h b/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTableExecutorEmitter.h
index 862f1e83c..33ee9171b 100644
--- a/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTableExecutorEmitter.h
+++ b/llvm/utils/TableGen/Common/GlobalISel/GlobalISelMatchTableExecutorEmitter.h
@@ -15,7 +15,7 @@
 #ifndef LLVM_UTILS_TABLEGEN_COMMON_GLOBALISEL_GLOBALISELMATCHTABLEEXECUTOREMITTER_H
 #define LLVM_UTILS_TABLEGEN_COMMON_GLOBALISEL_GLOBALISELMATCHTABLEEXECUTOREMITTER_H
 
-#include "Common/SubtargetFeatureInfo.h"
+#include "../SubtargetFeatureInfo.h"
 #include "llvm/ADT/ArrayRef.h"
 #include "llvm/ADT/StringRef.h"
 #include "llvm/ADT/Twine.h"
diff --git a/llvm/utils/TableGen/Common/GlobalISel/PatternParser.cpp b/llvm/utils/TableGen/Common/GlobalISel/PatternParser.cpp
index cb423ce14..602846c20 100644
--- a/llvm/utils/TableGen/Common/GlobalISel/PatternParser.cpp
+++ b/llvm/utils/TableGen/Common/GlobalISel/PatternParser.cpp
@@ -6,11 +6,11 @@
 //
 //===----------------------------------------------------------------------===//
 
-#include "Common/GlobalISel/PatternParser.h"
-#include "Basic/CodeGenIntrinsics.h"
-#include "Common/CodeGenTarget.h"
-#include "Common/GlobalISel/CombinerUtils.h"
-#include "Common/GlobalISel/Patterns.h"
+#include "PatternParser.h"
+#include "../../Basic/CodeGenIntrinsics.h"
+#include "../CodeGenTarget.h"
+#include "../GlobalISel/CombinerUtils.h"
+#include "../GlobalISel/Patterns.h"
 #include "llvm/ADT/StringRef.h"
 #include "llvm/Support/PrettyStackTrace.h"
 #include "llvm/Support/SaveAndRestore.h"
diff --git a/llvm/utils/TableGen/Common/GlobalISel/Patterns.cpp b/llvm/utils/TableGen/Common/GlobalISel/Patterns.cpp
index 0b84a9bbe..342c605ba 100644
--- a/llvm/utils/TableGen/Common/GlobalISel/Patterns.cpp
+++ b/llvm/utils/TableGen/Common/GlobalISel/Patterns.cpp
@@ -7,11 +7,11 @@
 //===----------------------------------------------------------------------===//
 
 #include "Patterns.h"
-#include "Basic/CodeGenIntrinsics.h"
+#include "../../Basic/CodeGenIntrinsics.h"
+#include "../CodeGenInstruction.h"
 #include "CXXPredicates.h"
 #include "CodeExpander.h"
 #include "CodeExpansions.h"
-#include "Common/CodeGenInstruction.h"
 #include "llvm/ADT/StringSet.h"
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/raw_ostream.h"
diff --git a/llvm/utils/TableGen/GlobalISelCombinerEmitter.cpp b/llvm/utils/TableGen/GlobalISelCombinerEmitter.cpp
index 770494405..898d03882 100644
--- a/llvm/utils/TableGen/GlobalISelCombinerEmitter.cpp
+++ b/llvm/utils/TableGen/GlobalISelCombinerEmitter.cpp
@@ -2649,15 +2649,15 @@ GICombinerEmitter::buildMatchTable(MutableArrayRef<RuleMatcher> Rules) {
       ++CurrentOrdering;
   }
 
-  llvm::stable_sort(InputRules, [&OpcodeOrder](const Matcher *A,
-                                               const Matcher *B) {
-    auto *L = static_cast<const RuleMatcher *>(A);
-    auto *R = static_cast<const RuleMatcher *>(B);
-    return std::tuple(OpcodeOrder[L->getOpcode()],
-                      L->insnmatchers_front().getNumOperandMatchers()) <
-           std::tuple(OpcodeOrder[R->getOpcode()],
-                      R->insnmatchers_front().getNumOperandMatchers());
-  });
+  llvm::stable_sort(
+      InputRules, [&OpcodeOrder](const Matcher *A, const Matcher *B) {
+        auto *L = static_cast<const RuleMatcher *>(A);
+        auto *R = static_cast<const RuleMatcher *>(B);
+        return std::tuple(OpcodeOrder[L->getOpcode()],
+                          L->insnmatchers_front().getNumOperandMatchers()) <
+               std::tuple(OpcodeOrder[R->getOpcode()],
+                          R->insnmatchers_front().getNumOperandMatchers());
+      });
 
   for (Matcher *Rule : InputRules)
     Rule->optimize();
